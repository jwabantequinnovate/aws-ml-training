{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fraud Detection Solution\n",
    "\n",
    "## Complete implementation with best practices\n",
    "\n",
    "This notebook provides the complete solution for the fraud detection exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, roc_auc_score, \n",
    "    precision_recall_curve, roc_curve, auc\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import shap\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.style.use('seaborn-v0_8')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fraud_dataset(n_samples=100000, fraud_ratio=0.02):\n",
    "    \"\"\"\n",
    "    Generate synthetic fraud detection dataset with realistic patterns\n",
    "    \"\"\"\n",
    "    n_fraud = int(n_samples * fraud_ratio)\n",
    "    n_legit = n_samples - n_fraud\n",
    "    \n",
    "    # Legitimate transactions\n",
    "    legit_data = {\n",
    "        'transaction_amount': np.random.gamma(2, 50, n_legit),\n",
    "        'hour_of_day': np.random.choice(range(24), n_legit, p=[\n",
    "            0.01, 0.01, 0.01, 0.01, 0.01, 0.02, 0.04, 0.06,\n",
    "            0.08, 0.07, 0.06, 0.07, 0.08, 0.07, 0.06, 0.05,\n",
    "            0.04, 0.05, 0.06, 0.07, 0.06, 0.04, 0.03, 0.02\n",
    "        ]),\n",
    "        'day_of_week': np.random.randint(0, 7, n_legit),\n",
    "        'merchant_category': np.random.choice(\n",
    "            ['retail', 'grocery', 'gas', 'restaurant', 'online'],\n",
    "            n_legit\n",
    "        ),\n",
    "        'distance_from_home': np.abs(np.random.normal(5, 10, n_legit)),\n",
    "        'distance_from_last_transaction': np.abs(np.random.normal(3, 5, n_legit)),\n",
    "        'transaction_velocity': np.random.poisson(2, n_legit),\n",
    "        'is_fraud': np.zeros(n_legit, dtype=int)\n",
    "    }\n",
    "    \n",
    "    # Fraudulent transactions (different patterns)\n",
    "    fraud_data = {\n",
    "        'transaction_amount': np.random.gamma(5, 100, n_fraud),  # Higher amounts\n",
    "        'hour_of_day': np.random.choice(range(24), n_fraud, p=[\n",
    "            0.08, 0.08, 0.07, 0.06, 0.05, 0.03, 0.02, 0.02,\n",
    "            0.02, 0.02, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
    "            0.03, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.08\n",
    "        ]),  # More at night\n",
    "        'day_of_week': np.random.randint(0, 7, n_fraud),\n",
    "        'merchant_category': np.random.choice(\n",
    "            ['retail', 'grocery', 'gas', 'restaurant', 'online'],\n",
    "            n_fraud,\n",
    "            p=[0.15, 0.10, 0.15, 0.10, 0.50]  # More online\n",
    "        ),\n",
    "        'distance_from_home': np.abs(np.random.normal(50, 100, n_fraud)),  # Farther\n",
    "        'distance_from_last_transaction': np.abs(np.random.normal(100, 200, n_fraud)),\n",
    "        'transaction_velocity': np.random.poisson(8, n_fraud),  # Higher velocity\n",
    "        'is_fraud': np.ones(n_fraud, dtype=int)\n",
    "    }\n",
    "    \n",
    "    # Combine and shuffle\n",
    "    df_legit = pd.DataFrame(legit_data)\n",
    "    df_fraud = pd.DataFrame(fraud_data)\n",
    "    df = pd.concat([df_legit, df_fraud], ignore_index=True)\n",
    "    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Generate dataset\n",
    "df = generate_fraud_dataset(n_samples=100000, fraud_ratio=0.02)\n",
    "\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"\\nClass distribution:\")\n",
    "print(df['is_fraud'].value_counts())\n",
    "print(\"\\nFraud ratio:\", df['is_fraud'].mean())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize class distribution\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Transaction amount\n",
    "axes[0, 0].hist(df[df['is_fraud']==0]['transaction_amount'], bins=50, alpha=0.5, label='Legit')\n",
    "axes[0, 0].hist(df[df['is_fraud']==1]['transaction_amount'], bins=50, alpha=0.5, label='Fraud')\n",
    "axes[0, 0].set_xlabel('Transaction Amount')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].set_title('Transaction Amount Distribution')\n",
    "\n",
    "# Hour of day\n",
    "df.groupby(['hour_of_day', 'is_fraud']).size().unstack().plot(ax=axes[0, 1])\n",
    "axes[0, 1].set_xlabel('Hour of Day')\n",
    "axes[0, 1].set_ylabel('Count')\n",
    "axes[0, 1].set_title('Transactions by Hour')\n",
    "\n",
    "# Distance from home\n",
    "axes[1, 0].hist(df[df['is_fraud']==0]['distance_from_home'], bins=50, alpha=0.5, label='Legit')\n",
    "axes[1, 0].hist(df[df['is_fraud']==1]['distance_from_home'], bins=50, alpha=0.5, label='Fraud')\n",
    "axes[1, 0].set_xlabel('Distance from Home')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].set_title('Distance from Home Distribution')\n",
    "\n",
    "# Transaction velocity\n",
    "axes[1, 1].hist(df[df['is_fraud']==0]['transaction_velocity'], bins=20, alpha=0.5, label='Legit')\n",
    "axes[1, 1].hist(df[df['is_fraud']==1]['transaction_velocity'], bins=20, alpha=0.5, label='Fraud')\n",
    "axes[1, 1].set_xlabel('Transaction Velocity')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].set_title('Transaction Velocity Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create additional features\n",
    "df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)\n",
    "df['is_night'] = ((df['hour_of_day'] >= 22) | (df['hour_of_day'] <= 6)).astype(int)\n",
    "df['amount_velocity_ratio'] = df['transaction_amount'] / (df['transaction_velocity'] + 1)\n",
    "df['distance_ratio'] = df['distance_from_last_transaction'] / (df['distance_from_home'] + 1)\n",
    "\n",
    "# Encode categorical variable\n",
    "le = LabelEncoder()\n",
    "df['merchant_category_encoded'] = le.fit_transform(df['merchant_category'])\n",
    "\n",
    "print(\"Engineered features:\")\n",
    "df[['is_weekend', 'is_night', 'amount_velocity_ratio', 'distance_ratio', 'merchant_category_encoded']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features\n",
    "feature_cols = [\n",
    "    'transaction_amount', 'hour_of_day', 'day_of_week',\n",
    "    'distance_from_home', 'distance_from_last_transaction',\n",
    "    'transaction_velocity', 'is_weekend', 'is_night',\n",
    "    'amount_velocity_ratio', 'distance_ratio', 'merchant_category_encoded'\n",
    "]\n",
    "\n",
    "X = df[feature_cols]\n",
    "y = df['is_fraud']\n",
    "\n",
    "# Split data: 60% train, 20% validation, 20% test\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"Train set: {X_train.shape}, Fraud ratio: {y_train.mean():.4f}\")\n",
    "print(f\"Validation set: {X_val.shape}, Fraud ratio: {y_val.mean():.4f}\")\n",
    "print(f\"Test set: {X_test.shape}, Fraud ratio: {y_test.mean():.4f}\")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Handle Imbalanced Data with SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SMOTE to training data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"Original training set: {X_train_scaled.shape}\")\n",
    "print(f\"Balanced training set: {X_train_balanced.shape}\")\n",
    "print(f\"\\nOriginal fraud ratio: {y_train.mean():.4f}\")\n",
    "print(f\"Balanced fraud ratio: {y_train_balanced.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Logistic Regression (baseline)\n",
    "lr_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "lr_model.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "# Train XGBoost\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42,\n",
    "    eval_metric='auc'\n",
    ")\n",
    "xgb_model.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "# Train LightGBM\n",
    "lgb_model = lgb.LGBMClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "lgb_model.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "print(\"All models trained successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on validation set\n",
    "models = {\n",
    "    'Logistic Regression': lr_model,\n",
    "    'XGBoost': xgb_model,\n",
    "    'LightGBM': lgb_model\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    y_pred_proba = model.predict_proba(X_val_scaled)[:, 1]\n",
    "    y_pred = (y_pred_proba >= 0.5).astype(int)\n",
    "    \n",
    "    roc_auc = roc_auc_score(y_val, y_pred_proba)\n",
    "    \n",
    "    results[name] = {\n",
    "        'predictions': y_pred,\n",
    "        'probabilities': y_pred_proba,\n",
    "        'roc_auc': roc_auc\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"ROC-AUC: {roc_auc:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_val, y_pred))\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(confusion_matrix(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curves\n",
    "plt.figure(figsize=(10, 8))\n",
    "for name, result in results.items():\n",
    "    fpr, tpr, _ = roc_curve(y_val, result['probabilities'])\n",
    "    plt.plot(fpr, tpr, label=f\"{name} (AUC = {result['roc_auc']:.4f})\")\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Precision-Recall curves\n",
    "plt.figure(figsize=(10, 8))\n",
    "for name, result in results.items():\n",
    "    precision, recall, _ = precision_recall_curve(y_val, result['probabilities'])\n",
    "    plt.plot(recall, precision, label=name)\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curves Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost-sensitive evaluation\n",
    "cost_fp = 5   # Cost of investigating a legitimate transaction\n",
    "cost_fn = 100  # Cost of missing a fraudulent transaction\n",
    "\n",
    "print(\"Cost-Sensitive Evaluation:\")\n",
    "print(\"=\"*50)\n",
    "for name, result in results.items():\n",
    "    tn, fp, fn, tp = confusion_matrix(y_val, result['predictions']).ravel()\n",
    "    total_cost = (fp * cost_fp) + (fn * cost_fn)\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  False Positives: {fp}, Cost: ${fp * cost_fp}\")\n",
    "    print(f\"  False Negatives: {fn}, Cost: ${fn * cost_fn}\")\n",
    "    print(f\"  Total Cost: ${total_cost}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Explainability with SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use best model (typically XGBoost or LightGBM)\n",
    "best_model = xgb_model\n",
    "\n",
    "# Calculate SHAP values\n",
    "explainer = shap.TreeExplainer(best_model)\n",
    "shap_values = explainer.shap_values(X_val_scaled[:1000])  # Sample for speed\n",
    "\n",
    "# Summary plot\n",
    "shap.summary_plot(shap_values, X_val.iloc[:1000], feature_names=feature_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': best_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance['feature'], feature_importance['importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Feature Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(feature_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain a single fraud prediction\n",
    "fraud_idx = y_val[y_val == 1].index[0]\n",
    "single_pred = X_val_scaled[fraud_idx:fraud_idx+1]\n",
    "\n",
    "print(f\"Prediction probability: {best_model.predict_proba(single_pred)[0, 1]:.4f}\")\n",
    "shap.force_plot(\n",
    "    explainer.expected_value,\n",
    "    shap_values[fraud_idx],\n",
    "    X_val.iloc[fraud_idx],\n",
    "    feature_names=feature_cols\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Final Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate best model on test set\n",
    "y_test_pred_proba = best_model.predict_proba(X_test_scaled)[:, 1]\n",
    "y_test_pred = (y_test_pred_proba >= 0.5).astype(int)\n",
    "\n",
    "print(\"Final Test Set Performance:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"ROC-AUC: {roc_auc_score(y_test, y_test_pred_proba):.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Model Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and preprocessing artifacts\n",
    "import os\n",
    "\n",
    "model_dir = '../../models'\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save model\n",
    "joblib.dump(best_model, f'{model_dir}/fraud_detection_model.pkl')\n",
    "joblib.dump(scaler, f'{model_dir}/fraud_detection_scaler.pkl')\n",
    "joblib.dump(le, f'{model_dir}/fraud_detection_encoder.pkl')\n",
    "joblib.dump(feature_cols, f'{model_dir}/fraud_detection_features.pkl')\n",
    "\n",
    "print(\"Model artifacts saved successfully!\")\n",
    "print(f\"Location: {model_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Class Imbalance**: SMOTE significantly improved model performance\n",
    "2. **Model Selection**: XGBoost/LightGBM outperformed logistic regression\n",
    "3. **Feature Engineering**: Temporal and distance-based features were crucial\n",
    "4. **Explainability**: SHAP values help understand and trust predictions\n",
    "5. **Cost-Sensitive**: Consider business costs when choosing threshold\n",
    "\n",
    "## Next Steps for Production\n",
    "\n",
    "1. Deploy to SageMaker real-time endpoint\n",
    "2. Set up model monitoring for data drift\n",
    "3. Implement automated retraining pipeline\n",
    "4. Add feature store for real-time features\n",
    "5. Set up A/B testing framework"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
