{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45561312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Environment Setup\n",
    "# ============================================================\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "project_root = os.path.abspath('../..')\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.steps import ProcessingStep, TrainingStep, CreateModelStep\n",
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "from sagemaker.workflow.conditions import ConditionGreaterThanOrEqualTo\n",
    "from sagemaker.workflow.condition_step import ConditionStep\n",
    "from sagemaker.workflow.functions import JsonGet, Join\n",
    "from sagemaker.workflow.parameters import ParameterString, ParameterFloat, ParameterInteger\n",
    "from sagemaker.workflow.properties import PropertyFile\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "from sagemaker.model_metrics import MetricsSource, ModelMetrics\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "\n",
    "try:\n",
    "    from utils.sagemaker_config import get_sagemaker_config\n",
    "    config = get_sagemaker_config(s3_prefix='lab7-pipelines')\n",
    "    role = config['role']\n",
    "    session = config['session']\n",
    "    bucket = config['bucket']\n",
    "    region = config['region']\n",
    "except ImportError:\n",
    "    role = get_execution_role()\n",
    "    session = sagemaker.Session()\n",
    "    bucket = session.default_bucket()\n",
    "    region = session.boto_region_name\n",
    "\n",
    "print(f\"Configuration complete\")\n",
    "print(f\"Region: {region}\")\n",
    "print(f\"S3 Bucket: s3://{bucket}\")\n",
    "print(f\"Role: {role}\")\n",
    "\n",
    "sm_client = boto3.client('sagemaker', region_name=region)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f2cf25",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 1: Pr√©parer les Scripts de Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f761f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cr√©er les scripts de processing et training\n",
    "# ============================================================\n",
    "\n",
    "# Script de preprocessing\n",
    "preprocessing_script = \"\"\"\n",
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--test-size', type=float, default=0.2)\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Generate synthetic data\n",
    "    np.random.seed(42)\n",
    "    n_samples = 5000\n",
    "    \n",
    "    X = np.random.randn(n_samples, 10)\n",
    "    y = (X[:, 0] + X[:, 1] - X[:, 2] > 0).astype(int)\n",
    "    \n",
    "    # Add noise features\n",
    "    noise = np.random.randn(n_samples, 5) * 0.1\n",
    "    X = np.hstack([X, noise])\n",
    "    \n",
    "    # Create DataFrame\n",
    "    feature_cols = [f'feature_{i}' for i in range(X.shape[1])]\n",
    "    df = pd.DataFrame(X, columns=feature_cols)\n",
    "    df['target'] = y\n",
    "    \n",
    "    # Split data\n",
    "    train_df, test_df = train_test_split(df, test_size=args.test_size, random_state=42)\n",
    "    \n",
    "    print(f\"Train size: {len(train_df)}\")\n",
    "    print(f\"Test size: {len(test_df)}\")\n",
    "    \n",
    "    # Save processed data\n",
    "    os.makedirs('/opt/ml/processing/train', exist_ok=True)\n",
    "    os.makedirs('/opt/ml/processing/test', exist_ok=True)\n",
    "    \n",
    "    train_df.to_csv('/opt/ml/processing/train/train.csv', index=False)\n",
    "    test_df.to_csv('/opt/ml/processing/test/test.csv', index=False)\n",
    "    \n",
    "    print(\"Preprocessing complete\")\n",
    "\"\"\"\n",
    "\n",
    "# Script de training\n",
    "training_script = \"\"\"\n",
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--n-estimators', type=int, default=100)\n",
    "    parser.add_argument('--max-depth', type=int, default=10)\n",
    "    parser.add_argument('--model-dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n",
    "    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))\n",
    "    parser.add_argument('--test', type=str, default=os.environ.get('SM_CHANNEL_TEST'))\n",
    "    parser.add_argument('--output-data-dir', type=str, default=os.environ.get('SM_OUTPUT_DATA_DIR'))\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Load data\n",
    "    train_df = pd.read_csv(os.path.join(args.train, 'train.csv'))\n",
    "    test_df = pd.read_csv(os.path.join(args.test, 'test.csv'))\n",
    "    \n",
    "    X_train = train_df.drop('target', axis=1)\n",
    "    y_train = train_df['target']\n",
    "    X_test = test_df.drop('target', axis=1)\n",
    "    y_test = test_df['target']\n",
    "    \n",
    "    # Train model\n",
    "    print(f\"Training with n_estimators={args.n_estimators}, max_depth={args.max_depth}\")\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=args.n_estimators,\n",
    "        max_depth=args.max_depth,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate\n",
    "    train_pred = model.predict(X_train)\n",
    "    test_pred = model.predict(X_test)\n",
    "    \n",
    "    metrics = {\n",
    "        'train_accuracy': float(accuracy_score(y_train, train_pred)),\n",
    "        'test_accuracy': float(accuracy_score(y_test, test_pred)),\n",
    "        'test_precision': float(precision_score(y_test, test_pred)),\n",
    "        'test_recall': float(recall_score(y_test, test_pred)),\n",
    "        'test_f1': float(f1_score(y_test, test_pred))\n",
    "    }\n",
    "    \n",
    "    print(f\"Metrics: {metrics}\")\n",
    "    \n",
    "    # Save model\n",
    "    joblib.dump(model, os.path.join(args.model_dir, 'model.pkl'))\n",
    "    \n",
    "    # Save metrics for pipeline evaluation\n",
    "    os.makedirs(args.output_data_dir, exist_ok=True)\n",
    "    with open(os.path.join(args.output_data_dir, 'evaluation.json'), 'w') as f:\n",
    "        json.dump(metrics, f)\n",
    "    \n",
    "    print(\"Training complete\")\n",
    "\"\"\"\n",
    "\n",
    "# Script d'√©valuation\n",
    "evaluation_script = \"\"\"\n",
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import joblib\n",
    "import tarfile\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--model-dir', type=str, default='/opt/ml/processing/model')\n",
    "    parser.add_argument('--test-data', type=str, default='/opt/ml/processing/test')\n",
    "    parser.add_argument('--output-dir', type=str, default='/opt/ml/processing/evaluation')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Extract model from tar.gz\n",
    "    model_path = os.path.join(args.model_dir, 'model.tar.gz')\n",
    "    extract_dir = '/tmp/model'\n",
    "    os.makedirs(extract_dir, exist_ok=True)\n",
    "    \n",
    "    with tarfile.open(model_path, 'r:gz') as tar:\n",
    "        tar.extractall(path=extract_dir)\n",
    "    \n",
    "    # Load model\n",
    "    model = joblib.load(os.path.join(extract_dir, 'model.pkl'))\n",
    "    \n",
    "    # Load test data\n",
    "    test_df = pd.read_csv(os.path.join(args.test_data, 'test.csv'))\n",
    "    X_test = test_df.drop('target', axis=1)\n",
    "    y_test = test_df['target']\n",
    "    \n",
    "    # Predict\n",
    "    predictions = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    report = classification_report(y_test, predictions, output_dict=True)\n",
    "    conf_matrix = confusion_matrix(y_test, predictions).tolist()\n",
    "    \n",
    "    evaluation_metrics = {\n",
    "        'classification_metrics': {\n",
    "            'accuracy': {'value': accuracy},\n",
    "            'precision': {'value': report['1']['precision']},\n",
    "            'recall': {'value': report['1']['recall']},\n",
    "            'f1_score': {'value': report['1']['f1-score']}\n",
    "        },\n",
    "        'confusion_matrix': conf_matrix\n",
    "    }\n",
    "    \n",
    "    print(f\"Evaluation metrics: {evaluation_metrics}\")\n",
    "    \n",
    "    # Save evaluation report\n",
    "    os.makedirs(args.output_dir, exist_ok=True)\n",
    "    with open(os.path.join(args.output_dir, 'evaluation.json'), 'w') as f:\n",
    "        json.dump(evaluation_metrics, f)\n",
    "    \n",
    "    print(\"Evaluation complete\")\n",
    "\"\"\"\n",
    "\n",
    "# Sauvegarder les scripts\n",
    "os.makedirs('pipeline_scripts', exist_ok=True)\n",
    "\n",
    "with open('pipeline_scripts/preprocessing.py', 'w') as f:\n",
    "    f.write(preprocessing_script)\n",
    "\n",
    "with open('pipeline_scripts/training.py', 'w') as f:\n",
    "    f.write(training_script)\n",
    "\n",
    "with open('pipeline_scripts/evaluation.py', 'w') as f:\n",
    "    f.write(evaluation_script)\n",
    "\n",
    "print(\"Scripts cr√©√©s:\")\n",
    "print(\"- pipeline_scripts/preprocessing.py\")\n",
    "print(\"- pipeline_scripts/training.py\")\n",
    "print(\"- pipeline_scripts/evaluation.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852f0916",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 2: D√©finir les Param√®tres du Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d69567e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Param√®tres du pipeline\n",
    "# ============================================================\n",
    "\n",
    "# Param√®tres d'entr√©e du pipeline (peuvent √™tre modifi√©s lors de l'ex√©cution)\n",
    "processing_instance_type = ParameterString(\n",
    "    name=\"ProcessingInstanceType\",\n",
    "    default_value=\"ml.t3.medium\"  # Changed from ml.m5.large due to quota limits\n",
    ")\n",
    "\n",
    "training_instance_type = ParameterString(\n",
    "    name=\"TrainingInstanceType\",\n",
    "    default_value=\"ml.m5.xlarge\"  # Training often has higher quotas\n",
    ")\n",
    "\n",
    "n_estimators = ParameterInteger(\n",
    "    name=\"NEstimators\",\n",
    "    default_value=100\n",
    ")\n",
    "\n",
    "max_depth = ParameterInteger(\n",
    "    name=\"MaxDepth\",\n",
    "    default_value=10\n",
    ")\n",
    "\n",
    "test_size = ParameterFloat(\n",
    "    name=\"TestSize\",\n",
    "    default_value=0.2\n",
    ")\n",
    "\n",
    "accuracy_threshold = ParameterFloat(\n",
    "    name=\"AccuracyThreshold\",\n",
    "    default_value=0.75\n",
    ")\n",
    "\n",
    "model_approval_status = ParameterString(\n",
    "    name=\"ModelApprovalStatus\",\n",
    "    default_value=\"PendingManualApproval\"\n",
    ")\n",
    "\n",
    "print(\"Param√®tres du pipeline d√©finis:\")\n",
    "print(f\"- ProcessingInstanceType: {processing_instance_type.default_value}\")\n",
    "print(f\"- TrainingInstanceType: {training_instance_type.default_value}\")\n",
    "print(f\"- NEstimators: {n_estimators.default_value}\")\n",
    "print(f\"- MaxDepth: {max_depth.default_value}\")\n",
    "print(f\"- TestSize: {test_size.default_value}\")\n",
    "print(f\"- AccuracyThreshold: {accuracy_threshold.default_value}\")\n",
    "print(f\"- ModelApprovalStatus: {model_approval_status.default_value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47d334e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 3: √âtape de Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61271a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# √âtape 1: Processing\n",
    "# ============================================================\n",
    "\n",
    "sklearn_processor = SKLearnProcessor(\n",
    "    framework_version='1.2-1',\n",
    "    role=role,\n",
    "    instance_type=processing_instance_type,\n",
    "    instance_count=1,\n",
    "    base_job_name='pipeline-preprocessing',\n",
    "    sagemaker_session=session\n",
    ")\n",
    "\n",
    "step_process = ProcessingStep(\n",
    "    name=\"PreprocessData\",\n",
    "    processor=sklearn_processor,\n",
    "    code='pipeline_scripts/preprocessing.py',\n",
    "    job_arguments=[\n",
    "        '--test-size', str(test_size.default_value)\n",
    "    ],\n",
    "    outputs=[\n",
    "        sagemaker.processing.ProcessingOutput(\n",
    "            output_name='train',\n",
    "            source='/opt/ml/processing/train',\n",
    "            destination=f's3://{bucket}/pipeline-data/train'\n",
    "        ),\n",
    "        sagemaker.processing.ProcessingOutput(\n",
    "            output_name='test',\n",
    "            source='/opt/ml/processing/test',\n",
    "            destination=f's3://{bucket}/pipeline-data/test'\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"√âtape de preprocessing cr√©√©e\")\n",
    "print(f\"Nom: {step_process.name}\")\n",
    "print(f\"Outputs: train, test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5c58f6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 4: √âtape de Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700f3f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# √âtape 2: Training\n",
    "# ============================================================\n",
    "\n",
    "sklearn_estimator = SKLearn(\n",
    "    entry_point='pipeline_scripts/training.py',\n",
    "    framework_version='1.2-1',\n",
    "    instance_type=training_instance_type,\n",
    "    instance_count=1,\n",
    "    role=role,\n",
    "    base_job_name='pipeline-training',\n",
    "    sagemaker_session=session,\n",
    "    hyperparameters={\n",
    "        'n-estimators': n_estimators,\n",
    "        'max-depth': max_depth\n",
    "    },\n",
    "    output_path=f's3://{bucket}/pipeline-models'\n",
    ")\n",
    "\n",
    "step_train = TrainingStep(\n",
    "    name=\"TrainModel\",\n",
    "    estimator=sklearn_estimator,\n",
    "    inputs={\n",
    "        'train': sagemaker.inputs.TrainingInput(\n",
    "            s3_data=step_process.properties.ProcessingOutputConfig.Outputs['train'].S3Output.S3Uri,\n",
    "            content_type='text/csv'\n",
    "        ),\n",
    "        'test': sagemaker.inputs.TrainingInput(\n",
    "            s3_data=step_process.properties.ProcessingOutputConfig.Outputs['test'].S3Output.S3Uri,\n",
    "            content_type='text/csv'\n",
    "        )\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"√âtape de training cr√©√©e\")\n",
    "print(f\"Nom: {step_train.name}\")\n",
    "print(f\"Framework: scikit-learn\")\n",
    "print(f\"Hyperparam√®tres: n_estimators={n_estimators.default_value}, max_depth={max_depth.default_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfc7283",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 5: √âtape d'√âvaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8893681c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# √âtape 3: Evaluation\n",
    "# ============================================================\n",
    "\n",
    "evaluation_processor = SKLearnProcessor(\n",
    "    framework_version='1.2-1',\n",
    "    role=role,\n",
    "    instance_type='ml.t3.medium',\n",
    "    instance_count=1,\n",
    "    base_job_name='pipeline-evaluation',\n",
    "    sagemaker_session=session\n",
    ")\n",
    "\n",
    "evaluation_report = PropertyFile(\n",
    "    name=\"EvaluationReport\",\n",
    "    output_name=\"evaluation\",\n",
    "    path=\"evaluation.json\"\n",
    ")\n",
    "\n",
    "step_eval = ProcessingStep(\n",
    "    name=\"EvaluateModel\",\n",
    "    processor=evaluation_processor,\n",
    "    code='pipeline_scripts/evaluation.py',\n",
    "    inputs=[\n",
    "        sagemaker.processing.ProcessingInput(\n",
    "            source=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "            destination='/opt/ml/processing/model',\n",
    "            input_name='model'\n",
    "        ),\n",
    "        sagemaker.processing.ProcessingInput(\n",
    "            source=step_process.properties.ProcessingOutputConfig.Outputs['test'].S3Output.S3Uri,\n",
    "            destination='/opt/ml/processing/test',\n",
    "            input_name='test'\n",
    "        )\n",
    "    ],\n",
    "    outputs=[\n",
    "        sagemaker.processing.ProcessingOutput(\n",
    "            output_name='evaluation',\n",
    "            source='/opt/ml/processing/evaluation',\n",
    "            destination=f's3://{bucket}/pipeline-evaluation'\n",
    "        )\n",
    "    ],\n",
    "    property_files=[evaluation_report]\n",
    ")\n",
    "\n",
    "print(\"√âtape d'√©valuation cr√©√©e\")\n",
    "print(f\"Nom: {step_eval.name}\")\n",
    "print(f\"Property file: {evaluation_report.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7422c4be",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 6: Enregistrement dans Model Registry\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa185159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# √âtape 4: Register Model\n",
    "# ============================================================\n",
    "\n",
    "model_package_group_name = 'pipeline-fraud-detection-models'\n",
    "\n",
    "# Cr√©er le Model Package Group s'il n'existe pas\n",
    "try:\n",
    "    sm_client.create_model_package_group(\n",
    "        ModelPackageGroupName=model_package_group_name,\n",
    "        ModelPackageGroupDescription='Models from SageMaker Pipeline'\n",
    "    )\n",
    "    print(f\"Model Package Group cr√©√©: {model_package_group_name}\")\n",
    "except:\n",
    "    print(f\"Model Package Group existe d√©j√†: {model_package_group_name}\")\n",
    "\n",
    "# D√©finir les m√©triques du mod√®le\n",
    "# Utiliser Join() pour concat√©ner les variables Pipeline\n",
    "model_metrics = ModelMetrics(\n",
    "    model_statistics=MetricsSource(\n",
    "        s3_uri=Join(\n",
    "            on=\"/\",\n",
    "            values=[\n",
    "                step_eval.properties.ProcessingOutputConfig.Outputs['evaluation'].S3Output.S3Uri,\n",
    "                \"evaluation.json\"\n",
    "            ]\n",
    "        ),\n",
    "        content_type=\"application/json\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# √âtape d'enregistrement\n",
    "step_register = RegisterModel(\n",
    "    name=\"RegisterModel\",\n",
    "    estimator=sklearn_estimator,\n",
    "    model_data=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    content_types=[\"text/csv\"],\n",
    "    response_types=[\"text/csv\"],\n",
    "    inference_instances=[\"ml.c5.xlarge\", \"ml.m5.xlarge\"],\n",
    "    transform_instances=[\"ml.c5.xlarge\"],\n",
    "    model_package_group_name=model_package_group_name,\n",
    "    approval_status=model_approval_status,\n",
    "    model_metrics=model_metrics\n",
    ")\n",
    "\n",
    "print(\"√âtape d'enregistrement cr√©√©e\")\n",
    "print(f\"Model Package Group: {model_package_group_name}\")\n",
    "print(f\"Approval status: {model_approval_status.default_value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a17e55",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 7: Condition de D√©ploiement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53722a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# √âtape 5: Conditional Step\n",
    "# ============================================================\n",
    "\n",
    "# Condition: d√©ployer seulement si accuracy >= threshold\n",
    "cond_gte_threshold = ConditionGreaterThanOrEqualTo(\n",
    "    left=JsonGet(\n",
    "        step_name=step_eval.name,\n",
    "        property_file=evaluation_report,\n",
    "        json_path=\"classification_metrics.accuracy.value\"\n",
    "    ),\n",
    "    right=accuracy_threshold\n",
    ")\n",
    "\n",
    "step_cond = ConditionStep(\n",
    "    name=\"CheckAccuracyThreshold\",\n",
    "    conditions=[cond_gte_threshold],\n",
    "    if_steps=[step_register],\n",
    "    else_steps=[]\n",
    ")\n",
    "\n",
    "print(\"Condition step cr√©√©e\")\n",
    "print(f\"Condition: accuracy >= {accuracy_threshold.default_value}\")\n",
    "print(f\"If true: enregistrer le mod√®le\")\n",
    "print(f\"If false: skip registration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5fdeaa",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 8: Cr√©er et Ex√©cuter le Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1d3a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cr√©er le pipeline\n",
    "# ============================================================\n",
    "\n",
    "pipeline_name = f'fraud-detection-pipeline-{int(time.time())}'\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        processing_instance_type,\n",
    "        training_instance_type,\n",
    "        n_estimators,\n",
    "        max_depth,\n",
    "        test_size,\n",
    "        accuracy_threshold,\n",
    "        model_approval_status\n",
    "    ],\n",
    "    steps=[\n",
    "        step_process,\n",
    "        step_train,\n",
    "        step_eval,\n",
    "        step_cond\n",
    "    ],\n",
    "    sagemaker_session=session\n",
    ")\n",
    "\n",
    "print(f\"Pipeline cr√©√©: {pipeline_name}\")\n",
    "print(f\"\\n√âtapes du pipeline:\")\n",
    "print(f\"1. {step_process.name}\")\n",
    "print(f\"2. {step_train.name}\")\n",
    "print(f\"3. {step_eval.name}\")\n",
    "print(f\"4. {step_cond.name}\")\n",
    "print(f\"   ‚îî‚îÄ {step_register.name} (si accuracy >= threshold)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2444a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cr√©er ou mettre √† jour le pipeline\n",
    "# ============================================================\n",
    "\n",
    "# Upsert le pipeline\n",
    "pipeline.upsert(role_arn=role)\n",
    "print(f\"\\nPipeline '{pipeline_name}' cr√©√© dans SageMaker\")\n",
    "\n",
    "# Afficher la d√©finition\n",
    "pipeline_definition = json.loads(pipeline.definition())\n",
    "print(f\"\\nNombre d'√©tapes: {len(pipeline_definition['Steps'])}\")\n",
    "print(f\"Nombre de param√®tres: {len(pipeline_definition['Parameters'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08539f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Ex√©cuter le pipeline\n",
    "# ============================================================\n",
    "\n",
    "execution = pipeline.start(\n",
    "    parameters={\n",
    "        'NEstimators': 50,\n",
    "        'MaxDepth': 10,\n",
    "        'AccuracyThreshold': 0.7\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"\\nEx√©cution du pipeline d√©marr√©e\")\n",
    "print(f\"Execution ARN: {execution.arn}\")\n",
    "execution_name = execution.arn.split('/')[-1]\n",
    "print(f\"Nom: {execution_name}\")\n",
    "print(f\"\\nVous pouvez suivre l'ex√©cution dans la console SageMaker:\")\n",
    "print(f\"https://console.aws.amazon.com/sagemaker/home?region={region}#/pipelines/{pipeline_name}/executions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8adddf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Attendre la fin de l'ex√©cution (optionnel)\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nAttente de la fin de l'ex√©cution...\")\n",
    "\n",
    "try:\n",
    "    execution.wait()\n",
    "    print(\"\\nEx√©cution termin√©e!\")\n",
    "    print(f\"Status: {execution.describe()['PipelineExecutionStatus']}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ö†Ô∏è  Erreur lors de l'ex√©cution du pipeline: {e}\")\n",
    "    print(\"\\nDiagnostic des erreurs...\")\n",
    "    \n",
    "    # Obtenir les d√©tails de l'ex√©cution\n",
    "    execution_details = execution.describe()\n",
    "    print(f\"\\nStatus: {execution_details['PipelineExecutionStatus']}\")\n",
    "    \n",
    "    # Lister les √©tapes et trouver celle qui a √©chou√©\n",
    "    steps_list = execution.list_steps()\n",
    "    \n",
    "    print(\"\\nüìã √âtat des √©tapes:\")\n",
    "    for step in steps_list:\n",
    "        status = step['StepStatus']\n",
    "        step_name = step['StepName']\n",
    "        \n",
    "        if status == 'Failed':\n",
    "            print(f\"\\n‚ùå {step_name}: FAILED\")\n",
    "            \n",
    "            # Afficher les d√©tails de l'erreur\n",
    "            if 'FailureReason' in step:\n",
    "                print(f\"   Raison: {step['FailureReason']}\")\n",
    "            \n",
    "            # Afficher les m√©tadonn√©es pour obtenir le nom du job\n",
    "            if 'Metadata' in step:\n",
    "                if 'ProcessingJob' in step['Metadata']:\n",
    "                    job_name = step['Metadata']['ProcessingJob']['Arn'].split('/')[-1]\n",
    "                    print(f\"   Processing Job: {job_name}\")\n",
    "                    \n",
    "                    # R√©cup√©rer les logs du processing job\n",
    "                    try:\n",
    "                        job_details = sm_client.describe_processing_job(ProcessingJobName=job_name)\n",
    "                        if 'FailureReason' in job_details:\n",
    "                            print(f\"   D√©tails: {job_details['FailureReason']}\")\n",
    "                    except:\n",
    "                        pass\n",
    "                        \n",
    "                elif 'TrainingJob' in step['Metadata']:\n",
    "                    job_name = step['Metadata']['TrainingJob']['Arn'].split('/')[-1]\n",
    "                    print(f\"   Training Job: {job_name}\")\n",
    "                    \n",
    "                    # R√©cup√©rer les logs du training job\n",
    "                    try:\n",
    "                        job_details = sm_client.describe_training_job(TrainingJobName=job_name)\n",
    "                        if 'FailureReason' in job_details:\n",
    "                            print(f\"   D√©tails: {job_details['FailureReason']}\")\n",
    "                    except:\n",
    "                        pass\n",
    "        else:\n",
    "            print(f\"‚úÖ {step_name}: {status}\")\n",
    "    \n",
    "    print(\"\\nüí° Conseil: V√©rifiez les logs CloudWatch pour plus de d√©tails sur l'erreur.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6dbc9d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 9: Analyser les R√©sultats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeba4cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Analyser l'ex√©cution\n",
    "# ============================================================\n",
    "\n",
    "# R√©cup√©rer les d√©tails de l'ex√©cution\n",
    "execution_details = execution.describe()\n",
    "\n",
    "print(\"D√©tails de l'ex√©cution:\")\n",
    "print(f\"Status: {execution_details['PipelineExecutionStatus']}\")\n",
    "print(f\"Start Time: {execution_details['CreationTime']}\")\n",
    "\n",
    "if 'LastModifiedTime' in execution_details:\n",
    "    duration = execution_details['LastModifiedTime'] - execution_details['CreationTime']\n",
    "    print(f\"Dur√©e totale: {duration}\")\n",
    "\n",
    "# Lister les √©tapes\n",
    "print(\"\\n\\n√âtapes ex√©cut√©es:\")\n",
    "steps_list = execution.list_steps()\n",
    "\n",
    "for step in steps_list:\n",
    "    print(f\"\\n{step['StepName']}:\")\n",
    "    print(f\"  Status: {step['StepStatus']}\")\n",
    "    \n",
    "    if 'StartTime' in step and 'EndTime' in step:\n",
    "        duration = step['EndTime'] - step['StartTime']\n",
    "        print(f\"  Dur√©e: {duration}\")\n",
    "    \n",
    "    if 'Metadata' in step:\n",
    "        if 'ProcessingJob' in step['Metadata']:\n",
    "            print(f\"  Processing Job: {step['Metadata']['ProcessingJob']['Arn'].split('/')[-1]}\")\n",
    "        elif 'TrainingJob' in step['Metadata']:\n",
    "            print(f\"  Training Job: {step['Metadata']['TrainingJob']['Arn'].split('/')[-1]}\")\n",
    "        elif 'RegisterModel' in step['Metadata']:\n",
    "            print(f\"  Model Package: {step['Metadata']['RegisterModel']['Arn'].split('/')[-1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96af357d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# R√©cup√©rer les m√©triques d'√©valuation\n",
    "# ============================================================\n",
    "\n",
    "import boto3\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Trouver le fichier d'√©valuation\n",
    "evaluation_s3_path = f's3://{bucket}/pipeline-evaluation/'\n",
    "\n",
    "print(\"M√©triques d'√©valuation:\")\n",
    "\n",
    "try:\n",
    "    # Lister les objets dans le bucket\n",
    "    response = s3.list_objects_v2(\n",
    "        Bucket=bucket,\n",
    "        Prefix='pipeline-evaluation/'\n",
    "    )\n",
    "    \n",
    "    if 'Contents' in response:\n",
    "        # Prendre le fichier le plus r√©cent\n",
    "        latest_file = sorted(response['Contents'], key=lambda x: x['LastModified'])[-1]\n",
    "        \n",
    "        # T√©l√©charger et lire le fichier\n",
    "        obj = s3.get_object(Bucket=bucket, Key=latest_file['Key'])\n",
    "        evaluation_metrics = json.loads(obj['Body'].read().decode('utf-8'))\n",
    "        \n",
    "        print(json.dumps(evaluation_metrics, indent=2))\n",
    "        \n",
    "        # Extraire accuracy\n",
    "        accuracy = evaluation_metrics['classification_metrics']['accuracy']['value']\n",
    "        print(f\"\\n\\nAccuracy du mod√®le: {accuracy:.4f}\")\n",
    "        \n",
    "        if accuracy >= accuracy_threshold.default_value:\n",
    "            print(f\"‚úÖ Mod√®le enregistr√© (accuracy >= {accuracy_threshold.default_value})\")\n",
    "        else:\n",
    "            print(f\"‚ùå Mod√®le non enregistr√© (accuracy < {accuracy_threshold.default_value})\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"Impossible de r√©cup√©rer les m√©triques: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316078c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# V√©rifier les mod√®les enregistr√©s\n",
    "# ============================================================\n",
    "\n",
    "# Lister les mod√®les dans le Model Package Group\n",
    "model_packages = sm_client.list_model_packages(\n",
    "    ModelPackageGroupName=model_package_group_name,\n",
    "    SortBy='CreationTime',\n",
    "    SortOrder='Descending',\n",
    "    MaxResults=5\n",
    ")\n",
    "\n",
    "print(f\"\\nMod√®les dans '{model_package_group_name}':\")\n",
    "print(f\"{'Version':<10} {'Status':<25} {'Date':<20}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "for package in model_packages['ModelPackageSummaryList']:\n",
    "    version = package['ModelPackageVersion']\n",
    "    status = package['ModelApprovalStatus']\n",
    "    date = package['CreationTime'].strftime('%Y-%m-%d %H:%M:%S')\n",
    "    print(f\"{version:<10} {status:<25} {date:<20}\")\n",
    "\n",
    "if len(model_packages['ModelPackageSummaryList']) > 0:\n",
    "    latest_package = model_packages['ModelPackageSummaryList'][0]\n",
    "    print(f\"\\n\\nDernier mod√®le enregistr√©:\")\n",
    "    print(f\"ARN: {latest_package['ModelPackageArn']}\")\n",
    "    print(f\"Status: {latest_package['ModelApprovalStatus']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad442b6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 10: Gestion du Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95870bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Re-ex√©cuter le pipeline avec diff√©rents param√®tres\n",
    "# ============================================================\n",
    "\n",
    "print(\"Ex√©cution du pipeline avec n_estimators=100...\")\n",
    "\n",
    "execution2 = pipeline.start(\n",
    "    parameters={\n",
    "        'NEstimators': 100,\n",
    "        'MaxDepth': 15,\n",
    "        'AccuracyThreshold': 0.75\n",
    "    }\n",
    ")\n",
    "\n",
    "execution2_name = execution2.arn.split('/')[-1]\n",
    "print(f\"Nouvelle ex√©cution d√©marr√©e: {execution2_name}\")\n",
    "print(\"Le pipeline s'ex√©cute en arri√®re-plan...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2380795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Lister toutes les ex√©cutions\n",
    "# ============================================================\n",
    "\n",
    "executions_response = sm_client.list_pipeline_executions(\n",
    "    PipelineName=pipeline_name,\n",
    "    SortBy='CreationTime',\n",
    "    SortOrder='Descending',\n",
    "    MaxResults=10\n",
    ")\n",
    "\n",
    "print(f\"\\nEx√©cutions du pipeline '{pipeline_name}':\")\n",
    "print(f\"{'Nom':<40} {'Status':<20} {'Date':<20}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for exec_summary in executions_response['PipelineExecutionSummaries']:\n",
    "    name = exec_summary['PipelineExecutionArn'].split('/')[-1]\n",
    "    status = exec_summary['PipelineExecutionStatus']\n",
    "    date = exec_summary['StartTime'].strftime('%Y-%m-%d %H:%M:%S')\n",
    "    print(f\"{name:<40} {status:<20} {date:<20}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237c0e81",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 11: Cleanup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4a986c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cleanup (optionnel)\n",
    "# ============================================================\n",
    "\n",
    "# Option 1: Supprimer le pipeline\n",
    "# sm_client.delete_pipeline(PipelineName=pipeline_name)\n",
    "# print(f\"Pipeline '{pipeline_name}' supprim√©\")\n",
    "\n",
    "# Option 2: Supprimer les artefacts S3 (attention!)\n",
    "# s3 = boto3.resource('s3')\n",
    "# bucket_obj = s3.Bucket(bucket)\n",
    "# bucket_obj.objects.filter(Prefix='pipeline-data/').delete()\n",
    "# bucket_obj.objects.filter(Prefix='pipeline-models/').delete()\n",
    "# bucket_obj.objects.filter(Prefix='pipeline-evaluation/').delete()\n",
    "\n",
    "print(\"\\nCleanup:\")\n",
    "print(\"- Pipeline conserv√© pour r√©f√©rence\")\n",
    "print(\"- Artefacts S3 conserv√©s\")\n",
    "print(f\"\\nPour supprimer manuellement:\")\n",
    "print(f\"  sm_client.delete_pipeline(PipelineName='{pipeline_name}')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6075109b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## R√©sum√©\n",
    "\n",
    "Dans ce lab, vous avez:\n",
    "\n",
    "1. **Cr√©√© un pipeline SageMaker complet** avec 5 √©tapes\n",
    "2. **Impl√©ment√© le preprocessing** des donn√©es\n",
    "3. **Entra√Æn√© un mod√®le** avec hyperparam√®tres configurables\n",
    "4. **√âvalu√© le mod√®le** automatiquement\n",
    "5. **Ajout√© une condition** pour l'enregistrement automatique\n",
    "6. **Enregistr√© le mod√®le** dans Model Registry\n",
    "7. **Ex√©cut√© et monitor√©** le pipeline\n",
    "\n",
    "### Architecture du Pipeline\n",
    "\n",
    "```\n",
    "Input Parameters\n",
    "      ‚Üì\n",
    "[1. PreprocessData] ‚Üí train.csv, test.csv\n",
    "      ‚Üì\n",
    "[2. TrainModel] ‚Üí model.tar.gz\n",
    "      ‚Üì\n",
    "[3. EvaluateModel] ‚Üí evaluation.json\n",
    "      ‚Üì\n",
    "[4. CheckAccuracyThreshold]\n",
    "      ‚Üì (if accuracy >= threshold)\n",
    "[5. RegisterModel] ‚Üí Model Registry\n",
    "```\n",
    "\n",
    "### Avantages des Pipelines\n",
    "\n",
    "- **Automatisation**: Bout-en-bout, reproductible\n",
    "- **Versioning**: Tous les artefacts sont versionn√©s\n",
    "- **Param√©trage**: Hyperparam√®tres configurables\n",
    "- **Conditions**: Logique de d√©cision automatique\n",
    "- **Tra√ßabilit√©**: Lineage complet\n",
    "- **Scalabilit√©**: Ex√©cution parall√®le possible\n",
    "\n",
    "### Cas d'Usage\n",
    "\n",
    "1. **CI/CD pour ML**: Automatiser re-training\n",
    "2. **Experimentation**: Tester plusieurs hyperparam√®tres\n",
    "3. **Production**: D√©ploiement automatique si qualit√© OK\n",
    "4. **Audit**: Tra√ßabilit√© compl√®te des mod√®les\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Lab 8: Deployment Strategies (Blue/Green, Canary)\n",
    "- Ajouter des tests de qualit√© de donn√©es\n",
    "- Impl√©menter Model Monitor\n",
    "- Int√©grer avec EventBridge pour scheduling\n",
    "\n",
    "---\n",
    "\n",
    "**Best Practice**: Utiliser les pipelines pour tout workflow ML r√©p√©titif\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
