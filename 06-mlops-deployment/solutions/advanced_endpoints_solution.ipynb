{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5df7d0a8",
   "metadata": {},
   "source": [
    "## Section 1: Environment Setup\n",
    "\n",
    "We'll reuse the model from Lab 5 or create a new one for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbb0d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Universal SageMaker Configuration\n",
    "# Compatible with Studio, Notebook Instances, and Local\n",
    "# ============================================================\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add project root to path\n",
    "project_root = os.path.abspath('../..')\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# SageMaker imports\n",
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.serverless import ServerlessInferenceConfig\n",
    "from sagemaker.async_inference import AsyncInferenceConfig\n",
    "from sagemaker.multidatamodel import MultiDataModel\n",
    "from sagemaker.transformer import Transformer\n",
    "\n",
    "# Data science imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Configuration\n",
    "try:\n",
    "    from utils.sagemaker_config import get_sagemaker_config\n",
    "    config = get_sagemaker_config(s3_prefix='lab6-endpoints')\n",
    "    role = config['role']\n",
    "    session = config['session']\n",
    "    bucket = config['bucket']\n",
    "    region = config['region']\n",
    "except ImportError:\n",
    "    print(\"Using fallback configuration method\")\n",
    "    role = get_execution_role()\n",
    "    session = sagemaker.Session()\n",
    "    bucket = session.default_bucket()\n",
    "    region = session.boto_region_name\n",
    "\n",
    "print(\"Configuration complete.\")\n",
    "print(f\"Region: {region}\")\n",
    "print(f\"S3 Bucket: s3://{bucket}\")\n",
    "print(f\"IAM Role: {role[:50]}...\")\n",
    "\n",
    "sm_client = boto3.client('sagemaker', region_name=region)\n",
    "s3_client = boto3.client('s3', region_name=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ad2b2e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 2: Prepare Model Artifacts\n",
    "\n",
    "For this lab, we'll use a pre-trained scikit-learn model. In a production scenario, this would come from your Model Registry.\n",
    "\n",
    "### Model Artifact Requirements\n",
    "\n",
    "A valid SageMaker model artifact must include:\n",
    "- Serialized model file (e.g., `model.pkl`)\n",
    "- Inference code in `code/` directory\n",
    "- Dependencies in `code/requirements.txt`\n",
    "- Packaged as `model.tar.gz`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0896e136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Create a Simple Model Artifact\n",
    "# ============================================================\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "import joblib\n",
    "import tarfile\n",
    "import shutil\n",
    "\n",
    "# Create a simple trained model\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
    "model = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Create directory structure\n",
    "model_dir = \"model_artifact\"\n",
    "code_dir = os.path.join(model_dir, \"code\")\n",
    "os.makedirs(code_dir, exist_ok=True)\n",
    "\n",
    "# Save model\n",
    "joblib.dump(model, os.path.join(model_dir, \"model.pkl\"))\n",
    "\n",
    "# Create inference script\n",
    "inference_code = '''\n",
    "import os\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    \"\"\"Load the model\"\"\"\n",
    "    return joblib.load(os.path.join(model_dir, \"model.pkl\"))\n",
    "\n",
    "def input_fn(request_body, content_type):\n",
    "    \"\"\"Parse input data\"\"\"\n",
    "    if content_type == \"application/json\":\n",
    "        data = json.loads(request_body)\n",
    "        return np.array(data[\"features\"]).reshape(1, -1)\n",
    "    raise ValueError(f\"Unsupported content type: {content_type}\")\n",
    "\n",
    "def predict_fn(input_data, model):\n",
    "    \"\"\"Run prediction\"\"\"\n",
    "    prediction = model.predict(input_data)\n",
    "    probability = model.predict_proba(input_data)\n",
    "    return {\n",
    "        \"prediction\": int(prediction[0]),\n",
    "        \"probability\": float(probability[0][1]),\n",
    "        \"confidence\": float(max(probability[0]))\n",
    "    }\n",
    "\n",
    "def output_fn(prediction, accept_type):\n",
    "    \"\"\"Format output\"\"\"\n",
    "    if accept_type == \"application/json\":\n",
    "        return json.dumps(prediction), accept_type\n",
    "    raise ValueError(f\"Unsupported accept type: {accept_type}\")\n",
    "'''\n",
    "\n",
    "with open(os.path.join(code_dir, \"inference.py\"), \"w\") as f:\n",
    "    f.write(inference_code)\n",
    "\n",
    "# Create requirements.txt\n",
    "with open(os.path.join(code_dir, \"requirements.txt\"), \"w\") as f:\n",
    "    f.write(\"scikit-learn==1.3.0\\nnumpy==1.24.3\\njoblib==1.3.1\\n\")\n",
    "\n",
    "# Create tar.gz\n",
    "tar_path = \"model.tar.gz\"\n",
    "with tarfile.open(tar_path, \"w:gz\") as tar:\n",
    "    tar.add(model_dir, arcname=\".\")\n",
    "\n",
    "# Upload to S3\n",
    "model_s3_uri = f\"s3://{bucket}/lab6-models/model.tar.gz\"\n",
    "s3_client.upload_file(tar_path, bucket, \"lab6-models/model.tar.gz\")\n",
    "\n",
    "print(f\"Model artifact created and uploaded:\")\n",
    "print(f\"   S3 URI: {model_s3_uri}\")\n",
    "\n",
    "# Cleanup local files\n",
    "shutil.rmtree(model_dir)\n",
    "os.remove(tar_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fdb55f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 3: Real-Time Endpoints with Auto-Scaling\n",
    "\n",
    "Real-time endpoints provide synchronous predictions with low latency. They are ideal for user-facing applications requiring immediate responses.\n",
    "\n",
    "### Key Characteristics\n",
    "\n",
    "- **Latency**: Sub-100ms typical\n",
    "- **Throughput**: Configurable via instance type and count\n",
    "- **Availability**: Always-on (instances running continuously)\n",
    "- **Cost**: Pay per instance hour, regardless of utilization\n",
    "\n",
    "### Auto-Scaling Configuration\n",
    "\n",
    "Auto-scaling adjusts instance count based on load:\n",
    "- **Target Metric**: InvocationsPerInstance (default)\n",
    "- **Scale-Out**: Add instances when metric exceeds threshold\n",
    "- **Scale-In**: Remove instances when metric drops\n",
    "- **Cool-Down**: Delay before scaling action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6668206d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Deploy Real-Time Endpoint with Auto-Scaling\n",
    "# ============================================================\n",
    "\n",
    "from sagemaker.sklearn import SKLearnModel\n",
    "\n",
    "# Create model object\n",
    "# Note: inference.py is already in code/ directory inside model.tar.gz\n",
    "sklearn_model = SKLearnModel(\n",
    "    model_data=model_s3_uri,\n",
    "    role=role,\n",
    "    framework_version=\"1.2-1\",\n",
    "    py_version=\"py3\",\n",
    "    sagemaker_session=session\n",
    ")\n",
    "\n",
    "# Deploy endpoint\n",
    "realtime_endpoint_name = f\"realtime-endpoint-{int(time.time())}\"\n",
    "\n",
    "print(f\"Deploying real-time endpoint: {realtime_endpoint_name}\")\n",
    "print(\"   Instance type: ml.t2.medium\")\n",
    "print(\"   Initial instance count: 1\")\n",
    "print(\"   Deployment in progress (4-6 minutes)...\")\n",
    "\n",
    "realtime_predictor = sklearn_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.t2.medium\",\n",
    "    endpoint_name=realtime_endpoint_name\n",
    ")\n",
    "\n",
    "print(f\"\\nEndpoint deployed successfully:\")\n",
    "print(f\"   Name: {realtime_endpoint_name}\")\n",
    "print(f\"   Status: InService\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373d12ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Configure Auto-Scaling\n",
    "# ============================================================\n",
    "\n",
    "autoscaling_client = boto3.client('application-autoscaling', region_name=region)\n",
    "\n",
    "# Resource ID for the endpoint\n",
    "resource_id = f\"endpoint/{realtime_endpoint_name}/variant/AllTraffic\"\n",
    "\n",
    "# Register scalable target\n",
    "response = autoscaling_client.register_scalable_target(\n",
    "    ServiceNamespace='sagemaker',\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension='sagemaker:variant:DesiredInstanceCount',\n",
    "    MinCapacity=1,\n",
    "    MaxCapacity=3\n",
    ")\n",
    "\n",
    "print(\"Scalable target registered:\")\n",
    "print(f\"   Min instances: 1\")\n",
    "print(f\"   Max instances: 3\")\n",
    "\n",
    "# Configure scaling policy\n",
    "response = autoscaling_client.put_scaling_policy(\n",
    "    PolicyName=f\"{realtime_endpoint_name}-scaling-policy\",\n",
    "    ServiceNamespace='sagemaker',\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension='sagemaker:variant:DesiredInstanceCount',\n",
    "    PolicyType='TargetTrackingScaling',\n",
    "    TargetTrackingScalingPolicyConfiguration={\n",
    "        'TargetValue': 5.0,  # Target 5 invocations per instance\n",
    "        'PredefinedMetricSpecification': {\n",
    "            'PredefinedMetricType': 'SageMakerVariantInvocationsPerInstance'\n",
    "        },\n",
    "        'ScaleInCooldown': 300,  # 5 minutes\n",
    "        'ScaleOutCooldown': 60   # 1 minute\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"\\nAuto-scaling policy configured:\")\n",
    "print(f\"   Target: 5 invocations per instance\")\n",
    "print(f\"   Scale-out cooldown: 60 seconds\")\n",
    "print(f\"   Scale-in cooldown: 300 seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91aeaa9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Test Real-Time Endpoint\n",
    "# ============================================================\n",
    "\n",
    "# Create test data\n",
    "test_features = np.random.randn(20).tolist()\n",
    "test_data = {\"features\": test_features}\n",
    "\n",
    "print(\"Testing real-time endpoint...\")\n",
    "print(f\"\\nInput: {len(test_features)} features\")\n",
    "\n",
    "# Measure latency\n",
    "start_time = time.time()\n",
    "result = realtime_predictor.predict(test_data)\n",
    "latency_ms = (time.time() - start_time) * 1000\n",
    "\n",
    "print(f\"\\nPrediction result:\")\n",
    "print(json.dumps(result, indent=2))\n",
    "print(f\"\\nLatency: {latency_ms:.2f} ms\")\n",
    "\n",
    "# Multiple invocations for testing\n",
    "print(\"\\nRunning 10 consecutive invocations...\")\n",
    "latencies = []\n",
    "for i in range(10):\n",
    "    start = time.time()\n",
    "    realtime_predictor.predict(test_data)\n",
    "    latencies.append((time.time() - start) * 1000)\n",
    "\n",
    "print(f\"Average latency: {np.mean(latencies):.2f} ms\")\n",
    "print(f\"P50 latency: {np.percentile(latencies, 50):.2f} ms\")\n",
    "print(f\"P95 latency: {np.percentile(latencies, 95):.2f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5024464b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 4: Serverless Endpoints\n",
    "\n",
    "Serverless endpoints automatically scale from zero to handle traffic spikes without managing instances. They are ideal for development, intermittent workloads, or cost optimization.\n",
    "\n",
    "### Key Characteristics\n",
    "\n",
    "- **No Infrastructure Management**: AWS handles all scaling\n",
    "- **Cost Model**: Pay only for inference time (per request)\n",
    "- **Cold Start**: First request after idle period takes ~1-2 seconds\n",
    "- **Concurrency**: Auto-scales to handle concurrent requests\n",
    "- **Memory Configuration**: 1 GB to 6 GB per request\n",
    "\n",
    "### When to Use Serverless\n",
    "\n",
    "- Development and testing environments\n",
    "- Unpredictable or bursty traffic patterns\n",
    "- Cost-sensitive applications with low QPS\n",
    "- Prototypes and MVPs\n",
    "\n",
    "### When NOT to Use Serverless\n",
    "\n",
    "- Consistent high traffic (real-time cheaper)\n",
    "- Ultra-low latency requirements (<500ms)\n",
    "- Very large models (>6 GB memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558e89dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Deploy Serverless Endpoint\n",
    "# ============================================================\n",
    "\n",
    "# Configure serverless inference\n",
    "serverless_config = ServerlessInferenceConfig(\n",
    "    memory_size_in_mb=2048,  # 2 GB memory\n",
    "    max_concurrency=10        # Handle up to 10 concurrent requests\n",
    ")\n",
    "\n",
    "# Create new model object for serverless\n",
    "serverless_model = SKLearnModel(\n",
    "    model_data=model_s3_uri,\n",
    "    role=role,\n",
    "    framework_version=\"1.2-1\",\n",
    "    py_version=\"py3\",\n",
    "    sagemaker_session=session\n",
    ")\n",
    "\n",
    "# Deploy serverless endpoint\n",
    "serverless_endpoint_name = f\"serverless-endpoint-{int(time.time())}\"\n",
    "\n",
    "print(f\"Deploying serverless endpoint: {serverless_endpoint_name}\")\n",
    "print(\"   Memory: 2048 MB\")\n",
    "print(\"   Max concurrency: 10\")\n",
    "print(\"   Deployment in progress (4-6 minutes)...\")\n",
    "\n",
    "serverless_predictor = serverless_model.deploy(\n",
    "    serverless_inference_config=serverless_config,\n",
    "    endpoint_name=serverless_endpoint_name\n",
    ")\n",
    "\n",
    "print(f\"\\nServerless endpoint deployed successfully:\")\n",
    "print(f\"   Name: {serverless_endpoint_name}\")\n",
    "print(f\"   Status: InService\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7596f39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Test Serverless Endpoint (Cold Start vs Warm)\n",
    "# ============================================================\n",
    "\n",
    "# Test cold start (first request after deployment)\n",
    "print(\"Testing serverless endpoint...\")\n",
    "print(\"\\n1. Cold start (first request):\")\n",
    "start_time = time.time()\n",
    "result = serverless_predictor.predict(test_data)\n",
    "cold_start_latency = (time.time() - start_time) * 1000\n",
    "\n",
    "print(f\"   Latency: {cold_start_latency:.2f} ms\")\n",
    "print(f\"   Result: {result}\")\n",
    "\n",
    "# Test warm invocations\n",
    "print(\"\\n2. Warm invocations (subsequent requests):\")\n",
    "warm_latencies = []\n",
    "for i in range(5):\n",
    "    start = time.time()\n",
    "    serverless_predictor.predict(test_data)\n",
    "    warm_latencies.append((time.time() - start) * 1000)\n",
    "    time.sleep(0.1)  # Small delay between requests\n",
    "\n",
    "print(f\"   Average latency: {np.mean(warm_latencies):.2f} ms\")\n",
    "print(f\"   Min latency: {np.min(warm_latencies):.2f} ms\")\n",
    "print(f\"   Max latency: {np.max(warm_latencies):.2f} ms\")\n",
    "\n",
    "print(f\"\\nCold start overhead: {cold_start_latency - np.mean(warm_latencies):.2f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400efac6",
   "metadata": {},
   "source": [
    "### Cost Comparison: Real-Time vs Serverless\n",
    "\n",
    "Let's estimate costs for different usage patterns:\n",
    "\n",
    "**Real-Time Endpoint (ml.t2.medium: $0.065/hour)**\n",
    "- Always-on cost: $0.065/hr √ó 730 hrs/month = **$47.50/month**\n",
    "- Best for: >1000 requests/hour\n",
    "\n",
    "**Serverless Endpoint ($0.00002 per inference second + $0.60 per GB-hour)**\n",
    "- 100 requests/day, 50ms each: ~$0.10/month\n",
    "- 1000 requests/day, 50ms each: ~$1.00/month\n",
    "- 10000 requests/day, 50ms each: ~$10/month\n",
    "- Best for: <1000 requests/hour\n",
    "\n",
    "**Rule of Thumb**: Serverless is cheaper for < 1000 requests/hour. Above that, real-time becomes more cost-effective.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391c7a48",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 5: Asynchronous Endpoints\n",
    "\n",
    "Asynchronous endpoints handle long-running inference requests (>60 seconds) by queueing requests and returning results via S3.\n",
    "\n",
    "### Key Characteristics\n",
    "\n",
    "- **Request Model**: Queue-based (SQS)\n",
    "- **Response Model**: S3 notification (SNS optional)\n",
    "- **Timeout**: Up to 1 hour per request\n",
    "- **Throughput**: High (queue buffers requests)\n",
    "- **Cost**: Pay per instance hour + S3 storage\n",
    "\n",
    "### Use Cases\n",
    "\n",
    "- Large document processing (OCR, PDF parsing)\n",
    "- Video/audio analysis\n",
    "- Batch-like workloads with SLA flexibility\n",
    "- Expensive model inference (LLMs, computer vision)\n",
    "\n",
    "### Architecture\n",
    "\n",
    "```\n",
    "Client Request ‚Üí API Gateway ‚Üí SageMaker Async Endpoint\n",
    "                                    ‚Üì\n",
    "                               SQS Queue\n",
    "                                    ‚Üì\n",
    "                            ML Model Inference\n",
    "                                    ‚Üì\n",
    "                          S3 (Result Output)\n",
    "                                    ‚Üì\n",
    "                          SNS Notification (optional)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c57da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Deploy Asynchronous Endpoint\n",
    "# ============================================================\n",
    "\n",
    "# Configure async inference\n",
    "async_config = AsyncInferenceConfig(\n",
    "    output_path=f\"s3://{bucket}/lab6-async-results/\",\n",
    "    max_concurrent_invocations_per_instance=4,\n",
    "    failure_path=f\"s3://{bucket}/lab6-async-failures/\"\n",
    ")\n",
    "\n",
    "# Create model for async endpoint\n",
    "async_model = SKLearnModel(\n",
    "    model_data=model_s3_uri,\n",
    "    role=role,\n",
    "    framework_version=\"1.2-1\",\n",
    "    py_version=\"py3\",\n",
    "    sagemaker_session=session\n",
    ")\n",
    "\n",
    "# Deploy async endpoint\n",
    "async_endpoint_name = f\"async-endpoint-{int(time.time())}\"\n",
    "\n",
    "print(f\"Deploying async endpoint: {async_endpoint_name}\")\n",
    "print(\"   Instance type: ml.t2.medium\")\n",
    "print(\"   Max concurrent invocations: 4\")\n",
    "print(f\"   Output path: s3://{bucket}/lab6-async-results/\")\n",
    "print(\"   Deployment in progress (4-6 minutes)...\")\n",
    "\n",
    "async_predictor = async_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.t2.medium\",\n",
    "    async_inference_config=async_config,\n",
    "    endpoint_name=async_endpoint_name\n",
    ")\n",
    "\n",
    "print(f\"\\nAsync endpoint deployed successfully:\")\n",
    "print(f\"   Name: {async_endpoint_name}\")\n",
    "print(f\"   Status: InService\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e945b193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Test Asynchronous Endpoint\n",
    "# ============================================================\n",
    "\n",
    "# Upload input data to S3\n",
    "input_data_key = \"lab6-async-input/test_input.json\"\n",
    "s3_client.put_object(\n",
    "    Bucket=bucket,\n",
    "    Key=input_data_key,\n",
    "    Body=json.dumps(test_data)\n",
    ")\n",
    "input_s3_uri = f\"s3://{bucket}/{input_data_key}\"\n",
    "\n",
    "print(f\"Test input uploaded: {input_s3_uri}\")\n",
    "\n",
    "# Invoke async endpoint\n",
    "print(\"\\nInvoking async endpoint...\")\n",
    "response = async_predictor.predict_async(input_path=input_s3_uri)\n",
    "\n",
    "# Get output location\n",
    "output_location = response.output_path\n",
    "print(f\"Request accepted. Output will be at:\")\n",
    "print(f\"   {output_location}\")\n",
    "\n",
    "# Wait for result\n",
    "print(\"\\nWaiting for inference to complete...\")\n",
    "import time as time_module\n",
    "max_wait = 60  # seconds\n",
    "elapsed = 0\n",
    "result_found = False\n",
    "\n",
    "while elapsed < max_wait:\n",
    "    try:\n",
    "        # Check if output exists\n",
    "        output_key = output_location.replace(f\"s3://{bucket}/\", \"\")\n",
    "        response = s3_client.head_object(Bucket=bucket, Key=output_key)\n",
    "        result_found = True\n",
    "        break\n",
    "    except s3_client.exceptions.ClientError:\n",
    "        time_module.sleep(5)\n",
    "        elapsed += 5\n",
    "        print(f\"   Waiting... ({elapsed}s)\")\n",
    "\n",
    "if result_found:\n",
    "    # Retrieve result\n",
    "    result_obj = s3_client.get_object(Bucket=bucket, Key=output_key)\n",
    "    result = json.loads(result_obj['Body'].read().decode('utf-8'))\n",
    "    \n",
    "    print(f\"\\nInference complete!\")\n",
    "    print(f\"Prediction result:\")\n",
    "    print(json.dumps(result, indent=2))\n",
    "else:\n",
    "    print(f\"\\nTimeout: Result not available after {max_wait} seconds\")\n",
    "    print(\"Check S3 output location later for result\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6020757f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 6: Multi-Model Endpoints\n",
    "\n",
    "Multi-Model Endpoints (MME) allow hosting multiple models on a single endpoint, sharing compute resources efficiently.\n",
    "\n",
    "### Key Characteristics\n",
    "\n",
    "- **Model Loading**: Dynamic (on-demand)\n",
    "- **Model Caching**: LRU cache in instance memory\n",
    "- **Cost Efficiency**: Share instances across models\n",
    "- **Scalability**: 1000s of models on single endpoint\n",
    "- **Invocation**: Specify target model in request\n",
    "\n",
    "### Use Cases\n",
    "\n",
    "- Personalized models per customer/region\n",
    "- A/B testing multiple model versions\n",
    "- Multi-tenant ML applications\n",
    "- Cost optimization for many small models\n",
    "\n",
    "### Architecture\n",
    "\n",
    "```\n",
    "Client Request (specifies TargetModel)\n",
    "         ‚Üì\n",
    "  Endpoint Instance\n",
    "         ‚Üì\n",
    "    Model Cache (LRU)\n",
    "  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "  ‚îÇ Model A (hot)   ‚îÇ  ‚Üê Most recently used\n",
    "  ‚îÇ Model B (hot)   ‚îÇ\n",
    "  ‚îÇ Model C (warm)  ‚îÇ\n",
    "  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "         ‚Üì\n",
    "   S3 Model Store\n",
    "  (100s-1000s models)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47776a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Create Multiple Model Variants\n",
    "# ============================================================\n",
    "\n",
    "# For demonstration, create 3 slightly different models\n",
    "print(\"Creating multiple model variants...\")\n",
    "\n",
    "model_artifacts = []\n",
    "\n",
    "for model_id in [\"model_A\", \"model_B\", \"model_C\"]:\n",
    "    # Create a slightly different model\n",
    "    X_temp, y_temp = make_classification(\n",
    "        n_samples=1000, \n",
    "        n_features=20, \n",
    "        n_classes=2, \n",
    "        random_state=42 + ord(model_id[-1])\n",
    "    )\n",
    "    temp_model = RandomForestClassifier(\n",
    "        n_estimators=50 + 10 * ord(model_id[-1]), \n",
    "        random_state=42\n",
    "    )\n",
    "    temp_model.fit(X_temp, y_temp)\n",
    "    \n",
    "    # Create model directory\n",
    "    temp_dir = f\"model_{model_id}\"\n",
    "    os.makedirs(temp_dir, exist_ok=True)\n",
    "    joblib.dump(temp_model, os.path.join(temp_dir, \"model.pkl\"))\n",
    "    \n",
    "    # Copy inference code\n",
    "    os.makedirs(os.path.join(temp_dir, \"code\"), exist_ok=True)\n",
    "    with open(os.path.join(temp_dir, \"code\", \"inference.py\"), \"w\") as f:\n",
    "        f.write(inference_code)\n",
    "    with open(os.path.join(temp_dir, \"code\", \"requirements.txt\"), \"w\") as f:\n",
    "        f.write(\"scikit-learn==1.3.0\\nnumpy==1.24.3\\njoblib==1.3.1\\n\")\n",
    "    \n",
    "    # Create tar.gz\n",
    "    tar_name = f\"{model_id}.tar.gz\"\n",
    "    with tarfile.open(tar_name, \"w:gz\") as tar:\n",
    "        tar.add(temp_dir, arcname=\".\")\n",
    "    \n",
    "    # Upload to S3\n",
    "    s3_key = f\"lab6-mme-models/{tar_name}\"\n",
    "    s3_client.upload_file(tar_name, bucket, s3_key)\n",
    "    model_uri = f\"s3://{bucket}/{s3_key}\"\n",
    "    model_artifacts.append((model_id, model_uri))\n",
    "    \n",
    "    print(f\"   {model_id}: {model_uri}\")\n",
    "    \n",
    "    # Cleanup\n",
    "    shutil.rmtree(temp_dir)\n",
    "    os.remove(tar_name)\n",
    "\n",
    "print(f\"\\n{len(model_artifacts)} model variants created and uploaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb7f3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Deploy Multi-Model Endpoint\n",
    "# ============================================================\n",
    "\n",
    "from sagemaker.sklearn import SKLearnModel\n",
    "from sagemaker.multidatamodel import MultiDataModel\n",
    "\n",
    "# Create a base model\n",
    "base_model = SKLearnModel(\n",
    "    model_data=model_artifacts[0][1],  # Use first model as base\n",
    "    role=role,\n",
    "    framework_version=\"1.2-1\",\n",
    "    py_version=\"py3\",\n",
    "    sagemaker_session=session\n",
    ")\n",
    "\n",
    "# Create MultiDataModel\n",
    "mme_model = MultiDataModel(\n",
    "    name=f\"mme-model-{int(time.time())}\",\n",
    "    model_data_prefix=f\"s3://{bucket}/lab6-mme-models/\",\n",
    "    model=base_model,\n",
    "    sagemaker_session=session\n",
    ")\n",
    "\n",
    "# Deploy MME endpoint\n",
    "mme_endpoint_name = f\"mme-endpoint-{int(time.time())}\"\n",
    "\n",
    "print(f\"Deploying multi-model endpoint: {mme_endpoint_name}\")\n",
    "print(f\"   Model prefix: s3://{bucket}/lab6-mme-models/\")\n",
    "print(f\"   Number of models: {len(model_artifacts)}\")\n",
    "print(\"   Deployment in progress (4-6 minutes)...\")\n",
    "\n",
    "mme_predictor = mme_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.t2.medium\",\n",
    "    endpoint_name=mme_endpoint_name\n",
    ")\n",
    "\n",
    "print(f\"\\nMulti-model endpoint deployed successfully:\")\n",
    "print(f\"   Name: {mme_endpoint_name}\")\n",
    "print(f\"   Models available: {len(model_artifacts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c224c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Test Multi-Model Endpoint\n",
    "# ============================================================\n",
    "\n",
    "print(\"Testing multi-model endpoint with different models...\\n\")\n",
    "\n",
    "for model_id, model_uri in model_artifacts:\n",
    "    print(f\"Testing {model_id}:\")\n",
    "    \n",
    "    # Invoke with target model\n",
    "    start_time = time.time()\n",
    "    result = mme_predictor.predict(\n",
    "        test_data,\n",
    "        target_model=f\"{model_id}.tar.gz\"\n",
    "    )\n",
    "    latency = (time.time() - start_time) * 1000\n",
    "    \n",
    "    print(f\"   Prediction: {result['prediction']}\")\n",
    "    print(f\"   Confidence: {result['confidence']:.4f}\")\n",
    "    print(f\"   Latency: {latency:.2f} ms\")\n",
    "    print()\n",
    "\n",
    "print(\"All models invoked successfully!\")\n",
    "print(\"\\nNote: First invocation per model may be slower (model loading)\")\n",
    "print(\"Subsequent invocations use cached model in memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303ea0dd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 7: Batch Transform\n",
    "\n",
    "Batch Transform runs inference on large datasets stored in S3 without deploying a persistent endpoint. It's ideal for periodic bulk predictions.\n",
    "\n",
    "### Key Characteristics\n",
    "\n",
    "- **Execution Model**: Job-based (not persistent)\n",
    "- **Input**: S3 dataset (CSV, JSON, etc.)\n",
    "- **Output**: S3 results directory\n",
    "- **Parallelism**: Configurable instance count\n",
    "- **Cost**: Pay only during job execution\n",
    "\n",
    "### Use Cases\n",
    "\n",
    "- Periodic batch scoring (daily/weekly)\n",
    "- One-time predictions on large datasets\n",
    "- Offline model evaluation\n",
    "- Data preprocessing pipelines\n",
    "\n",
    "### Advantages over Endpoints\n",
    "\n",
    "- No idle instance costs\n",
    "- Automatic data parallelization\n",
    "- No endpoint management\n",
    "- Built-in data filtering and joining\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32c15fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Prepare Batch Input Data\n",
    "# ============================================================\n",
    "\n",
    "# Create sample batch input\n",
    "batch_size = 100\n",
    "batch_data = []\n",
    "\n",
    "for i in range(batch_size):\n",
    "    features = np.random.randn(20).tolist()\n",
    "    batch_data.append({\"features\": features})\n",
    "\n",
    "# Save as JSONL (one JSON object per line)\n",
    "batch_input_file = \"batch_input.jsonl\"\n",
    "with open(batch_input_file, \"w\") as f:\n",
    "    for item in batch_data:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "# Upload to S3\n",
    "batch_input_key = \"lab6-batch-input/batch_input.jsonl\"\n",
    "s3_client.upload_file(batch_input_file, bucket, batch_input_key)\n",
    "batch_input_s3 = f\"s3://{bucket}/{batch_input_key}\"\n",
    "\n",
    "print(f\"Batch input prepared:\")\n",
    "print(f\"   Records: {batch_size}\")\n",
    "print(f\"   S3 Location: {batch_input_s3}\")\n",
    "\n",
    "# Cleanup local file\n",
    "os.remove(batch_input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea52bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Create and Run Batch Transform Job\n",
    "# ============================================================\n",
    "\n",
    "# Create model for batch transform\n",
    "batch_model = SKLearnModel(\n",
    "    model_data=model_s3_uri,\n",
    "    role=role,\n",
    "    framework_version=\"1.2-1\",\n",
    "    py_version=\"py3\",\n",
    "    sagemaker_session=session\n",
    ")\n",
    "\n",
    "# Create transformer\n",
    "transformer = batch_model.transformer(\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.t2.medium\",\n",
    "    output_path=f\"s3://{bucket}/lab6-batch-output/\",\n",
    "    assemble_with=\"Line\",  # Combine predictions line by line\n",
    "    accept=\"application/json\"\n",
    ")\n",
    "\n",
    "# Start transform job\n",
    "job_name = f\"batch-transform-{int(time.time())}\"\n",
    "\n",
    "print(f\"Starting batch transform job: {job_name}\")\n",
    "print(f\"   Input: {batch_input_s3}\")\n",
    "print(f\"   Output: s3://{bucket}/lab6-batch-output/\")\n",
    "print(f\"   Instance: ml.t2.medium\")\n",
    "print(\"   Job in progress (3-5 minutes)...\")\n",
    "\n",
    "transformer.transform(\n",
    "    data=batch_input_s3,\n",
    "    content_type=\"application/json\",\n",
    "    split_type=\"Line\",  # Each line is a separate request\n",
    "    job_name=job_name,\n",
    "    wait=True  # Wait for job to complete\n",
    ")\n",
    "\n",
    "print(f\"\\nBatch transform job completed:\")\n",
    "print(f\"   Job name: {job_name}\")\n",
    "print(f\"   Status: Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99881603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Retrieve and Analyze Batch Results\n",
    "# ============================================================\n",
    "\n",
    "# List output files\n",
    "output_prefix = \"lab6-batch-output/\"\n",
    "response = s3_client.list_objects_v2(Bucket=bucket, Prefix=output_prefix)\n",
    "\n",
    "output_files = [obj['Key'] for obj in response.get('Contents', []) \n",
    "                if obj['Key'].endswith('.out')]\n",
    "\n",
    "print(f\"Batch transform output files: {len(output_files)}\")\n",
    "\n",
    "if output_files:\n",
    "    # Download first output file\n",
    "    output_key = output_files[0]\n",
    "    output_file = \"batch_output.jsonl\"\n",
    "    s3_client.download_file(bucket, output_key, output_file)\n",
    "    \n",
    "    print(f\"\\nDownloaded results: {output_key}\")\n",
    "    \n",
    "    # Parse results\n",
    "    results = []\n",
    "    with open(output_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            results.append(json.loads(line))\n",
    "    \n",
    "    print(f\"\\nProcessed {len(results)} predictions\")\n",
    "    print(\"\\nFirst 5 predictions:\")\n",
    "    for i, result in enumerate(results[:5], 1):\n",
    "        print(f\"   {i}. Prediction: {result['prediction']}, \"\n",
    "              f\"Confidence: {result['confidence']:.4f}\")\n",
    "    \n",
    "    # Summary statistics\n",
    "    predictions = [r['prediction'] for r in results]\n",
    "    confidences = [r['confidence'] for r in results]\n",
    "    \n",
    "    print(f\"\\nSummary Statistics:\")\n",
    "    print(f\"   Class 0 predictions: {predictions.count(0)}\")\n",
    "    print(f\"   Class 1 predictions: {predictions.count(1)}\")\n",
    "    print(f\"   Average confidence: {np.mean(confidences):.4f}\")\n",
    "    print(f\"   Min confidence: {np.min(confidences):.4f}\")\n",
    "    print(f\"   Max confidence: {np.max(confidences):.4f}\")\n",
    "    \n",
    "    # Cleanup\n",
    "    os.remove(output_file)\n",
    "else:\n",
    "    print(\"No output files found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2bcd4f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 8: Resource Cleanup\n",
    "\n",
    "Clean up all deployed resources to avoid unnecessary costs.\n",
    "\n",
    "**Important:** Endpoints continue to incur charges while running. Always delete endpoints when not in use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7126b812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Delete All Endpoints\n",
    "# ============================================================\n",
    "\n",
    "endpoints_to_delete = [\n",
    "    realtime_endpoint_name,\n",
    "    serverless_endpoint_name,\n",
    "    async_endpoint_name,\n",
    "    mme_endpoint_name\n",
    "]\n",
    "\n",
    "print(\"Deleting endpoints...\\n\")\n",
    "\n",
    "for endpoint_name in endpoints_to_delete:\n",
    "    try:\n",
    "        sm_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "        print(f\"   Deleted: {endpoint_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   Warning: Could not delete {endpoint_name}: {str(e)}\")\n",
    "\n",
    "# Also delete endpoint configurations\n",
    "print(\"\\nDeleting endpoint configurations...\")\n",
    "for endpoint_name in endpoints_to_delete:\n",
    "    try:\n",
    "        sm_client.delete_endpoint_config(EndpointConfigName=endpoint_name)\n",
    "        print(f\"   Deleted config: {endpoint_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   Warning: Could not delete config {endpoint_name}: {str(e)}\")\n",
    "\n",
    "# Delete auto-scaling policy\n",
    "print(\"\\nDeleting auto-scaling configuration...\")\n",
    "try:\n",
    "    resource_id = f\"endpoint/{realtime_endpoint_name}/variant/AllTraffic\"\n",
    "    autoscaling_client.deregister_scalable_target(\n",
    "        ServiceNamespace='sagemaker',\n",
    "        ResourceId=resource_id,\n",
    "        ScalableDimension='sagemaker:variant:DesiredInstanceCount'\n",
    "    )\n",
    "    print(\"   Auto-scaling policy deleted\")\n",
    "except Exception as e:\n",
    "    print(f\"   Warning: {str(e)}\")\n",
    "\n",
    "print(\"\\nAll resources cleaned up successfully!\")\n",
    "print(\"\\nNote: S3 objects are retained for analysis.\")\n",
    "print(\"Delete manually if needed: Console > S3 > {bucket} > lab6-*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3b188f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary and Key Learnings\n",
    "\n",
    "### What You Accomplished\n",
    "\n",
    "1. **Real-Time Endpoints**:\n",
    "   - Deployed synchronous inference endpoint\n",
    "   - Configured auto-scaling based on invocation metrics\n",
    "   - Measured sub-100ms latency performance\n",
    "\n",
    "2. **Serverless Endpoints**:\n",
    "   - Deployed pay-per-request serverless endpoint\n",
    "   - Compared cold start vs warm invocation latency\n",
    "   - Understood cost trade-offs vs real-time endpoints\n",
    "\n",
    "3. **Asynchronous Endpoints**:\n",
    "   - Deployed queue-based async endpoint\n",
    "   - Handled long-running inference requests\n",
    "   - Retrieved results from S3\n",
    "\n",
    "4. **Multi-Model Endpoints**:\n",
    "   - Hosted multiple models on single endpoint\n",
    "   - Invoked specific models by name\n",
    "   - Understood cost efficiency for many models\n",
    "\n",
    "5. **Batch Transform**:\n",
    "   - Executed bulk predictions on S3 data\n",
    "   - Processed 100 predictions without persistent endpoint\n",
    "   - Analyzed batch results\n",
    "\n",
    "### Deployment Pattern Decision Tree\n",
    "\n",
    "```\n",
    "How often is inference needed?\n",
    "‚îú‚îÄ Continuously ‚Üí Real-Time Endpoint\n",
    "‚îÇ   ‚îú‚îÄ High QPS ‚Üí Real-Time with Auto-Scaling\n",
    "‚îÇ   ‚îî‚îÄ Low/Variable QPS ‚Üí Serverless Endpoint\n",
    "‚îÇ\n",
    "‚îú‚îÄ Occasionally ‚Üí Batch Transform\n",
    "‚îÇ   ‚îî‚îÄ (For periodic bulk scoring)\n",
    "‚îÇ\n",
    "‚îî‚îÄ Request takes >60s ‚Üí Async Endpoint\n",
    "    ‚îî‚îÄ (For long-running jobs)\n",
    "\n",
    "How many models?\n",
    "‚îú‚îÄ 1 model ‚Üí Standard Endpoint\n",
    "‚îî‚îÄ 100s-1000s models ‚Üí Multi-Model Endpoint\n",
    "```\n",
    "\n",
    "### Cost Optimization Best Practices\n",
    "\n",
    "1. **Use Serverless for**:\n",
    "   - Development and testing\n",
    "   - Traffic < 1000 req/hour\n",
    "   - Unpredictable workloads\n",
    "\n",
    "2. **Use Real-Time for**:\n",
    "   - Production with consistent traffic\n",
    "   - Traffic > 1000 req/hour\n",
    "   - Ultra-low latency requirements\n",
    "\n",
    "3. **Use Batch Transform for**:\n",
    "   - Periodic scoring (daily/weekly)\n",
    "   - One-time bulk predictions\n",
    "   - No real-time requirements\n",
    "\n",
    "4. **Use Multi-Model for**:\n",
    "   - Many small models\n",
    "   - Per-customer/region models\n",
    "   - Cost-sensitive multi-tenant apps\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "**Lab 7**: SageMaker Pipelines, Experiments, and Model Explainability with Clarify\n",
    "\n",
    "**Lab 8**: Model Monitor, Blue/Green, Canary, and Shadow Deployments\n",
    "\n",
    "---\n",
    "\n",
    "## Reflection Questions\n",
    "\n",
    "1. **When** would you choose a serverless endpoint over a real-time endpoint?\n",
    "2. **How** does multi-model endpoint reduce costs compared to individual endpoints?\n",
    "3. **What** are the trade-offs between async endpoints and batch transform?\n",
    "4. **Why** is auto-scaling important for production endpoints?\n",
    "\n",
    "---\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "- [SageMaker Inference Documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/deploy-model.html)\n",
    "- [Serverless Inference Guide](https://docs.aws.amazon.com/sagemaker/latest/dg/serverless-endpoints.html)\n",
    "- [Multi-Model Endpoints](https://docs.aws.amazon.com/sagemaker/latest/dg/multi-model-endpoints.html)\n",
    "- [Batch Transform](https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11b3bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup: Delete BYOC endpoint\n",
    "try:\n",
    "    byoc_predictor.delete_endpoint(delete_endpoint_config=True)\n",
    "    print(f\"‚úÖ Deleted BYOC endpoint: {byoc_endpoint_name}\")\n",
    "except:\n",
    "    print(\"‚ö†Ô∏è  BYOC endpoint not deployed or already deleted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2b5ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BYOC: Test custom container endpoint with numeric data\n",
    "test_data_byoc = {\n",
    "    \"instances\": [np.random.randn(20).tolist()]\n",
    "}\n",
    "\n",
    "print(\"üß™ Testing BYOC endpoint...\")\n",
    "print(f\"Input: 1 instance with 20 features\")\n",
    "\n",
    "try:\n",
    "    response = byoc_predictor.predict(test_data_byoc)\n",
    "    print(\"\\n‚úÖ Prediction successful!\")\n",
    "    print(json.dumps(response, indent=2))\n",
    "    \n",
    "    print(\"\\nüîç What just happened:\")\n",
    "    print(\"   1. Your custom Flask server received the request\")\n",
    "    print(\"   2. inference.py loaded model.pkl from /opt/ml/model/\")\n",
    "    print(\"   3. Prediction ran with custom preprocessing logic\")\n",
    "    print(\"   4. Response formatted as JSON\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Error: {e}\")\n",
    "    print(\"   Tip: Ensure Docker container is built and pushed to ECR\")\n",
    "\n",
    "print(\"\"\"\n",
    "üí° Custom Container Benefits:\n",
    "   ‚úÖ Full control over inference logic\n",
    "   ‚úÖ Add proprietary preprocessing libraries\n",
    "   ‚úÖ Optimize for specific hardware (GPU, custom chips)\n",
    "   ‚úÖ Include custom authentication/logging\n",
    "   ‚úÖ Support multiple data formats (JSON, CSV, binary)\n",
    "   \n",
    "üìÇ Container Files (docker/sklearn-custom/):\n",
    "   ‚Ä¢ Dockerfile - Python 3.9 + Flask + sklearn\n",
    "   ‚Ä¢ inference.py - Generic inference (numeric & text)\n",
    "   ‚Ä¢ serve - Gunicorn entrypoint\n",
    "   ‚Ä¢ requirements.txt - Python dependencies\n",
    "   \n",
    "üîß Customization Ideas:\n",
    "   ‚Ä¢ Add caching layer (Redis)\n",
    "   ‚Ä¢ Include feature engineering pipelines\n",
    "   ‚Ä¢ Support streaming inference\n",
    "   ‚Ä¢ Add custom metrics/monitoring\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb6f111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BYOC: Deploy model using custom Docker container\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "# Use the same model we created earlier (RandomForest with 20 features)\n",
    "print(\"üê≥ Deploying with custom Docker container...\")\n",
    "print(f\"   Model artifact: {model_s3_uri}\")\n",
    "print(f\"   Container image: {image_uri}\")\n",
    "\n",
    "# Create SageMaker Model with custom container\n",
    "byoc_model = Model(\n",
    "    image_uri=image_uri,  # Our custom ECR image\n",
    "    model_data=model_s3_uri,  # Use Lab 6 model\n",
    "    role=role,\n",
    "    sagemaker_session=session,\n",
    "    name=f\"byoc-model-{int(time.time())}\"\n",
    ")\n",
    "\n",
    "# Deploy to endpoint\n",
    "byoc_endpoint_name = f\"byoc-endpoint-{int(time.time())}\"\n",
    "\n",
    "print(\"\\n‚è≥ Deploying BYOC endpoint (4-6 minutes)...\")\n",
    "print(\"   This uses YOUR custom container from ECR\")\n",
    "print(\"   Inference logic: docker/sklearn-custom/inference.py\")\n",
    "\n",
    "byoc_predictor = byoc_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.t2.medium',\n",
    "    endpoint_name=byoc_endpoint_name,\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer()\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ BYOC Endpoint deployed: {byoc_endpoint_name}\")\n",
    "print(f\"üì¶ Model: RandomForestClassifier (20 features)\")\n",
    "print(f\"üê≥ Container: Custom scikit-learn with Flask\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe352a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BYOC: Build and push custom Docker container to ECR\n",
    "import boto3\n",
    "\n",
    "# Get AWS account ID and region\n",
    "sts_client = boto3.client('sts')\n",
    "account_id = sts_client.get_caller_identity()['Account']\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "# ECR repository details\n",
    "ecr_repository = 'sagemaker-sklearn-custom'\n",
    "image_tag = 'latest'\n",
    "image_uri = f\"{account_id}.dkr.ecr.{region}.amazonaws.com/{ecr_repository}:{image_tag}\"\n",
    "\n",
    "print(f\"üì¶ Custom Container Image URI:\")\n",
    "print(f\"   {image_uri}\")\n",
    "print(f\"\\nüî® To build and push the container:\")\n",
    "print(f\"\"\"\n",
    "   cd docker/sklearn-custom\n",
    "   \n",
    "   # Build Docker image\n",
    "   docker build -t {ecr_repository}:{image_tag} .\n",
    "   \n",
    "   # Create ECR repository (if doesn't exist)\n",
    "   aws ecr create-repository --repository-name {ecr_repository} || true\n",
    "   \n",
    "   # Login to ECR\n",
    "   aws ecr get-login-password --region {region} | \\\\\n",
    "       docker login --username AWS --password-stdin {account_id}.dkr.ecr.{region}.amazonaws.com\n",
    "   \n",
    "   # Tag and push\n",
    "   docker tag {ecr_repository}:{image_tag} {image_uri}\n",
    "   docker push {image_uri}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168d0c22",
   "metadata": {},
   "source": [
    "---\n",
    "## üê≥ Bonus: Bring Your Own Container (BYOC)\n",
    "\n",
    "**Why Custom Containers?**\n",
    "- Add custom preprocessing libraries\n",
    "- Control the exact inference environment\n",
    "- Include proprietary code or models\n",
    "- Optimize performance for your specific use case\n",
    "\n",
    "**Our Custom Container:**\n",
    "- Python 3.9 with scikit-learn\n",
    "- Flask-based inference server\n",
    "- Automatic model loading from S3\n",
    "- Supports text classification with TF-IDF\n",
    "\n",
    "**Location:** `docker/sklearn-custom/`\n",
    "\n",
    "Let's deploy using the custom container from ECR!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
