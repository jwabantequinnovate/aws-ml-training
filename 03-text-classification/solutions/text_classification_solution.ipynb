{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4462c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Environment Setup\n",
    "# ============================================================\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add project root to path\n",
    "project_root = os.path.abspath('../..')\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# Standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import json\n",
    "import re\n",
    "\n",
    "# NLP imports\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, \n",
    "    accuracy_score, f1_score\n",
    ")\n",
    "\n",
    "# SageMaker imports\n",
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.sklearn import SKLearnModel\n",
    "\n",
    "# Configuration\n",
    "try:\n",
    "    from utils.sagemaker_config import get_sagemaker_config\n",
    "    config = get_sagemaker_config(s3_prefix='lab3-text-classification')\n",
    "    role = config['role']\n",
    "    session = config['session']\n",
    "    bucket = config['bucket']\n",
    "    region = config['region']\n",
    "except ImportError:\n",
    "    print(\"Using fallback configuration\")\n",
    "    role = get_execution_role()\n",
    "    session = sagemaker.Session()\n",
    "    bucket = session.default_bucket()\n",
    "    region = session.boto_region_name\n",
    "\n",
    "print(\"Configuration complete.\")\n",
    "print(f\"Region: {region}\")\n",
    "print(f\"S3 Bucket: {bucket}\")\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29093322",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è Endpoint Cleanup (Run if getting scaler.pkl error)\n",
    "\n",
    "Si vous obtenez l'erreur `scaler.pkl not found`, ex√©cutez cette cellule pour supprimer les anciens endpoints avant de red√©ployer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafa0391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cleanup Old Endpoints (if needed)\n",
    "# ============================================================\n",
    "\n",
    "import boto3\n",
    "\n",
    "print(\"üîç Checking for old text-classification endpoints...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "sm_client = boto3.client('sagemaker')\n",
    "\n",
    "try:\n",
    "    # List existing endpoints\n",
    "    response = sm_client.list_endpoints(\n",
    "        NameContains='text-classification',\n",
    "        StatusEquals='InService'\n",
    "    )\n",
    "    \n",
    "    endpoints = response['Endpoints']\n",
    "    \n",
    "    if not endpoints:\n",
    "        print(\"‚úÖ No old endpoints found. You can proceed with deployment.\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è  Found {len(endpoints)} existing endpoint(s):\\n\")\n",
    "        for ep in endpoints:\n",
    "            print(f\"  ‚Ä¢ {ep['EndpointName']} (Created: {ep['CreationTime']})\")\n",
    "        \n",
    "        print(\"\\nüóëÔ∏è  Deleting old endpoints to avoid scaler.pkl errors...\")\n",
    "        for ep in endpoints:\n",
    "            endpoint_name = ep['EndpointName']\n",
    "            sm_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "            print(f\"‚úÖ Deleted: {endpoint_name}\")\n",
    "        \n",
    "        print(\"\\n‚úÖ Cleanup complete! You can now deploy a new endpoint.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Note: {e}\")\n",
    "    print(\"Proceeding with notebook...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3c9c60",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 1: Data Generation and Text Preprocessing\n",
    "\n",
    "We'll generate synthetic customer support tickets across different categories.\n",
    "\n",
    "### Categories\n",
    "\n",
    "| Category | Description | Example Issues |\n",
    "|----------|-------------|----------------|\n",
    "| Technical | Software bugs, errors | \"App crashes when uploading\", \"Login error\" |\n",
    "| Billing | Payment, invoices | \"Incorrect charge\", \"Refund request\" |\n",
    "| Account | Profile, settings | \"Can't reset password\", \"Update email\" |\n",
    "| Product | Features, usage | \"How to use feature X\", \"Product info\" |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195d0bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Generate Synthetic Support Ticket Dataset\n",
    "# ============================================================\n",
    "\n",
    "def generate_support_tickets(n_samples=2000, random_state=42):\n",
    "    \"\"\"\n",
    "    Generate synthetic customer support tickets\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Define ticket templates by category\n",
    "    templates = {\n",
    "        'Technical': [\n",
    "            \"The app crashes when I try to {}\",\n",
    "            \"Getting error message: {}\",\n",
    "            \"Unable to {} in the system\",\n",
    "            \"Bug report: {} not working\",\n",
    "            \"Technical issue with {}\",\n",
    "            \"Software freezes during {}\",\n",
    "            \"Connection timeout when {}\",\n",
    "            \"API error: {}\",\n",
    "            \"Database sync failed for {}\",\n",
    "            \"Performance issues with {}\"\n",
    "        ],\n",
    "        'Billing': [\n",
    "            \"I was charged {} incorrectly\",\n",
    "            \"Need refund for {}\",\n",
    "            \"My invoice shows {} but should be {}\",\n",
    "            \"Payment failed for {}\",\n",
    "            \"Billing question about {}\",\n",
    "            \"Double charged for {}\",\n",
    "            \"Need receipt for {}\",\n",
    "            \"Subscription cancelled but still charged {}\",\n",
    "            \"Price changed without notice for {}\",\n",
    "            \"Discount code {} not working\"\n",
    "        ],\n",
    "        'Account': [\n",
    "            \"Cannot reset my password for {}\",\n",
    "            \"Need to update my {} information\",\n",
    "            \"Account locked, unable to {}\",\n",
    "            \"Email verification not working for {}\",\n",
    "            \"Profile {} needs to be changed\",\n",
    "            \"Security question issue with {}\",\n",
    "            \"Two-factor authentication problem for {}\",\n",
    "            \"Want to delete my account {}\",\n",
    "            \"Merge accounts for {}\",\n",
    "            \"Access denied to {}\"\n",
    "        ],\n",
    "        'Product': [\n",
    "            \"How do I use {} feature?\",\n",
    "            \"Need information about {}\",\n",
    "            \"Does the product support {}?\",\n",
    "            \"Feature request: {}\",\n",
    "            \"Documentation for {}\",\n",
    "            \"Tutorial needed for {}\",\n",
    "            \"Comparison between {} and {}\",\n",
    "            \"Is {} available in my plan?\",\n",
    "            \"How to configure {}\",\n",
    "            \"Best practices for {}\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Filler words for templates\n",
    "    fillers = {\n",
    "        'actions': ['upload', 'download', 'export', 'import', 'share', 'sync', 'save', 'delete'],\n",
    "        'features': ['dashboard', 'reports', 'analytics', 'integrations', 'API', 'mobile app', 'notifications'],\n",
    "        'billing': ['subscription', 'upgrade', 'invoice', 'payment', 'trial period', 'annual plan'],\n",
    "        'account': ['email', 'username', 'profile', 'settings', 'preferences', 'security'],\n",
    "        'products': ['premium features', 'data export', 'team collaboration', 'storage limit', 'API access']\n",
    "    }\n",
    "    \n",
    "    tickets = []\n",
    "    categories = []\n",
    "    \n",
    "    samples_per_category = n_samples // 4\n",
    "    \n",
    "    for category, template_list in templates.items():\n",
    "        for _ in range(samples_per_category):\n",
    "            template = np.random.choice(template_list)\n",
    "            \n",
    "            # Fill template with random fillers\n",
    "            if category == 'Technical':\n",
    "                filler = np.random.choice(fillers['actions'] + fillers['features'])\n",
    "            elif category == 'Billing':\n",
    "                filler = np.random.choice(fillers['billing'])\n",
    "            elif category == 'Account':\n",
    "                filler = np.random.choice(fillers['account'])\n",
    "            else:  # Product\n",
    "                filler = np.random.choice(fillers['products'])\n",
    "            \n",
    "            # Handle templates with multiple placeholders\n",
    "            if template.count('{}') == 2:\n",
    "                filler2 = np.random.choice(fillers['products'])\n",
    "                text = template.format(filler, filler2)\n",
    "            else:\n",
    "                text = template.format(filler)\n",
    "            \n",
    "            # Add some variation\n",
    "            if np.random.random() < 0.3:\n",
    "                text = text.upper()\n",
    "            elif np.random.random() < 0.3:\n",
    "                text = text.lower()\n",
    "            \n",
    "            # Add urgency markers sometimes\n",
    "            if np.random.random() < 0.2:\n",
    "                text = \"URGENT: \" + text\n",
    "            \n",
    "            # Add noise (typos, extra spaces)\n",
    "            if np.random.random() < 0.1:\n",
    "                text = text + \"!!!\"\n",
    "            \n",
    "            tickets.append(text)\n",
    "            categories.append(category)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'ticket_id': [f'TKT_{i:06d}' for i in range(len(tickets))],\n",
    "        'text': tickets,\n",
    "        'category': categories,\n",
    "        'priority': np.random.choice(['Low', 'Medium', 'High'], len(tickets), p=[0.5, 0.3, 0.2])\n",
    "    })\n",
    "    \n",
    "    # Shuffle\n",
    "    df = df.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Generate dataset\n",
    "print(\"Generating support ticket dataset...\")\n",
    "tickets_df = generate_support_tickets(n_samples=2000, random_state=42)\n",
    "\n",
    "print(f\"\\nDataset shape: {tickets_df.shape}\")\n",
    "print(f\"\\nCategory distribution:\")\n",
    "print(tickets_df['category'].value_counts())\n",
    "print(f\"\\nSample tickets:\")\n",
    "tickets_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e965d98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Text Preprocessing\n",
    "# ============================================================\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Clean and preprocess text data\n",
    "    \"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URGENT markers\n",
    "    text = re.sub(r'urgent:\\s*', '', text)\n",
    "    \n",
    "    # Remove extra punctuation\n",
    "    text = re.sub(r'!+', '!', text)\n",
    "    text = re.sub(r'\\?+', '?', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Strip\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply preprocessing\n",
    "tickets_df['text_clean'] = tickets_df['text'].apply(preprocess_text)\n",
    "\n",
    "print(\"Text Preprocessing Complete\")\n",
    "print(\"\\nOriginal vs Cleaned Examples:\")\n",
    "for idx in range(3):\n",
    "    print(f\"\\nOriginal: {tickets_df.iloc[idx]['text']}\")\n",
    "    print(f\"Cleaned:  {tickets_df.iloc[idx]['text_clean']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f4675b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 2: Exploratory Data Analysis\n",
    "\n",
    "Understanding text characteristics and patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5bcc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Text Analysis\n",
    "# ============================================================\n",
    "\n",
    "# Text length analysis\n",
    "tickets_df['text_length'] = tickets_df['text_clean'].str.len()\n",
    "tickets_df['word_count'] = tickets_df['text_clean'].str.split().str.len()\n",
    "\n",
    "print(\"Text Statistics by Category:\")\n",
    "print(\"=\"*60)\n",
    "print(tickets_df.groupby('category')[['text_length', 'word_count']].agg(['mean', 'median', 'std']))\n",
    "\n",
    "# Visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Category distribution\n",
    "ax = axes[0, 0]\n",
    "tickets_df['category'].value_counts().plot(kind='bar', ax=ax, color='skyblue')\n",
    "ax.set_title('Ticket Distribution by Category')\n",
    "ax.set_xlabel('Category')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n",
    "\n",
    "# Text length by category\n",
    "ax = axes[0, 1]\n",
    "tickets_df.boxplot(column='text_length', by='category', ax=ax)\n",
    "ax.set_title('Text Length Distribution by Category')\n",
    "ax.set_xlabel('Category')\n",
    "ax.set_ylabel('Characters')\n",
    "\n",
    "# Word count by category\n",
    "ax = axes[1, 0]\n",
    "tickets_df.boxplot(column='word_count', by='category', ax=ax)\n",
    "ax.set_title('Word Count Distribution by Category')\n",
    "ax.set_xlabel('Category')\n",
    "ax.set_ylabel('Words')\n",
    "\n",
    "# Priority distribution\n",
    "ax = axes[1, 1]\n",
    "pd.crosstab(tickets_df['category'], tickets_df['priority']).plot(kind='bar', stacked=True, ax=ax)\n",
    "ax.set_title('Priority by Category')\n",
    "ax.set_xlabel('Category')\n",
    "ax.set_ylabel('Count')\n",
    "ax.legend(title='Priority')\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44aae04",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 3: Feature Extraction with TF-IDF\n",
    "\n",
    "TF-IDF (Term Frequency-Inverse Document Frequency) is cost-effective and performs well for text classification.\n",
    "\n",
    "### TF-IDF Advantages\n",
    "\n",
    "- **Fast**: No GPU required\n",
    "- **Interpretable**: See which words matter\n",
    "- **Efficient**: Low memory footprint\n",
    "- **Effective**: Good baseline performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84c15f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TF-IDF Vectorization\n",
    "# ============================================================\n",
    "\n",
    "# Prepare data\n",
    "X_text = tickets_df['text_clean']\n",
    "y = tickets_df['category']\n",
    "\n",
    "# Label encoding\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "print(\"Label mapping:\")\n",
    "for idx, label in enumerate(label_encoder.classes_):\n",
    "    print(f\"  {label}: {idx}\")\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_text, y_encoded,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y_encoded\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "# Using limited features to keep model lightweight\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=1000,  # Limit vocabulary size\n",
    "    ngram_range=(1, 2),  # Unigrams and bigrams\n",
    "    min_df=2,  # Minimum document frequency\n",
    "    max_df=0.8,  # Maximum document frequency\n",
    "    strip_accents='unicode',\n",
    "    lowercase=True,\n",
    "    stop_words='english'\n",
    ")\n",
    "\n",
    "# Fit and transform\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "print(f\"\\nTF-IDF matrix shape: {X_train_tfidf.shape}\")\n",
    "print(f\"Vocabulary size: {len(tfidf.vocabulary_)}\")\n",
    "print(f\"Matrix density: {X_train_tfidf.nnz / (X_train_tfidf.shape[0] * X_train_tfidf.shape[1]):.4f}\")\n",
    "\n",
    "# Show top features per category\n",
    "print(\"\\nTop 10 features per category:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "for idx, category in enumerate(label_encoder.classes_):\n",
    "    # Get samples for this category\n",
    "    category_mask = y_train == idx\n",
    "    category_tfidf = X_train_tfidf[category_mask].mean(axis=0).A1\n",
    "    \n",
    "    # Get top features\n",
    "    top_indices = category_tfidf.argsort()[-10:][::-1]\n",
    "    top_features = [feature_names[i] for i in top_indices]\n",
    "    \n",
    "    print(f\"\\n{category}:\")\n",
    "    print(f\"  {', '.join(top_features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4364f786",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 4: Model Training and Comparison\n",
    "\n",
    "Training multiple lightweight models for text classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef17e4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Train Multiple Models\n",
    "# ============================================================\n",
    "\n",
    "models = {\n",
    "    'Naive Bayes': MultinomialNB(alpha=0.1),\n",
    "    'Logistic Regression': LogisticRegression(\n",
    "        random_state=42, \n",
    "        max_iter=500,\n",
    "        class_weight='balanced',\n",
    "        solver='lbfgs'\n",
    "    )\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "print(\"Training models...\\n\")\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    \n",
    "    # Train\n",
    "    model.fit(X_train_tfidf, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = model.predict(X_test_tfidf)\n",
    "    y_pred_proba = model.predict_proba(X_test_tfidf)\n",
    "    \n",
    "    # Metrics\n",
    "    results[name] = {\n",
    "        'model': model,\n",
    "        'predictions': y_pred,\n",
    "        'probabilities': y_pred_proba,\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'f1_macro': f1_score(y_test, y_pred, average='macro')\n",
    "    }\n",
    "    \n",
    "    print(f\"  Accuracy: {results[name]['accuracy']:.4f}\")\n",
    "    print(f\"  F1 Score (macro): {results[name]['f1_macro']:.4f}\\n\")\n",
    "\n",
    "# Select best model\n",
    "best_model_name = max(results, key=lambda x: results[x]['accuracy'])\n",
    "best_model = results[best_model_name]['model']\n",
    "\n",
    "print(f\"Best model: {best_model_name}\")\n",
    "print(f\"Accuracy: {results[best_model_name]['accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26208856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Detailed Evaluation\n",
    "# ============================================================\n",
    "\n",
    "y_pred_best = results[best_model_name]['predictions']\n",
    "\n",
    "print(f\"Detailed Evaluation: {best_model_name}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(\n",
    "    y_test, \n",
    "    y_pred_best, \n",
    "    target_names=label_encoder.classes_\n",
    "))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_best)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "    cm, \n",
    "    annot=True, \n",
    "    fmt='d', \n",
    "    cmap='Blues',\n",
    "    xticklabels=label_encoder.classes_,\n",
    "    yticklabels=label_encoder.classes_\n",
    ")\n",
    "plt.title(f'Confusion Matrix - {best_model_name}')\n",
    "plt.ylabel('True Category')\n",
    "plt.xlabel('Predicted Category')\n",
    "plt.show()\n",
    "\n",
    "# Per-category performance\n",
    "print(\"\\nPer-Category Accuracy:\")\n",
    "for idx, category in enumerate(label_encoder.classes_):\n",
    "    mask = y_test == idx\n",
    "    category_accuracy = accuracy_score(y_test[mask], y_pred_best[mask])\n",
    "    print(f\"  {category}: {category_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa9ae77",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 5: Model Interpretation\n",
    "\n",
    "Understanding which words drive predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d03b485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Feature Importance Analysis\n",
    "# ============================================================\n",
    "\n",
    "if best_model_name == 'Logistic Regression':\n",
    "    # Get feature importance from coefficients\n",
    "    feature_names = tfidf.get_feature_names_out()\n",
    "    \n",
    "    print(\"Most Important Words per Category:\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for idx, category in enumerate(label_encoder.classes_):\n",
    "        # Get coefficients for this category\n",
    "        coef = best_model.coef_[idx]\n",
    "        \n",
    "        # Top positive features\n",
    "        top_positive_idx = coef.argsort()[-10:][::-1]\n",
    "        top_positive = [(feature_names[i], coef[i]) for i in top_positive_idx]\n",
    "        \n",
    "        print(f\"\\n{category} - Top indicators:\")\n",
    "        for word, score in top_positive:\n",
    "            print(f\"  {word}: {score:.4f}\")\n",
    "\n",
    "# Test with sample predictions\n",
    "print(\"\\n\\nSample Predictions:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_samples = [\n",
    "    \"app crashes when uploading files\",\n",
    "    \"need refund for incorrect charge\",\n",
    "    \"cannot reset my password\",\n",
    "    \"how to use the new analytics feature\"\n",
    "]\n",
    "\n",
    "for sample in test_samples:\n",
    "    # Preprocess\n",
    "    clean_sample = preprocess_text(sample)\n",
    "    \n",
    "    # Vectorize\n",
    "    sample_tfidf = tfidf.transform([clean_sample])\n",
    "    \n",
    "    # Predict\n",
    "    pred_idx = best_model.predict(sample_tfidf)[0]\n",
    "    pred_proba = best_model.predict_proba(sample_tfidf)[0]\n",
    "    pred_category = label_encoder.classes_[pred_idx]\n",
    "    \n",
    "    print(f\"\\nText: '{sample}'\")\n",
    "    print(f\"Predicted: {pred_category} (confidence: {pred_proba[pred_idx]:.2%})\")\n",
    "    print(f\"All probabilities: {dict(zip(label_encoder.classes_, pred_proba))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b8d481",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 6: Model Deployment\n",
    "\n",
    "Package and deploy the text classification model to SageMaker.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99d06b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Package Model for Deployment\n",
    "# ============================================================\n",
    "\n",
    "import joblib\n",
    "import tarfile\n",
    "import shutil\n",
    "\n",
    "# Create model directory\n",
    "model_dir = 'text_classification_model'\n",
    "if os.path.exists(model_dir):\n",
    "    shutil.rmtree(model_dir)\n",
    "os.makedirs(model_dir)\n",
    "\n",
    "# Save model artifacts\n",
    "joblib.dump(best_model, os.path.join(model_dir, 'model.pkl'))\n",
    "joblib.dump(tfidf, os.path.join(model_dir, 'tfidf.pkl'))\n",
    "joblib.dump(label_encoder, os.path.join(model_dir, 'label_encoder.pkl'))\n",
    "\n",
    "print(\"Model artifacts saved\")\n",
    "\n",
    "# Create inference script\n",
    "inference_code = \"\"\"\n",
    "import joblib\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    model = joblib.load(os.path.join(model_dir, 'model.pkl'))\n",
    "    tfidf = joblib.load(os.path.join(model_dir, 'tfidf.pkl'))\n",
    "    label_encoder = joblib.load(os.path.join(model_dir, 'label_encoder.pkl'))\n",
    "    return {'model': model, 'tfidf': tfidf, 'label_encoder': label_encoder}\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'urgent:\\\\s*', '', text)\n",
    "    text = re.sub(r'!+', '!', text)\n",
    "    text = re.sub(r'\\\\?+', '?', text)\n",
    "    text = re.sub(r'\\\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def input_fn(request_body, request_content_type):\n",
    "    if request_content_type == 'application/json':\n",
    "        data = json.loads(request_body)\n",
    "        if isinstance(data, dict):\n",
    "            return [data.get('text', '')]\n",
    "        elif isinstance(data, list):\n",
    "            return [item.get('text', '') if isinstance(item, dict) else str(item) for item in data]\n",
    "        else:\n",
    "            return [str(data)]\n",
    "    else:\n",
    "        return [request_body]\n",
    "\n",
    "def predict_fn(input_data, model_dict):\n",
    "    model = model_dict['model']\n",
    "    tfidf = model_dict['tfidf']\n",
    "    label_encoder = model_dict['label_encoder']\n",
    "    \n",
    "    # Preprocess\n",
    "    texts_clean = [preprocess_text(text) for text in input_data]\n",
    "    \n",
    "    # Vectorize\n",
    "    X_tfidf = tfidf.transform(texts_clean)\n",
    "    \n",
    "    # Predict\n",
    "    predictions = model.predict(X_tfidf)\n",
    "    probabilities = model.predict_proba(X_tfidf)\n",
    "    \n",
    "    # Format results\n",
    "    results = []\n",
    "    for pred_idx, proba in zip(predictions, probabilities):\n",
    "        category = label_encoder.classes_[pred_idx]\n",
    "        confidence = float(proba[pred_idx])\n",
    "        all_probs = {label: float(prob) for label, prob in zip(label_encoder.classes_, proba)}\n",
    "        \n",
    "        results.append({\n",
    "            'category': category,\n",
    "            'confidence': confidence,\n",
    "            'all_probabilities': all_probs\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "def output_fn(prediction, accept):\n",
    "    if accept == 'application/json':\n",
    "        return json.dumps(prediction), accept\n",
    "    raise ValueError(f'Unsupported accept type: {accept}')\n",
    "\"\"\"\n",
    "\n",
    "with open(os.path.join(model_dir, 'inference.py'), 'w') as f:\n",
    "    f.write(inference_code)\n",
    "\n",
    "print(\"Inference script created\")\n",
    "\n",
    "# Create requirements\n",
    "requirements = \"\"\"scikit-learn==1.3.0\n",
    "joblib==1.3.2\n",
    "numpy==1.24.3\n",
    "\"\"\"\n",
    "\n",
    "with open(os.path.join(model_dir, 'requirements.txt'), 'w') as f:\n",
    "    f.write(requirements)\n",
    "\n",
    "print(\"Requirements file created\")\n",
    "\n",
    "# Create tar.gz\n",
    "model_archive = 'model.tar.gz'\n",
    "with tarfile.open(model_archive, 'w:gz') as tar:\n",
    "    tar.add(model_dir, arcname='.')\n",
    "\n",
    "print(f\"Model archive created: {model_archive}\")\n",
    "\n",
    "# Upload to S3\n",
    "model_s3_key = f'lab3-text-classification/models/{datetime.now().strftime(\"%Y%m%d-%H%M%S\")}/model.tar.gz'\n",
    "s3_client = boto3.client('s3')\n",
    "s3_client.upload_file(model_archive, bucket, model_s3_key)\n",
    "\n",
    "model_s3_uri = f's3://{bucket}/{model_s3_key}'\n",
    "print(f\"\\nModel uploaded to: {model_s3_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa010a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Deploy to SageMaker Endpoint\n",
    "# ============================================================\n",
    "\n",
    "from sagemaker.sklearn import SKLearnModel\n",
    "\n",
    "# Create model\n",
    "sklearn_model = SKLearnModel(\n",
    "    model_data=model_s3_uri,\n",
    "    role=role,\n",
    "    entry_point='inference.py',\n",
    "    framework_version='1.2-1',\n",
    "    py_version='py3',\n",
    "    name=f'text-classification-{datetime.now().strftime(\"%Y%m%d-%H%M%S\")}'\n",
    ")\n",
    "\n",
    "print(\"Deploying model to endpoint...\")\n",
    "print(\"This will take 5-10 minutes...\")\n",
    "\n",
    "# Deploy\n",
    "predictor = sklearn_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m5.large',\n",
    "    endpoint_name=f'text-classification-{datetime.now().strftime(\"%Y%m%d-%H%M%S\")}'\n",
    ")\n",
    "\n",
    "print(f\"\\nEndpoint deployed: {predictor.endpoint_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640b00b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Test Endpoint\n",
    "# ============================================================\n",
    "\n",
    "# Test with sample tickets\n",
    "test_tickets = [\n",
    "    {\"text\": \"The application crashes every time I try to upload a document\"},\n",
    "    {\"text\": \"I was charged twice for my subscription this month\"},\n",
    "    {\"text\": \"How do I reset my password? The email link expired\"},\n",
    "    {\"text\": \"What features are included in the premium plan?\"}\n",
    "]\n",
    "\n",
    "print(\"Testing endpoint with sample tickets:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for ticket in test_tickets:\n",
    "    response = predictor.predict(ticket)\n",
    "    \n",
    "    print(f\"\\nTicket: '{ticket['text']}'\")\n",
    "    print(f\"Category: {response[0]['category']}\")\n",
    "    print(f\"Confidence: {response[0]['confidence']:.2%}\")\n",
    "    print(f\"All probabilities: {response[0]['all_probabilities']}\")\n",
    "\n",
    "# Test batch prediction\n",
    "print(\"\\n\\nBatch prediction test:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "batch_response = predictor.predict(test_tickets)\n",
    "\n",
    "for i, (ticket, result) in enumerate(zip(test_tickets, batch_response)):\n",
    "    print(f\"\\n{i+1}. {result['category']} ({result['confidence']:.2%}) - '{ticket['text'][:50]}...'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30709ea4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 8: SageMaker Clarify - Explainability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807ebceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SageMaker Clarify - Model Explainability\n",
    "# ============================================================\n",
    "\n",
    "from sagemaker import clarify\n",
    "\n",
    "print(\"SageMaker Clarify - Model Explainability\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nClarify helps explain model predictions and detect bias\\n\")\n",
    "\n",
    "# Manual Feature Importance Analysis (cost-effective alternative)\n",
    "print(\"Feature Importance Analysis:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Obtenir les coefficients du mod√®le\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "coefficients = model.coef_\n",
    "\n",
    "# Pour chaque cat√©gorie\n",
    "categories = label_encoder.classes_\n",
    "for idx, category in enumerate(categories):\n",
    "    print(f\"\\n{category.upper()} Category:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Top 10 mots positifs\n",
    "    top_indices = np.argsort(coefficients[idx])[-10:][::-1]\n",
    "    print(f\"Top positive indicators:\")\n",
    "    for i, feat_idx in enumerate(top_indices, 1):\n",
    "        print(f\"  {i}. '{feature_names[feat_idx]}' (weight: {coefficients[idx][feat_idx]:.3f})\")\n",
    "\n",
    "# Visualiser les poids pour chaque cat√©gorie\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, category in enumerate(categories):\n",
    "    # Top 15 features par valeur absolue\n",
    "    top_features_idx = np.argsort(np.abs(coefficients[idx]))[-15:]\n",
    "    top_features = [feature_names[i] for i in top_features_idx]\n",
    "    top_weights = coefficients[idx][top_features_idx]\n",
    "    \n",
    "    # Plot\n",
    "    colors = ['green' if w > 0 else 'red' for w in top_weights]\n",
    "    axes[idx].barh(range(len(top_features)), top_weights, color=colors, alpha=0.7)\n",
    "    axes[idx].set_yticks(range(len(top_features)))\n",
    "    axes[idx].set_yticklabels(top_features)\n",
    "    axes[idx].set_xlabel('Weight')\n",
    "    axes[idx].set_title(f'{category} - Feature Importance')\n",
    "    axes[idx].axvline(x=0, color='black', linestyle='--', linewidth=0.5)\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('clarify_feature_importance.png', dpi=100, bbox_inches='tight')\n",
    "print(f\"\\nüìä Feature importance visualization saved\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbe6cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Explain Individual Predictions\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\\nExplaining Individual Predictions:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Prendre un exemple de chaque cat√©gorie\n",
    "example_tickets = {\n",
    "    'Technical': \"The software freezes when I try to export data\",\n",
    "    'Billing': \"I was charged for a service I cancelled last month\",\n",
    "    'Account': \"I forgot my username and need to recover my account\",\n",
    "    'Product': \"What's the difference between the basic and pro plans?\"\n",
    "}\n",
    "\n",
    "for category, ticket_text in example_tickets.items():\n",
    "    print(f\"\\n{category} Example:\")\n",
    "    print(f\"Ticket: '{ticket_text}'\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Vectoriser le ticket\n",
    "    ticket_vector = vectorizer.transform([ticket_text])\n",
    "    \n",
    "    # Obtenir les pr√©dictions\n",
    "    prediction = model.predict(ticket_vector)[0]\n",
    "    probabilities = model.predict_proba(ticket_vector)[0]\n",
    "    \n",
    "    # Trouver les mots du ticket qui ont le plus d'influence\n",
    "    feature_indices = ticket_vector.nonzero()[1]\n",
    "    ticket_features = [(feature_names[i], ticket_vector[0, i], coefficients[prediction][i]) \n",
    "                       for i in feature_indices]\n",
    "    ticket_features.sort(key=lambda x: abs(x[2]), reverse=True)\n",
    "    \n",
    "    print(f\"Predicted: {label_encoder.classes_[prediction]}\")\n",
    "    print(f\"Confidence: {max(probabilities):.2%}\")\n",
    "    print(f\"\\nTop influential words:\")\n",
    "    for word, tfidf, coef in ticket_features[:5]:\n",
    "        impact = tfidf * coef\n",
    "        print(f\"  '{word}': TF-IDF={tfidf:.3f}, Coef={coef:.3f}, Impact={impact:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef93fa0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### SageMaker Clarify Benefits\n",
    "\n",
    "**Model Explainability:**\n",
    "- Feature importance scores (which words matter most)\n",
    "- SHAP values for individual predictions\n",
    "- Global and local explanations\n",
    "\n",
    "**Bias Detection:**\n",
    "- Pre-training bias (data imbalance)\n",
    "- Post-training bias (model fairness)\n",
    "- Fairness metrics across groups\n",
    "\n",
    "**Use Cases:**\n",
    "- Compliance (GDPR, regulatory requirements)\n",
    "- Model debugging and improvement\n",
    "- Building trust with stakeholders\n",
    "- Identifying data quality issues\n",
    "\n",
    "**Production Integration:**\n",
    "```python\n",
    "# In production, use Clarify for:\n",
    "clarify_processor = clarify.SageMakerClarifyProcessor(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.large'\n",
    ")\n",
    "\n",
    "# Run explainability analysis\n",
    "clarify_processor.run_explainability(\n",
    "    data_config=data_config,\n",
    "    model_config=model_config,\n",
    "    explainability_config=shap_config\n",
    ")\n",
    "```\n",
    "\n",
    "**Key Metrics:**\n",
    "- Feature importance rankings\n",
    "- SHAP values (contribution of each feature)\n",
    "- Bias metrics (DI, DPL, KL divergence)\n",
    "- Fairness metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd89ff90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cleanup All Resources\n",
    "# ============================================================\n",
    "\n",
    "print(\"üßπ Cleaning up Lab 3 resources...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Delete ALL text-classification endpoints\n",
    "sm_client = boto3.client('sagemaker')\n",
    "\n",
    "try:\n",
    "    response = sm_client.list_endpoints(NameContains='text-classification')\n",
    "    endpoints = response['Endpoints']\n",
    "    \n",
    "    if endpoints:\n",
    "        print(f\"\\nüóëÔ∏è  Deleting {len(endpoints)} endpoint(s)...\")\n",
    "        for ep in endpoints:\n",
    "            endpoint_name = ep['EndpointName']\n",
    "            try:\n",
    "                sm_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "                print(f\"  ‚úÖ Deleted: {endpoint_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ö†Ô∏è  {endpoint_name}: {e}\")\n",
    "    else:\n",
    "        print(\"  No endpoints to delete\")\n",
    "except Exception as e:\n",
    "    print(f\"  Error listing endpoints: {e}\")\n",
    "\n",
    "# Cleanup local files\n",
    "print(\"\\nüìÅ Cleaning up local files...\")\n",
    "try:\n",
    "    if 'model_dir' in globals() and os.path.exists(model_dir):\n",
    "        shutil.rmtree(model_dir)\n",
    "        print(\"  ‚úÖ Deleted model directory\")\n",
    "    if 'model_archive' in globals() and os.path.exists(model_archive):\n",
    "        os.remove(model_archive)\n",
    "        print(\"  ‚úÖ Deleted model archive\")\n",
    "except Exception as e:\n",
    "    print(f\"  ‚ö†Ô∏è  {e}\")\n",
    "\n",
    "print(\"\\n‚úÖ Lab 3 cleanup complete!\")\n",
    "print(\"üí° Note: S3 data and models are kept for future use\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fc6b2f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this lab, you:\n",
    "\n",
    "1. Generated and preprocessed synthetic support ticket data\n",
    "2. Extracted features using TF-IDF vectorization (cost-effective)\n",
    "3. Trained and compared lightweight classification models\n",
    "4. Interpreted model predictions with feature importance\n",
    "5. Deployed the model to a SageMaker endpoint\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **TF-IDF**: Cost-effective alternative to transformers for many tasks\n",
    "- **Text Preprocessing**: Critical for model performance\n",
    "- **Model Selection**: Logistic Regression and Naive Bayes are fast and effective\n",
    "- **Interpretability**: TF-IDF features are easy to understand\n",
    "- **SageMaker**: Simplifies deployment and scaling\n",
    "\n",
    "### Cost Optimization\n",
    "\n",
    "| Component | Choice | Savings |\n",
    "|-----------|--------|---------|\n",
    "| Model | TF-IDF + Logistic Regression | No GPU needed |\n",
    "| Instance | ml.m5.large | 90% cheaper than GPU |\n",
    "| Training | < 1 minute | Minimal compute cost |\n",
    "| Features | Limited to 1000 | Smaller model size |\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Lab 4: Sentiment Analysis\n",
    "- Try different n-gram ranges\n",
    "- Experiment with different vectorization parameters\n",
    "- Add custom preprocessing rules\n",
    "\n",
    "---\n",
    "\n",
    "**Remember to delete your endpoint to avoid charges!**\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
