{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fb73a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Environment Setup\n",
    "# ============================================================\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add project root to path\n",
    "project_root = os.path.abspath('../..')\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# Standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# ML imports\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, roc_auc_score,\n",
    "    roc_curve, precision_recall_curve, f1_score\n",
    ")\n",
    "\n",
    "# SageMaker imports\n",
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.sklearn import SKLearnModel\n",
    "from sagemaker.feature_store.feature_group import FeatureGroup\n",
    "\n",
    "# Configuration\n",
    "try:\n",
    "    from utils.sagemaker_config import get_sagemaker_config\n",
    "    config = get_sagemaker_config(s3_prefix='lab2-churn')\n",
    "    role = config['role']\n",
    "    session = config['session']\n",
    "    bucket = config['bucket']\n",
    "    region = config['region']\n",
    "except ImportError:\n",
    "    print(\"Using fallback configuration\")\n",
    "    role = get_execution_role()\n",
    "    session = sagemaker.Session()\n",
    "    bucket = session.default_bucket()\n",
    "    region = session.boto_region_name\n",
    "\n",
    "print(\"Configuration complete.\")\n",
    "print(f\"Region: {region}\")\n",
    "print(f\"S3 Bucket: {bucket}\")\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20692294",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 1: Data Generation and Exploration\n",
    "\n",
    "We'll generate synthetic customer data representing a telecommunications company with realistic churn patterns.\n",
    "\n",
    "### Feature Categories\n",
    "\n",
    "| Category | Features | Description |\n",
    "|----------|----------|-------------|\n",
    "| Demographics | Age, Gender, Tenure | Customer profile |\n",
    "| Services | Internet, Phone, Streaming | Service usage |\n",
    "| Contract | Type, Payment Method | Contract details |\n",
    "| Usage | Minutes, Data GB, Support Calls | Usage patterns |\n",
    "| Billing | Monthly Charges, Total Charges | Financial metrics |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5958c8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Generate Synthetic Churn Dataset\n",
    "# ============================================================\n",
    "\n",
    "def generate_churn_data(n_customers=5000, random_state=42):\n",
    "    \"\"\"\n",
    "    Generate synthetic customer churn dataset\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Demographics\n",
    "    age = np.random.normal(45, 15, n_customers).clip(18, 80).astype(int)\n",
    "    gender = np.random.choice(['Male', 'Female'], n_customers)\n",
    "    tenure = np.random.exponential(24, n_customers).clip(0, 72).astype(int)\n",
    "    \n",
    "    # Services\n",
    "    has_internet = np.random.choice([0, 1], n_customers, p=[0.2, 0.8])\n",
    "    has_phone = np.random.choice([0, 1], n_customers, p=[0.1, 0.9])\n",
    "    has_streaming = has_internet * np.random.choice([0, 1], n_customers, p=[0.6, 0.4])\n",
    "    has_tech_support = has_internet * np.random.choice([0, 1], n_customers, p=[0.7, 0.3])\n",
    "    \n",
    "    # Contract\n",
    "    contract_type = np.random.choice(['Month-to-month', 'One year', 'Two year'], \n",
    "                                    n_customers, p=[0.5, 0.3, 0.2])\n",
    "    payment_method = np.random.choice(['Electronic', 'Mailed check', 'Bank transfer', 'Credit card'],\n",
    "                                     n_customers, p=[0.4, 0.2, 0.2, 0.2])\n",
    "    \n",
    "    # Usage patterns\n",
    "    monthly_minutes = np.random.gamma(3, 200, n_customers) * has_phone\n",
    "    monthly_data_gb = np.random.gamma(2, 15, n_customers) * has_internet\n",
    "    support_calls = np.random.poisson(1.5, n_customers).clip(0, 10)\n",
    "    \n",
    "    # Billing\n",
    "    base_charge = 30 + (has_internet * 30) + (has_phone * 20) + (has_streaming * 15)\n",
    "    monthly_charges = base_charge + np.random.normal(0, 5, n_customers)\n",
    "    total_charges = monthly_charges * tenure + np.random.normal(0, 100, n_customers)\n",
    "    total_charges = total_charges.clip(0)\n",
    "    \n",
    "    # Create churn probability based on risk factors\n",
    "    churn_prob = 0.1  # Base probability\n",
    "    \n",
    "    # Risk factors increase churn\n",
    "    churn_prob += (tenure < 6) * 0.3  # New customers\n",
    "    churn_prob += (contract_type == 'Month-to-month') * 0.25  # No commitment\n",
    "    churn_prob += (support_calls > 3) * 0.2  # High support needs\n",
    "    churn_prob += (monthly_charges > 80) * 0.15  # High bills\n",
    "    churn_prob += (has_tech_support == 0) * 0.1  # No tech support\n",
    "    \n",
    "    # Protective factors decrease churn\n",
    "    churn_prob -= (tenure > 24) * 0.2  # Loyal customers\n",
    "    churn_prob -= (contract_type == 'Two year') * 0.25  # Long commitment\n",
    "    churn_prob -= (has_streaming == 1) * 0.1  # More engaged\n",
    "    \n",
    "    churn_prob = churn_prob.clip(0, 0.9)\n",
    "    churn = (np.random.random(n_customers) < churn_prob).astype(int)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'customer_id': [f'CUST_{i:06d}' for i in range(n_customers)],\n",
    "        'age': age,\n",
    "        'gender': gender,\n",
    "        'tenure_months': tenure,\n",
    "        'has_internet': has_internet,\n",
    "        'has_phone_service': has_phone,\n",
    "        'has_streaming': has_streaming,\n",
    "        'has_tech_support': has_tech_support,\n",
    "        'contract_type': contract_type,\n",
    "        'payment_method': payment_method,\n",
    "        'monthly_minutes': monthly_minutes.round(0),\n",
    "        'monthly_data_gb': monthly_data_gb.round(1),\n",
    "        'support_calls': support_calls,\n",
    "        'monthly_charges': monthly_charges.round(2),\n",
    "        'total_charges': total_charges.round(2),\n",
    "        'churn': churn\n",
    "    })\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Generate dataset\n",
    "print(\"Generating customer churn dataset...\")\n",
    "churn_df = generate_churn_data(n_customers=5000, random_state=42)\n",
    "\n",
    "print(f\"\\nDataset shape: {churn_df.shape}\")\n",
    "print(f\"Churn rate: {churn_df['churn'].mean()*100:.1f}%\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "churn_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1028265e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Exploratory Data Analysis\n",
    "# ============================================================\n",
    "\n",
    "print(\"Dataset Overview:\")\n",
    "print(\"=\"*60)\n",
    "print(churn_df.info())\n",
    "\n",
    "print(\"\\n\\nNumerical Features Statistics:\")\n",
    "print(\"=\"*60)\n",
    "print(churn_df.describe())\n",
    "\n",
    "print(\"\\n\\nCategorical Features Distribution:\")\n",
    "print(\"=\"*60)\n",
    "categorical_cols = ['gender', 'contract_type', 'payment_method']\n",
    "for col in categorical_cols:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(churn_df[col].value_counts())\n",
    "\n",
    "print(\"\\n\\nChurn Distribution:\")\n",
    "print(\"=\"*60)\n",
    "print(churn_df['churn'].value_counts())\n",
    "print(f\"Churn rate: {churn_df['churn'].mean()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fab9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Visualizations\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# 1. Churn by tenure\n",
    "ax = axes[0, 0]\n",
    "churn_df.groupby('tenure_months')['churn'].mean().plot(ax=ax)\n",
    "ax.set_title('Churn Rate by Tenure')\n",
    "ax.set_xlabel('Tenure (months)')\n",
    "ax.set_ylabel('Churn Rate')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Churn by contract type\n",
    "ax = axes[0, 1]\n",
    "churn_by_contract = churn_df.groupby('contract_type')['churn'].mean()\n",
    "churn_by_contract.plot(kind='bar', ax=ax, color='coral')\n",
    "ax.set_title('Churn Rate by Contract Type')\n",
    "ax.set_ylabel('Churn Rate')\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n",
    "\n",
    "# 3. Monthly charges distribution\n",
    "ax = axes[0, 2]\n",
    "churn_df[churn_df['churn']==0]['monthly_charges'].hist(ax=ax, alpha=0.6, label='Not Churned', bins=30)\n",
    "churn_df[churn_df['churn']==1]['monthly_charges'].hist(ax=ax, alpha=0.6, label='Churned', bins=30)\n",
    "ax.set_title('Monthly Charges Distribution')\n",
    "ax.set_xlabel('Monthly Charges ($)')\n",
    "ax.legend()\n",
    "\n",
    "# 4. Support calls vs churn\n",
    "ax = axes[1, 0]\n",
    "churn_by_support = churn_df.groupby('support_calls')['churn'].mean()\n",
    "churn_by_support.plot(kind='bar', ax=ax, color='skyblue')\n",
    "ax.set_title('Churn Rate by Support Calls')\n",
    "ax.set_xlabel('Number of Support Calls')\n",
    "ax.set_ylabel('Churn Rate')\n",
    "\n",
    "# 5. Age distribution\n",
    "ax = axes[1, 1]\n",
    "churn_df[churn_df['churn']==0]['age'].hist(ax=ax, alpha=0.6, label='Not Churned', bins=20)\n",
    "churn_df[churn_df['churn']==1]['age'].hist(ax=ax, alpha=0.6, label='Churned', bins=20)\n",
    "ax.set_title('Age Distribution')\n",
    "ax.set_xlabel('Age')\n",
    "ax.legend()\n",
    "\n",
    "# 6. Correlation heatmap\n",
    "ax = axes[1, 2]\n",
    "numeric_cols = ['age', 'tenure_months', 'monthly_charges', 'total_charges', 'support_calls', 'churn']\n",
    "corr_matrix = churn_df[numeric_cols].corr()\n",
    "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', ax=ax, center=0)\n",
    "ax.set_title('Feature Correlations')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Observations:\")\n",
    "print(\"- New customers (< 6 months) have higher churn\")\n",
    "print(\"- Month-to-month contracts have highest churn rate\")\n",
    "print(\"- High support calls correlate with churn\")\n",
    "print(\"- Monthly charges show moderate correlation with churn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f04ade3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 2: Feature Engineering\n",
    "\n",
    "Creating derived features to improve model performance.\n",
    "\n",
    "### Feature Engineering Strategies\n",
    "\n",
    "1. **Interaction Features**: Combine related features\n",
    "2. **Binning**: Group continuous variables\n",
    "3. **Encoding**: Convert categorical to numerical\n",
    "4. **Scaling**: Normalize numerical features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43addf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Feature Engineering\n",
    "# ============================================================\n",
    "\n",
    "def engineer_features(df):\n",
    "    \"\"\"Create derived features for churn prediction\"\"\"\n",
    "    \n",
    "    df_fe = df.copy()\n",
    "    \n",
    "    # 1. Customer value metrics\n",
    "    df_fe['avg_monthly_charge'] = df_fe['total_charges'] / (df_fe['tenure_months'] + 1)\n",
    "    df_fe['charge_per_service'] = df_fe['monthly_charges'] / (\n",
    "        df_fe['has_internet'] + df_fe['has_phone_service'] + \n",
    "        df_fe['has_streaming'] + df_fe['has_tech_support'] + 1\n",
    "    )\n",
    "    \n",
    "    # 2. Engagement metrics\n",
    "    df_fe['service_count'] = (\n",
    "        df_fe['has_internet'] + df_fe['has_phone_service'] + \n",
    "        df_fe['has_streaming'] + df_fe['has_tech_support']\n",
    "    )\n",
    "    df_fe['minutes_per_month'] = df_fe['monthly_minutes'] / (df_fe['tenure_months'] + 1)\n",
    "    df_fe['data_gb_per_month'] = df_fe['monthly_data_gb'] / (df_fe['tenure_months'] + 1)\n",
    "    \n",
    "    # 3. Risk indicators\n",
    "    df_fe['is_new_customer'] = (df_fe['tenure_months'] < 6).astype(int)\n",
    "    df_fe['high_support_calls'] = (df_fe['support_calls'] > 3).astype(int)\n",
    "    df_fe['high_charges'] = (df_fe['monthly_charges'] > df_fe['monthly_charges'].median()).astype(int)\n",
    "    \n",
    "    # 4. Contract stability\n",
    "    df_fe['has_long_contract'] = (df_fe['contract_type'].isin(['One year', 'Two year'])).astype(int)\n",
    "    df_fe['electronic_payment'] = (df_fe['payment_method'] == 'Electronic').astype(int)\n",
    "    \n",
    "    # 5. Age groups\n",
    "    df_fe['age_group'] = pd.cut(df_fe['age'], \n",
    "                                bins=[0, 30, 45, 60, 100],\n",
    "                                labels=['Young', 'Middle', 'Senior', 'Elderly'])\n",
    "    \n",
    "    # 6. Tenure groups\n",
    "    df_fe['tenure_group'] = pd.cut(df_fe['tenure_months'],\n",
    "                                   bins=[-1, 6, 24, 48, 100],\n",
    "                                   labels=['New', 'Recent', 'Established', 'Loyal'])\n",
    "    \n",
    "    return df_fe\n",
    "\n",
    "# Apply feature engineering\n",
    "churn_fe = engineer_features(churn_df)\n",
    "\n",
    "print(\"New Features Created:\")\n",
    "new_features = [col for col in churn_fe.columns if col not in churn_df.columns]\n",
    "for feat in new_features:\n",
    "    print(f\"  - {feat}\")\n",
    "\n",
    "print(f\"\\nTotal features: {len(churn_fe.columns)}\")\n",
    "churn_fe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762c1884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Data Preparation for Modeling\n",
    "# ============================================================\n",
    "\n",
    "# Separate features and target\n",
    "X = churn_fe.drop(['customer_id', 'churn'], axis=1)\n",
    "y = churn_fe['churn']\n",
    "\n",
    "# Encode categorical variables\n",
    "categorical_features = X.select_dtypes(include=['object', 'category']).columns\n",
    "print(f\"Encoding {len(categorical_features)} categorical features:\")\n",
    "for feat in categorical_features:\n",
    "    print(f\"  - {feat}\")\n",
    "\n",
    "# One-hot encoding\n",
    "X_encoded = pd.get_dummies(X, columns=categorical_features, drop_first=True)\n",
    "\n",
    "print(f\"\\nFeature matrix shape after encoding: {X_encoded.shape}\")\n",
    "print(f\"Number of features: {X_encoded.shape[1]}\")\n",
    "\n",
    "# Train-test split (stratified)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_encoded, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "print(f\"Train churn rate: {y_train.mean()*100:.1f}%\")\n",
    "print(f\"Test churn rate: {y_test.mean()*100:.1f}%\")\n",
    "\n",
    "# Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"\\nFeatures scaled using StandardScaler\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258df08d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 3: Model Training and Comparison\n",
    "\n",
    "We'll train multiple models and compare their performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e048bbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Train Multiple Models with Class Weights\n",
    "# ============================================================\n",
    "\n",
    "# Note: We use class_weight='balanced' to handle imbalance without increasing dataset size\n",
    "# This is more cost-effective than oversampling techniques\n",
    "\n",
    "print(\"Class distribution:\")\n",
    "print(f\"  Class 0 (No churn): {(y_train == 0).sum()} ({(y_train == 0).mean()*100:.1f}%)\")\n",
    "print(f\"  Class 1 (Churn): {(y_train == 1).sum()} ({(y_train == 1).mean()*100:.1f}%)\\n\")\n",
    "\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000, class_weight='balanced'),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=-1, \n",
    "                                           class_weight='balanced', max_depth=10)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "print(\"Training models...\\n\")\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    \n",
    "    # Train on original data (not oversampled)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "    \n",
    "    # Metrics\n",
    "    results[name] = {\n",
    "        'model': model,\n",
    "        'predictions': y_pred,\n",
    "        'probabilities': y_pred_proba,\n",
    "        'roc_auc': roc_auc_score(y_test, y_pred_proba),\n",
    "        'f1_score': f1_score(y_test, y_pred)\n",
    "    }\n",
    "    \n",
    "    print(f\"  ROC-AUC: {results[name]['roc_auc']:.4f}\")\n",
    "    print(f\"  F1 Score: {results[name]['f1_score']:.4f}\\n\")\n",
    "\n",
    "# Select best model (by ROC-AUC)\n",
    "best_model_name = max(results, key=lambda x: results[x]['roc_auc'])\n",
    "best_model = results[best_model_name]['model']\n",
    "\n",
    "print(f\"Best model: {best_model_name}\")\n",
    "print(f\"ROC-AUC: {results[best_model_name]['roc_auc']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f277b8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Detailed Evaluation of Best Model\n",
    "# ============================================================\n",
    "\n",
    "y_pred_best = results[best_model_name]['predictions']\n",
    "y_pred_proba_best = results[best_model_name]['probabilities']\n",
    "\n",
    "print(f\"Detailed Evaluation: {best_model_name}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_best, \n",
    "                          target_names=['No Churn', 'Churn']))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_best)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Visualize confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['No Churn', 'Churn'],\n",
    "            yticklabels=['No Churn', 'Churn'])\n",
    "plt.title(f'Confusion Matrix - {best_model_name}')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba_best)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr, tpr, label=f'{best_model_name} (AUC = {results[best_model_name][\"roc_auc\"]:.4f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Feature importance (for tree-based models)\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X_encoded.columns,\n",
    "        'importance': best_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 10 Most Important Features:\")\n",
    "    print(feature_importance.head(10).to_string(index=False))\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    feature_importance.head(15).plot(x='feature', y='importance', kind='barh')\n",
    "    plt.xlabel('Importance')\n",
    "    plt.title('Feature Importance')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceaec4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Business Metrics Analysis\n",
    "# ============================================================\n",
    "\n",
    "# Calculate business impact\n",
    "print(\"Business Impact Analysis\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Assumptions\n",
    "avg_customer_value = 1000  # Average customer lifetime value\n",
    "retention_cost = 100  # Cost to retain one customer\n",
    "retention_success_rate = 0.3  # 30% of interventions successful\n",
    "\n",
    "# Get churners and predictions\n",
    "actual_churners = (y_test == 1).sum()\n",
    "predicted_churners = (y_pred_best == 1).sum()\n",
    "\n",
    "# True positives (correctly identified churners)\n",
    "true_positives = ((y_test == 1) & (y_pred_best == 1)).sum()\n",
    "\n",
    "# Business calculations\n",
    "potential_saves = true_positives * retention_success_rate\n",
    "saved_revenue = potential_saves * avg_customer_value\n",
    "retention_costs = predicted_churners * retention_cost\n",
    "net_benefit = saved_revenue - retention_costs\n",
    "\n",
    "print(f\"\\nActual churners in test set: {actual_churners}\")\n",
    "print(f\"Predicted churners: {predicted_churners}\")\n",
    "print(f\"Correctly identified churners: {true_positives}\")\n",
    "print(f\"\\nExpected successful retentions: {potential_saves:.0f}\")\n",
    "print(f\"Saved revenue: ${saved_revenue:,.0f}\")\n",
    "print(f\"Retention costs: ${retention_costs:,.0f}\")\n",
    "print(f\"Net benefit: ${net_benefit:,.0f}\")\n",
    "\n",
    "# Calculate ROI\n",
    "roi = (net_benefit / retention_costs) * 100 if retention_costs > 0 else 0\n",
    "print(f\"\\nReturn on Investment: {roi:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7dbb04",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 5: Model Deployment\n",
    "\n",
    "Deploy the best model to SageMaker for production use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e800ed55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Package Model for Deployment\n",
    "# ============================================================\n",
    "\n",
    "import joblib\n",
    "import tarfile\n",
    "import shutil\n",
    "\n",
    "# Create model directory\n",
    "model_dir = \"churn_model\"\n",
    "code_dir = os.path.join(model_dir, \"code\")\n",
    "os.makedirs(code_dir, exist_ok=True)\n",
    "\n",
    "# Save model and preprocessing objects\n",
    "joblib.dump(best_model, os.path.join(model_dir, \"model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"scaler.pkl\"))\n",
    "joblib.dump(X_encoded.columns.tolist(), os.path.join(model_dir, \"feature_names.pkl\"))\n",
    "\n",
    "print(\"Model artifacts saved:\")\n",
    "print(f\"  - model.pkl\")\n",
    "print(f\"  - scaler.pkl\")\n",
    "print(f\"  - feature_names.pkl\")\n",
    "\n",
    "# Create inference script\n",
    "inference_code = '''\n",
    "import os\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    \"\"\"Load model and preprocessing objects\"\"\"\n",
    "    model = joblib.load(os.path.join(model_dir, \"model.pkl\"))\n",
    "    scaler = joblib.load(os.path.join(model_dir, \"scaler.pkl\"))\n",
    "    feature_names = joblib.load(os.path.join(model_dir, \"feature_names.pkl\"))\n",
    "    \n",
    "    return {\n",
    "        \"model\": model,\n",
    "        \"scaler\": scaler,\n",
    "        \"feature_names\": feature_names\n",
    "    }\n",
    "\n",
    "def input_fn(request_body, content_type):\n",
    "    \"\"\"Parse input data\"\"\"\n",
    "    if content_type == \"application/json\":\n",
    "        data = json.loads(request_body)\n",
    "        return pd.DataFrame([data])\n",
    "    raise ValueError(f\"Unsupported content type: {content_type}\")\n",
    "\n",
    "def predict_fn(input_data, model_dict):\n",
    "    \"\"\"Make prediction\"\"\"\n",
    "    model = model_dict[\"model\"]\n",
    "    scaler = model_dict[\"scaler\"]\n",
    "    feature_names = model_dict[\"feature_names\"]\n",
    "    \n",
    "    # Ensure all expected features are present\n",
    "    for feat in feature_names:\n",
    "        if feat not in input_data.columns:\n",
    "            input_data[feat] = 0\n",
    "    \n",
    "    # Select and order features\n",
    "    input_data = input_data[feature_names]\n",
    "    \n",
    "    # Scale\n",
    "    input_scaled = scaler.transform(input_data)\n",
    "    \n",
    "    # Predict\n",
    "    prediction = model.predict(input_scaled)[0]\n",
    "    probability = model.predict_proba(input_scaled)[0]\n",
    "    \n",
    "    return {\n",
    "        \"churn_prediction\": int(prediction),\n",
    "        \"churn_probability\": float(probability[1]),\n",
    "        \"retention_recommended\": bool(probability[1] > 0.5)\n",
    "    }\n",
    "\n",
    "def output_fn(prediction, accept_type):\n",
    "    \"\"\"Format output\"\"\"\n",
    "    if accept_type == \"application/json\":\n",
    "        return json.dumps(prediction), accept_type\n",
    "    raise ValueError(f\"Unsupported accept type: {accept_type}\")\n",
    "'''\n",
    "\n",
    "with open(os.path.join(code_dir, \"inference.py\"), \"w\") as f:\n",
    "    f.write(inference_code)\n",
    "\n",
    "# Create requirements\n",
    "with open(os.path.join(code_dir, \"requirements.txt\"), \"w\") as f:\n",
    "    f.write(\"scikit-learn==1.3.0\\nnumpy==1.24.3\\npandas==2.0.3\\njoblib==1.3.1\\n\")\n",
    "\n",
    "print(\"\\nInference code created\")\n",
    "\n",
    "# Also create inference.py at root level for SageMaker deployment\n",
    "with open(\"inference.py\", \"w\") as f:\n",
    "    f.write(inference_code)\n",
    "\n",
    "# Create tar.gz\n",
    "tar_path = \"churn_model.tar.gz\"\n",
    "with tarfile.open(tar_path, \"w:gz\") as tar:\n",
    "    tar.add(model_dir, arcname=\".\")\n",
    "\n",
    "print(f\"\\nModel package created: {tar_path}\")\n",
    "\n",
    "# Upload to S3\n",
    "s3_client = boto3.client('s3')\n",
    "model_s3_key = \"lab2-churn-model/model.tar.gz\"\n",
    "s3_client.upload_file(tar_path, bucket, model_s3_key)\n",
    "model_s3_uri = f\"s3://{bucket}/{model_s3_key}\"\n",
    "\n",
    "print(f\"Model uploaded to: {model_s3_uri}\")\n",
    "\n",
    "# Cleanup (keep inference.py for deployment)\n",
    "shutil.rmtree(model_dir)\n",
    "os.remove(tar_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3decfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Deploy to SageMaker Endpoint\n",
    "# ============================================================\n",
    "\n",
    "from sagemaker.sklearn import SKLearnModel\n",
    "import time\n",
    "\n",
    "# Create model\n",
    "sklearn_model = SKLearnModel(\n",
    "    model_data=model_s3_uri,\n",
    "    role=role,\n",
    "    entry_point=\"inference.py\",\n",
    "    framework_version=\"1.2-1\",\n",
    "    py_version=\"py3\",\n",
    "    sagemaker_session=session\n",
    ")\n",
    "\n",
    "# Deploy\n",
    "endpoint_name = f\"churn-prediction-{int(time.time())}\"\n",
    "\n",
    "print(f\"Deploying model to endpoint: {endpoint_name}\")\n",
    "print(\"This will take 4-6 minutes...\")\n",
    "\n",
    "predictor = sklearn_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.t2.medium\",\n",
    "    endpoint_name=endpoint_name\n",
    ")\n",
    "\n",
    "print(f\"\\nEndpoint deployed successfully!\")\n",
    "print(f\"Endpoint name: {endpoint_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4a08a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Test the Deployed Endpoint\n",
    "# ============================================================\n",
    "\n",
    "# Create test customer\n",
    "test_customer = {\n",
    "    'age': 45,\n",
    "    'gender_Male': 1,\n",
    "    'tenure_months': 3,\n",
    "    'has_internet': 1,\n",
    "    'has_phone_service': 1,\n",
    "    'has_streaming': 0,\n",
    "    'has_tech_support': 0,\n",
    "    'contract_type_One year': 0,\n",
    "    'contract_type_Two year': 0,\n",
    "    'monthly_charges': 85.0,\n",
    "    'support_calls': 4\n",
    "}\n",
    "\n",
    "print(\"Testing endpoint with sample customer:\")\n",
    "print(json.dumps(test_customer, indent=2))\n",
    "\n",
    "# Invoke endpoint\n",
    "result = predictor.predict(test_customer)\n",
    "\n",
    "print(f\"\\nPrediction result:\")\n",
    "print(json.dumps(result, indent=2))\n",
    "\n",
    "if result['retention_recommended']:\n",
    "    print(\"\\n‚ö†Ô∏è HIGH CHURN RISK - Retention action recommended\")\n",
    "else:\n",
    "    print(\"\\n‚úì LOW CHURN RISK - Customer likely to stay\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373b63e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cleanup\n",
    "# ============================================================\n",
    "\n",
    "print(\"Cleaning up endpoint...\")\n",
    "\n",
    "try:\n",
    "    predictor.delete_endpoint()\n",
    "    print(f\"Endpoint deleted: {endpoint_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error deleting endpoint: {e}\")\n",
    "\n",
    "print(\"\\nLab 2 complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c67be2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary and Key Learnings\n",
    "\n",
    "### What You Accomplished\n",
    "\n",
    "1. **Data Analysis**:\n",
    "   - Generated realistic churn dataset\n",
    "   - Performed EDA to understand churn drivers\n",
    "   - Identified key risk factors\n",
    "\n",
    "2. **Feature Engineering**:\n",
    "   - Created derived features (value metrics, engagement)\n",
    "   - Encoded categorical variables\n",
    "   - Scaled numerical features\n",
    "\n",
    "3. **Handled Class Imbalance**:\n",
    "   - Applied SMOTE for balanced training\n",
    "   - Increased minority class representation\n",
    "   - Maintained test set distribution\n",
    "\n",
    "4. **Model Development**:\n",
    "   - Trained multiple classification models\n",
    "   - Compared performance metrics\n",
    "   - Selected best model based on ROC-AUC\n",
    "\n",
    "5. **SageMaker Feature Store**:\n",
    "   - Created Feature Group for customer features\n",
    "   - Configured online and offline stores\n",
    "   - Centralized feature management for reusability\n",
    "\n",
    "6. **SageMaker Training Jobs**:\n",
    "   - Launched managed training on ml.m5.xlarge\n",
    "   - Tracked experiments with SageMaker Experiments\n",
    "   - Automated hyperparameter and metric logging\n",
    "\n",
    "7. **Model Registry**:\n",
    "   - Registered model with version control\n",
    "   - Configured approval workflow\n",
    "   - Enabled deployment tracking\n",
    "\n",
    "8. **Business Impact**:\n",
    "   - Calculated expected revenue savings\n",
    "   - Estimated ROI of retention program\n",
    "   - Provided actionable recommendations\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "**Churn Prediction Insights:**\n",
    "- New customers (< 6 months) have highest churn risk\n",
    "- Month-to-month contracts correlate with churn\n",
    "- High support calls indicate dissatisfaction\n",
    "- Service engagement reduces churn\n",
    "\n",
    "**Technical Best Practices:**\n",
    "- Always handle class imbalance in churn data\n",
    "- Feature engineering significantly improves performance\n",
    "- Use ROC-AUC for imbalanced classification\n",
    "- Feature Store enables feature reusability across teams\n",
    "\n",
    "**SageMaker MLOps:**\n",
    "- Training Jobs provide managed, scalable training\n",
    "- Experiments automatically track runs and metrics\n",
    "- Model Registry enables governance and approvals\n",
    "- Feature Store centralizes feature definitions\n",
    "\n",
    "**Business Value:**\n",
    "- Early churn prediction enables proactive retention\n",
    "- Targeting high-risk customers optimizes costs\n",
    "- 30% retention success can generate significant ROI\n",
    "- Model provides actionable customer insights\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "**Lab 3**: Text Classification with NLP techniques\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c958731",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üè™ SageMaker Feature Store: Centralized Feature Management\n",
    "\n",
    "Feature Store permet de :\n",
    "- **Stocker** les features de mani√®re centralis√©e\n",
    "- **Servir** des features pour l'entra√Ænement (batch) et l'inf√©rence (temps r√©el)\n",
    "- **Versionner** automatiquement les features\n",
    "- **Partager** les features entre √©quipes\n",
    "\n",
    "### Pourquoi Feature Store pour le Churn ?\n",
    "- R√©utiliser les features client calcul√©es\n",
    "- Assurer la coh√©rence entre entra√Ænement et production\n",
    "- Acc√®s temps r√©el aux derni√®res donn√©es client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30675136",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.feature_store.feature_group import FeatureGroup\n",
    "from sagemaker.feature_store.feature_definition import (\n",
    "    FeatureDefinition,\n",
    "    FeatureTypeEnum,\n",
    ")\n",
    "import time\n",
    "\n",
    "# Create Feature Group for customer features\n",
    "feature_group_name = f\"customer-churn-features-{int(time.time())}\"\n",
    "\n",
    "# Prepare data with required columns\n",
    "feature_data = churn_df.copy()\n",
    "feature_data['customer_id'] = feature_data['customer_id']  # Already exists\n",
    "feature_data['event_time'] = pd.Timestamp.now().isoformat()\n",
    "\n",
    "# Define feature definitions\n",
    "feature_definitions = [\n",
    "    FeatureDefinition(feature_name='customer_id', feature_type=FeatureTypeEnum.STRING),\n",
    "    FeatureDefinition(feature_name='event_time', feature_type=FeatureTypeEnum.STRING),\n",
    "]\n",
    "\n",
    "# Add all other columns as features\n",
    "for col in feature_data.columns:\n",
    "    if col not in ['customer_id', 'event_time']:\n",
    "        if feature_data[col].dtype in ['int64', 'float64']:\n",
    "            feature_definitions.append(\n",
    "                FeatureDefinition(feature_name=col, feature_type=FeatureTypeEnum.FRACTIONAL)\n",
    "            )\n",
    "        else:\n",
    "            feature_definitions.append(\n",
    "                FeatureDefinition(feature_name=col, feature_type=FeatureTypeEnum.STRING)\n",
    "            )\n",
    "\n",
    "print(f\"üìä Defined {len(feature_definitions)} features\")\n",
    "\n",
    "# Create Feature Group with feature definitions\n",
    "feature_group = FeatureGroup(\n",
    "    name=feature_group_name,\n",
    "    sagemaker_session=session,\n",
    "    feature_definitions=feature_definitions\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Creating Feature Group: {feature_group_name}\")\n",
    "print(f\"üí° Feature Store will be created in S3 and available for online/offline access\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0794e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Feature Group (Note: This may take a few minutes)\n",
    "try:\n",
    "    feature_group.create(\n",
    "        s3_uri=f\"s3://{bucket}/feature-store/customer-churn\",\n",
    "        record_identifier_name='customer_id',\n",
    "        event_time_feature_name='event_time',\n",
    "        role_arn=role,\n",
    "        enable_online_store=True  # For real-time inference\n",
    "    )\n",
    "    \n",
    "    print(f\"‚è≥ Waiting for Feature Group to be created...\")\n",
    "    status = feature_group.describe().get(\"FeatureGroupStatus\")\n",
    "    while status == \"Creating\":\n",
    "        time.sleep(5)\n",
    "        status = feature_group.describe().get(\"FeatureGroupStatus\")\n",
    "    \n",
    "    print(f\"‚úÖ Feature Group created: {feature_group_name}\")\n",
    "    print(f\"   Status: {status}\")\n",
    "    print(f\"   Online Store: Enabled (for real-time predictions)\")\n",
    "    print(f\"   Offline Store: s3://{bucket}/feature-store/customer-churn\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Note: Feature Group creation can take time. Error: {str(e)}\")\n",
    "    print(f\"üí° You can check status in SageMaker Console ‚Üí Feature Store\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6430ee6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üßπ Cleanup Resources\n",
    "\n",
    "Nettoyez les ressources AWS cr√©√©es dans ce lab."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
