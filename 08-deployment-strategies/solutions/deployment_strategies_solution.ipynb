{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0f1ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Environment Setup\n",
    "# ============================================================\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "project_root = os.path.abspath('../..')\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.sklearn import SKLearnModel\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.deserializers import CSVDeserializer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "try:\n",
    "    from utils.sagemaker_config import get_sagemaker_config\n",
    "    config = get_sagemaker_config(s3_prefix='lab8-deployment-strategies')\n",
    "    role = config['role']\n",
    "    session = config['session']\n",
    "    bucket = config['bucket']\n",
    "    region = config['region']\n",
    "except ImportError:\n",
    "    role = get_execution_role()\n",
    "    session = sagemaker.Session()\n",
    "    bucket = session.default_bucket()\n",
    "    region = session.boto_region_name\n",
    "\n",
    "print(f\"Configuration complete\")\n",
    "print(f\"Region: {region}\")\n",
    "print(f\"S3 Bucket: s3://{bucket}\")\n",
    "\n",
    "sm_client = boto3.client('sagemaker', region_name=region)\n",
    "sm_runtime = boto3.client('sagemaker-runtime', region_name=region)\n",
    "cloudwatch = boto3.client('cloudwatch', region_name=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6ee699",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 1: PrÃ©parer Deux Versions de ModÃ¨le\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee4d35b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 1: SageMaker Projects - MLOps Template for Fraud Detection\n",
    "\n",
    "**SageMaker Projects** provides MLOps templates with built-in CI/CD pipelines for production ML workflows.\n",
    "\n",
    "### What is a SageMaker Project?\n",
    "\n",
    "SageMaker Projects are pre-configured templates that include:\n",
    "- **Source Code Repository** (CodeCommit or GitHub)\n",
    "- **CI/CD Pipeline** (CodePipeline + CodeBuild)\n",
    "- **Model Registry Integration**\n",
    "- **Automated Deployment** (Dev, Staging, Production)\n",
    "- **Infrastructure as Code** (CloudFormation)\n",
    "\n",
    "### Benefits for Fraud Detection\n",
    "\n",
    "| Feature | Benefit |\n",
    "|---------|---------|\n",
    "| Model Versioning | Track all fraud model versions |\n",
    "| Automated Training | Retrain when new fraud patterns detected |\n",
    "| Multi-Environment | Test in staging before production |\n",
    "| Approval Workflow | Manual approval for fraud model changes |\n",
    "| Audit Trail | Complete history of model deployments |\n",
    "\n",
    "### Project Templates Available\n",
    "\n",
    "1. **MLOps template for model building, training, and deployment**\n",
    "   - End-to-end CI/CD for fraud detection\n",
    "   - Automated retraining pipeline\n",
    "   - Multi-account deployment\n",
    "\n",
    "2. **MLOps template for model deployment**\n",
    "   - Focus on deployment strategies\n",
    "   - Blue/Green and Canary deployments\n",
    "   - Rollback capabilities\n",
    "\n",
    "3. **Custom templates**\n",
    "   - Organization-specific workflows\n",
    "   - Compliance requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e64c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Create SageMaker Project for Fraud Detection\n",
    "# ============================================================\n",
    "\n",
    "print(\"SageMaker Projects - Fraud Detection MLOps Pipeline\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Note: Projects require SageMaker Studio and appropriate IAM permissions\n",
    "# This demonstrates the API; actual creation is best done via Console first\n",
    "\n",
    "project_name = \"fraud-detection-mlops\"\n",
    "\n",
    "# Example: Create project using built-in template\n",
    "project_template_example = \"\"\"\n",
    "# Via AWS Console:\n",
    "# 1. Open SageMaker Studio\n",
    "# 2. Go to Projects â†’ Create Project\n",
    "# 3. Select: \"MLOps template for model building, training, and deployment\"\n",
    "# 4. Project name: fraud-detection-mlops\n",
    "# 5. This creates:\n",
    "#    - CodeCommit repo with pipeline code\n",
    "#    - SageMaker Pipeline for training\n",
    "#    - CodePipeline for deployment\n",
    "#    - Model Registry integration\n",
    "#    - Dev and Prod endpoints\n",
    "\n",
    "# Via CLI:\n",
    "aws sagemaker create-project \\\\\n",
    "    --project-name fraud-detection-mlops \\\\\n",
    "    --service-catalog-provisioning-details '{\n",
    "        \"ProductId\": \"prod-xxxxxx\",\n",
    "        \"ProvisioningArtifactId\": \"pa-xxxxxx\"\n",
    "    }' \\\\\n",
    "    --tags Key=UseCase,Value=FraudDetection\n",
    "\"\"\"\n",
    "\n",
    "print(project_template_example)\n",
    "\n",
    "# List available project templates\n",
    "sm_client = boto3.client('sagemaker')\n",
    "\n",
    "print(\"\\n\\nğŸ“‹ Checking SageMaker Project Templates:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "try:\n",
    "    # List service catalog products (project templates)\n",
    "    sc_client = boto3.client('servicecatalog')\n",
    "    \n",
    "    # Note: This requires appropriate permissions\n",
    "    print(\"Available templates (requires SageMaker Studio and IAM permissions):\")\n",
    "    print(\"1. MLOps template for model building, training, and deployment\")\n",
    "    print(\"2. MLOps template for model deployment\")\n",
    "    print(\"3. Custom organizational templates\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Note: Service Catalog access requires SageMaker Studio setup\")\n",
    "    print(f\"Typically configured through SageMaker Studio UI\")\n",
    "\n",
    "print(\"\\n\\nğŸ—ï¸  Project Structure (once created):\")\n",
    "print(\"-\" * 60)\n",
    "print(\"\"\"\n",
    "fraud-detection-mlops/\n",
    "â”œâ”€â”€ ğŸ“ CodeCommit Repository\n",
    "â”‚   â”œâ”€â”€ pipelines/\n",
    "â”‚   â”‚   â”œâ”€â”€ fraud/\n",
    "â”‚   â”‚   â”‚   â”œâ”€â”€ pipeline.py          # Training pipeline definition\n",
    "â”‚   â”‚   â”‚   â”œâ”€â”€ preprocess.py        # Data preprocessing\n",
    "â”‚   â”‚   â”‚   â”œâ”€â”€ evaluate.py          # Model evaluation\n",
    "â”‚   â”‚   â”‚   â””â”€â”€ requirements.txt\n",
    "â”‚   â”‚   â””â”€â”€ codebuild-buildspec.yml  # CI/CD build spec\n",
    "â”‚   â”œâ”€â”€ tests/                       # Unit tests\n",
    "â”‚   â””â”€â”€ README.md\n",
    "â”‚\n",
    "â”œâ”€â”€ ğŸ”„ CodePipeline (Build)\n",
    "â”‚   â”œâ”€â”€ Source (CodeCommit trigger)\n",
    "â”‚   â”œâ”€â”€ Build (CodeBuild)\n",
    "â”‚   â””â”€â”€ Deploy Pipeline to SageMaker\n",
    "â”‚\n",
    "â”œâ”€â”€ ğŸš€ CodePipeline (Deploy)\n",
    "â”‚   â”œâ”€â”€ Source (Model Registry approval)\n",
    "â”‚   â”œâ”€â”€ DeployStaging (Test environment)\n",
    "â”‚   â”œâ”€â”€ Manual Approval\n",
    "â”‚   â””â”€â”€ DeployProd (Production)\n",
    "â”‚\n",
    "â””â”€â”€ ğŸ“Š Model Registry\n",
    "    â””â”€â”€ fraud-detection-model-group\n",
    "        â”œâ”€â”€ Version 1 (Approved)\n",
    "        â”œâ”€â”€ Version 2 (PendingApproval)\n",
    "        â””â”€â”€ Version 3 (Draft)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e252dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Explore SageMaker Pipeline (Created by Project)\n",
    "# ============================================================\n",
    "\n",
    "print(\"SageMaker Pipeline - Automated Training Workflow\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Once project is created, it generates a SageMaker Pipeline\n",
    "# Let's show what a fraud detection pipeline would contain\n",
    "\n",
    "pipeline_structure = \"\"\"\n",
    "Pipeline Name: fraud-detection-pipeline\n",
    "\n",
    "Steps:\n",
    "1. ğŸ“¥ Data Processing\n",
    "   - Input: Raw fraud transaction data from S3\n",
    "   - Process: Feature engineering, train/test split\n",
    "   - Output: Processed data for training\n",
    "\n",
    "2. ğŸ‹ï¸  Model Training\n",
    "   - Algorithm: XGBoost with SMOTE\n",
    "   - Hyperparameters: Auto-tuned or configured\n",
    "   - Output: Trained model artifact\n",
    "\n",
    "3. ğŸ“Š Model Evaluation\n",
    "   - Metrics: Precision, Recall, F1, ROC-AUC\n",
    "   - Threshold: Minimum performance criteria\n",
    "   - Decision: Register model if metrics pass\n",
    "\n",
    "4. ğŸ·ï¸  Conditional Model Registration\n",
    "   - Condition: If evaluation passes threshold\n",
    "   - Action: Register to Model Registry\n",
    "   - Status: PendingManualApproval\n",
    "\n",
    "5. ğŸš€ Trigger Deployment Pipeline\n",
    "   - Event: Model approval in registry\n",
    "   - Action: Start deployment to staging\n",
    "\"\"\"\n",
    "\n",
    "print(pipeline_structure)\n",
    "\n",
    "# Example: Query existing pipelines\n",
    "print(\"\\n\\nğŸ” Checking Existing Pipelines:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "try:\n",
    "    pipelines = sm_client.list_pipelines(\n",
    "        MaxResults=10\n",
    "    )\n",
    "    \n",
    "    if pipelines['PipelineSummaries']:\n",
    "        print(f\"Found {len(pipelines['PipelineSummaries'])} pipelines:\")\n",
    "        for pipeline in pipelines['PipelineSummaries']:\n",
    "            print(f\"  â€¢ {pipeline['PipelineName']}\")\n",
    "            print(f\"    Created: {pipeline['CreationTime']}\")\n",
    "    else:\n",
    "        print(\"No pipelines found yet.\")\n",
    "        print(\"Pipelines are automatically created when you create a SageMaker Project.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Note: {e}\")\n",
    "\n",
    "print(\"\\n\\nğŸ’¡ Pipeline Benefits for Fraud Detection:\")\n",
    "print(\"-\" * 60)\n",
    "print(\"\"\"\n",
    "âœ“ Automated Retraining\n",
    "  - Schedule: Daily/weekly checks for data drift\n",
    "  - Trigger: New fraud patterns detected\n",
    "  - Action: Automatic pipeline execution\n",
    "\n",
    "âœ“ Consistent Training Process\n",
    "  - Same steps every time\n",
    "  - Version-controlled code\n",
    "  - Reproducible results\n",
    "\n",
    "âœ“ Built-in Quality Gates\n",
    "  - Minimum AUC threshold (e.g., 0.85)\n",
    "  - Precision/recall requirements\n",
    "  - Only deploy models that pass\n",
    "\n",
    "âœ“ CI/CD Integration\n",
    "  - Code change â†’ Pipeline update\n",
    "  - Model approval â†’ Auto-deployment\n",
    "  - Rollback capability\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e79c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Model Registry Integration with Projects\n",
    "# ============================================================\n",
    "\n",
    "print(\"Model Registry - Approval Workflow for Fraud Models\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# SageMaker Projects integrate with Model Registry for approval workflow\n",
    "\n",
    "# Example: List model packages (versions) in registry\n",
    "model_package_group_name = \"fraud-detection-model-group\"\n",
    "\n",
    "print(f\"\\nğŸ“¦ Model Package Group: {model_package_group_name}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "try:\n",
    "    # Check if model package group exists\n",
    "    response = sm_client.list_model_package_groups(\n",
    "        NameContains='fraud'\n",
    "    )\n",
    "    \n",
    "    if response['ModelPackageGroupSummaryList']:\n",
    "        print(f\"Found {len(response['ModelPackageGroupSummaryList'])} fraud-related model groups:\")\n",
    "        for group in response['ModelPackageGroupSummaryList']:\n",
    "            print(f\"\\n  ğŸ“ {group['ModelPackageGroupName']}\")\n",
    "            print(f\"     Created: {group['CreationTime']}\")\n",
    "            \n",
    "            # List versions in this group\n",
    "            versions = sm_client.list_model_packages(\n",
    "                ModelPackageGroupName=group['ModelPackageGroupName'],\n",
    "                MaxResults=5,\n",
    "                SortBy='CreationTime',\n",
    "                SortOrder='Descending'\n",
    "            )\n",
    "            \n",
    "            if versions['ModelPackageSummaryList']:\n",
    "                print(f\"     Versions: {len(versions['ModelPackageSummaryList'])}\")\n",
    "                for version in versions['ModelPackageSummaryList']:\n",
    "                    status = version['ModelApprovalStatus']\n",
    "                    status_icon = \"âœ…\" if status == \"Approved\" else \"â³\" if status == \"PendingManualApproval\" else \"âŒ\"\n",
    "                    print(f\"       {status_icon} v{version['ModelPackageVersion']}: {status}\")\n",
    "    else:\n",
    "        print(\"No fraud model groups found yet.\")\n",
    "        print(\"\\nğŸ’¡ Model Registry is automatically created with SageMaker Projects\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Info: {e}\")\n",
    "    print(\"\\nğŸ’¡ Model Registry Setup:\")\n",
    "\n",
    "print(\"\\n\\nğŸ”„ Approval Workflow in Projects:\")\n",
    "print(\"-\" * 60)\n",
    "print(\"\"\"\n",
    "1. Pipeline Training Completes\n",
    "   â”œâ”€ Model artifacts saved to S3\n",
    "   â”œâ”€ Evaluation metrics calculated\n",
    "   â””â”€ IF metrics pass â†’ Register model\n",
    "\n",
    "2. Model Registered (PendingManualApproval)\n",
    "   â”œâ”€ Notification sent to ML team\n",
    "   â”œâ”€ Review metrics dashboard\n",
    "   â””â”€ Manual review required\n",
    "\n",
    "3. Data Scientist Approves Model\n",
    "   â”œâ”€ Change status: PendingManualApproval â†’ Approved\n",
    "   â””â”€ Triggers deployment pipeline automatically\n",
    "\n",
    "4. Deployment Pipeline Executes\n",
    "   â”œâ”€ Deploy to Staging environment\n",
    "   â”œâ”€ Run integration tests\n",
    "   â”œâ”€ Manual approval gate\n",
    "   â””â”€ Deploy to Production\n",
    "\n",
    "5. Monitor in Production\n",
    "   â”œâ”€ Model Monitor tracks performance\n",
    "   â”œâ”€ Data drift detection\n",
    "   â””â”€ Triggers retraining if needed\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95129490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Example: Programmatic Model Approval\n",
    "# ============================================================\n",
    "\n",
    "print(\"Programmatic Model Approval Example\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# This shows how to approve a model programmatically\n",
    "# In practice, this would be done after reviewing metrics\n",
    "\n",
    "approval_example = \"\"\"\n",
    "# Scenario: Pipeline trained a new fraud model\n",
    "# Model registered as: fraud-detection-model-group version 3\n",
    "# Status: PendingManualApproval\n",
    "\n",
    "# Step 1: Review model metrics\n",
    "model_package_arn = \"arn:aws:sagemaker:region:account:model-package/fraud-detection-model-group/3\"\n",
    "\n",
    "# Retrieve model metrics from Model Registry\n",
    "response = sm_client.describe_model_package(\n",
    "    ModelPackageName=model_package_arn\n",
    ")\n",
    "\n",
    "metrics = response.get('ModelMetrics', {})\n",
    "print(\"Model Performance:\")\n",
    "print(f\"  AUC: {metrics.get('auc', 'N/A')}\")\n",
    "print(f\"  Precision: {metrics.get('precision', 'N/A')}\")\n",
    "print(f\"  Recall: {metrics.get('recall', 'N/A')}\")\n",
    "\n",
    "# Step 2: If metrics meet requirements, approve\n",
    "# Decision criteria for fraud detection:\n",
    "# - AUC >= 0.85 (good discrimination)\n",
    "# - Precision >= 0.80 (low false positives)\n",
    "# - Recall >= 0.75 (catch most fraud)\n",
    "\n",
    "if meets_criteria:\n",
    "    # Approve the model\n",
    "    sm_client.update_model_package(\n",
    "        ModelPackageArn=model_package_arn,\n",
    "        ModelApprovalStatus='Approved',\n",
    "        ApprovalDescription='Model meets production criteria for fraud detection'\n",
    "    )\n",
    "    print(\"âœ… Model approved - Deployment pipeline will start automatically\")\n",
    "else:\n",
    "    # Reject the model\n",
    "    sm_client.update_model_package(\n",
    "        ModelPackageArn=model_package_arn,\n",
    "        ModelApprovalStatus='Rejected',\n",
    "        ApprovalDescription='Model does not meet minimum performance thresholds'\n",
    "    )\n",
    "    print(\"âŒ Model rejected - Pipeline needs retraining\")\n",
    "\"\"\"\n",
    "\n",
    "print(approval_example)\n",
    "\n",
    "print(\"\\n\\nğŸ¯ Approval Criteria for Fraud Detection:\")\n",
    "print(\"-\" * 60)\n",
    "print(\"\"\"\n",
    "Metric                  Threshold    Rationale\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "ROC-AUC                â‰¥ 0.85       Strong discrimination between fraud/legitimate\n",
    "Precision              â‰¥ 0.80       Limit false positives (customer inconvenience)\n",
    "Recall                 â‰¥ 0.75       Catch majority of fraudulent transactions\n",
    "F1-Score               â‰¥ 0.77       Balanced performance\n",
    "Data Drift Score       < 0.30       Model still relevant to current data\n",
    "Latency (P95)          < 100ms      Real-time transaction approval\n",
    "Model Size             < 500MB      Fast deployment and scaling\n",
    "\n",
    "Compliance Checks:\n",
    "âœ“ Model training data retention (audit trail)\n",
    "âœ“ Feature importance explainability\n",
    "âœ“ Bias testing across demographics\n",
    "âœ“ Security scan (no vulnerabilities)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86647c59",
   "metadata": {},
   "source": [
    "### ğŸ” Monitoring Project Status\n",
    "\n",
    "Une fois le projet crÃ©Ã©, vous pouvez surveiller son Ã©tat et ses exÃ©cutions via la console SageMaker Studio ou programmatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de648d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Monitor SageMaker Project Executions\n",
    "# ============================================================\n",
    "\n",
    "print(\"Monitoring Project Pipelines and Deployments\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check project status\n",
    "project_name = \"fraud-detection-mlops\"\n",
    "\n",
    "try:\n",
    "    # Get project details\n",
    "    project = sm_client.describe_project(\n",
    "        ProjectName=project_name\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nğŸ“ Project: {project['ProjectName']}\")\n",
    "    print(f\"   Status: {project['ProjectStatus']}\")\n",
    "    print(f\"   Created: {project['CreationTime']}\")\n",
    "    print(f\"   ID: {project['ProjectId']}\")\n",
    "    \n",
    "except sm_client.exceptions.ResourceNotFound:\n",
    "    print(f\"\\nProject '{project_name}' not found.\")\n",
    "    print(\"ğŸ’¡ Create project via SageMaker Studio â†’ Projects â†’ Create Project\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nDemo mode: {e}\")\n",
    "\n",
    "print(\"\\n\\nğŸ”„ Recent Pipeline Executions:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Show how to monitor pipeline executions\n",
    "monitoring_example = \"\"\"\n",
    "# List recent pipeline executions\n",
    "pipeline_name = \"fraud-detection-pipeline\"\n",
    "\n",
    "executions = sm_client.list_pipeline_executions(\n",
    "    PipelineName=pipeline_name,\n",
    "    MaxResults=10,\n",
    "    SortBy='CreationTime',\n",
    "    SortOrder='Descending'\n",
    ")\n",
    "\n",
    "for execution in executions['PipelineExecutionSummaries']:\n",
    "    status = execution['PipelineExecutionStatus']\n",
    "    status_icon = \"âœ…\" if status == \"Succeeded\" else \"ğŸ”„\" if status == \"Executing\" else \"âŒ\"\n",
    "    \n",
    "    print(f\"{status_icon} Execution: {execution['PipelineExecutionDisplayName']}\")\n",
    "    print(f\"   Status: {status}\")\n",
    "    print(f\"   Started: {execution['StartTime']}\")\n",
    "    \n",
    "    # Get execution details\n",
    "    details = sm_client.describe_pipeline_execution(\n",
    "        PipelineExecutionArn=execution['PipelineExecutionArn']\n",
    "    )\n",
    "    \n",
    "    # Show step status\n",
    "    steps = sm_client.list_pipeline_execution_steps(\n",
    "        PipelineExecutionArn=execution['PipelineExecutionArn']\n",
    "    )\n",
    "    \n",
    "    print(\"   Steps:\")\n",
    "    for step in steps['PipelineExecutionSteps']:\n",
    "        step_status = step['StepStatus']\n",
    "        step_icon = \"âœ…\" if step_status == \"Succeeded\" else \"ğŸ”„\" if step_status == \"Executing\" else \"âŒ\"\n",
    "        print(f\"     {step_icon} {step['StepName']}: {step_status}\")\n",
    "\"\"\"\n",
    "\n",
    "print(monitoring_example)\n",
    "\n",
    "print(\"\\n\\nğŸ“Š Deployment Status:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "deployment_monitoring = \"\"\"\n",
    "# Monitor CodePipeline deployment status\n",
    "codepipeline = boto3.client('codepipeline')\n",
    "\n",
    "pipeline_name = \"sagemaker-fraud-detection-mlops-modeldeploy\"\n",
    "\n",
    "# Get pipeline state\n",
    "response = codepipeline.get_pipeline_state(\n",
    "    name=pipeline_name\n",
    ")\n",
    "\n",
    "print(f\"Deployment Pipeline: {pipeline_name}\")\n",
    "print(f\"Updated: {response['updated']}\")\n",
    "print()\n",
    "\n",
    "for stage in response['stageStates']:\n",
    "    print(f\"Stage: {stage['stageName']}\")\n",
    "    \n",
    "    for action in stage.get('actionStates', []):\n",
    "        status = action.get('latestExecution', {}).get('status', 'N/A')\n",
    "        status_icon = \"âœ…\" if status == \"Succeeded\" else \"ğŸ”„\" if status == \"InProgress\" else \"âŒ\"\n",
    "        \n",
    "        print(f\"  {status_icon} {action['actionName']}: {status}\")\n",
    "        \n",
    "        if action['actionName'] == 'DeployStaging':\n",
    "            print(f\"      â†’ Staging endpoint: fraud-detection-staging\")\n",
    "        elif action['actionName'] == 'DeployProd':\n",
    "            print(f\"      â†’ Production endpoint: fraud-detection-prod\")\n",
    "\"\"\"\n",
    "\n",
    "print(deployment_monitoring)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1e6bf9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“š RÃ©sumÃ©: SageMaker Projects vs Manual Deployment\n",
    "\n",
    "**Comparaison des approches pour la dÃ©tection de fraude:**\n",
    "\n",
    "| Aspect | Manual Deployment | SageMaker Projects |\n",
    "|--------|-------------------|-------------------|\n",
    "| **Setup** | Configure chaque composant manuellement | Template prÃ©-configurÃ© avec CI/CD |\n",
    "| **Code Management** | Notebooks locaux ou scripts ad-hoc | Repository Git avec versioning |\n",
    "| **Training** | Lancer Training Jobs manuellement | Pipeline automatisÃ© avec dÃ©clencheurs |\n",
    "| **Quality Gates** | VÃ©rification manuelle des mÃ©triques | Seuils automatiques + approbation |\n",
    "| **Deployment** | CrÃ©er endpoints manuellement | Pipeline multi-environnements (Dev/Staging/Prod) |\n",
    "| **Rollback** | Processus manuel complexe | Rollback automatique si Ã©chec |\n",
    "| **Monitoring** | Configuration manuelle de Model Monitor | IntÃ©grÃ© avec alertes CloudWatch |\n",
    "| **Collaboration** | Partage de notebooks | Code review via pull requests |\n",
    "| **Audit Trail** | Logs dispersÃ©s | Historique complet des dÃ©ploiements |\n",
    "| **ScalabilitÃ©** | RÃ©pÃ©tition manuelle pour nouveaux modÃ¨les | RÃ©utilisation du template |\n",
    "\n",
    "**Recommandation pour Production:**\n",
    "- **POC/Experimentation**: Manual deployment (plus rapide pour tester)\n",
    "- **Production**: SageMaker Projects (governance, automation, collaboration)\n",
    "\n",
    "**Prochaine section**: Nous allons maintenant explorer les stratÃ©gies de dÃ©ploiement (Blue/Green, Canary) en utilisant les modÃ¨les crÃ©Ã©s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6e6539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CrÃ©er deux modÃ¨les (v1 et v2)\n",
    "# ============================================================\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import joblib\n",
    "import tarfile\n",
    "\n",
    "# GÃ©nÃ©rer des donnÃ©es\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "X = np.random.randn(n_samples, 5)\n",
    "y = (X[:, 0] + X[:, 1] > 0).astype(int)\n",
    "\n",
    "# ModÃ¨le v1 - Simple (baseline)\n",
    "model_v1 = LogisticRegression(random_state=42)\n",
    "model_v1.fit(X, y)\n",
    "\n",
    "# ModÃ¨le v2 - Plus complexe (improved)\n",
    "model_v2 = RandomForestClassifier(n_estimators=50, max_depth=10, random_state=42)\n",
    "model_v2.fit(X, y)\n",
    "\n",
    "# Fonction pour sauvegarder et uploader\n",
    "def save_and_upload_model(model, version):\n",
    "    joblib.dump(model, 'model.pkl')\n",
    "    \n",
    "    with tarfile.open('model.tar.gz', 'w:gz') as tar:\n",
    "        tar.add('model.pkl')\n",
    "    \n",
    "    model_s3_key = f'deployment-models/{version}/model.tar.gz'\n",
    "    boto3.client('s3').upload_file('model.tar.gz', bucket, model_s3_key)\n",
    "    model_s3_uri = f's3://{bucket}/{model_s3_key}'\n",
    "    \n",
    "    os.remove('model.pkl')\n",
    "    os.remove('model.tar.gz')\n",
    "    \n",
    "    return model_s3_uri\n",
    "\n",
    "model_v1_uri = save_and_upload_model(model_v1, 'v1')\n",
    "model_v2_uri = save_and_upload_model(model_v2, 'v2')\n",
    "\n",
    "print(f\"Model v1 (Logistic): {model_v1_uri}\")\n",
    "print(f\"Model v2 (RandomForest): {model_v2_uri}\")\n",
    "\n",
    "# DonnÃ©es de test\n",
    "test_data = np.array([[0.5, 0.3, -0.2, 0.1, 0.4]])\n",
    "print(f\"\\nTest data shape: {test_data.shape}\")\n",
    "\n",
    "# Comparer les prÃ©dictions\n",
    "pred_v1 = model_v1.predict_proba(test_data)[0]\n",
    "pred_v2 = model_v2.predict_proba(test_data)[0]\n",
    "\n",
    "print(f\"\\nModel v1 proba: {pred_v1}\")\n",
    "print(f\"Model v2 proba: {pred_v2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ceaa527",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 2: Blue/Green Deployment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c690338a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Ã‰tape 1: DÃ©ployer la version Blue (v1)\n",
    "# ============================================================\n",
    "\n",
    "# CrÃ©er le modÃ¨le v1 (Blue)\n",
    "sklearn_model_v1 = SKLearnModel(\n",
    "    model_data=model_v1_uri,\n",
    "    role=role,\n",
    "    framework_version='1.2-1',\n",
    "    py_version='py3',\n",
    "    name=f'model-v1-{int(time.time())}'\n",
    ")\n",
    "\n",
    "# DÃ©ployer Blue\n",
    "endpoint_name_bg = f'blue-green-endpoint-{int(time.time())}'\n",
    "\n",
    "predictor_blue = sklearn_model_v1.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m5.large',\n",
    "    endpoint_name=endpoint_name_bg,\n",
    "    serializer=CSVSerializer(),\n",
    "    deserializer=CSVDeserializer()\n",
    ")\n",
    "\n",
    "print(f\"Blue deployment (v1) complete\")\n",
    "print(f\"Endpoint: {endpoint_name_bg}\")\n",
    "\n",
    "# Tester Blue\n",
    "result_blue = predictor_blue.predict(test_data)\n",
    "print(f\"Blue prediction: {result_blue}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4089bc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Ã‰tape 2: DÃ©ployer la version Green (v2) sur le mÃªme endpoint\n",
    "# ============================================================\n",
    "\n",
    "# CrÃ©er le modÃ¨le v2 (Green)\n",
    "sklearn_model_v2 = SKLearnModel(\n",
    "    model_data=model_v2_uri,\n",
    "    role=role,\n",
    "    framework_version='1.2-1',\n",
    "    py_version='py3',\n",
    "    name=f'model-v2-{int(time.time())}'\n",
    ")\n",
    "\n",
    "# CrÃ©er la configuration Green\n",
    "production_variants = [\n",
    "    {\n",
    "        'VariantName': 'Blue',\n",
    "        'ModelName': sklearn_model_v1.name,\n",
    "        'InstanceType': 'ml.m5.large',\n",
    "        'InitialInstanceCount': 1,\n",
    "        'InitialVariantWeight': 1.0  # 100% du trafic\n",
    "    },\n",
    "    {\n",
    "        'VariantName': 'Green',\n",
    "        'ModelName': sklearn_model_v2.name,\n",
    "        'InstanceType': 'ml.m5.large',\n",
    "        'InitialInstanceCount': 1,\n",
    "        'InitialVariantWeight': 0.0  # 0% du trafic (shadow)\n",
    "    }\n",
    "]\n",
    "\n",
    "# CrÃ©er la nouvelle configuration\n",
    "endpoint_config_name = f'bg-config-{int(time.time())}'\n",
    "\n",
    "sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=production_variants\n",
    ")\n",
    "\n",
    "# Mettre Ã  jour l'endpoint\n",
    "sm_client.update_endpoint(\n",
    "    EndpointName=endpoint_name_bg,\n",
    "    EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "\n",
    "print(f\"\\nGreen deployment (v2) started\")\n",
    "print(f\"Endpoint config: {endpoint_config_name}\")\n",
    "print(\"Waiting for update to complete...\")\n",
    "\n",
    "# Attendre la mise Ã  jour\n",
    "waiter = sm_client.get_waiter('endpoint_in_service')\n",
    "waiter.wait(EndpointName=endpoint_name_bg)\n",
    "\n",
    "print(\"Green deployment complete (0% traffic)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c0561d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Ã‰tape 3: Basculer le trafic de Blue vers Green\n",
    "# ============================================================\n",
    "\n",
    "# Tester d'abord avec TargetVariant\n",
    "print(\"Testing Green variant before switching traffic...\")\n",
    "\n",
    "response = sm_runtime.invoke_endpoint(\n",
    "    EndpointName=endpoint_name_bg,\n",
    "    Body=','.join(map(str, test_data[0])),\n",
    "    ContentType='text/csv',\n",
    "    TargetVariant='Green'\n",
    ")\n",
    "\n",
    "result_green = response['Body'].read().decode()\n",
    "print(f\"Green prediction: {result_green}\")\n",
    "\n",
    "# Switch progressif: 50% Blue, 50% Green\n",
    "print(\"\\nSwitching to 50/50 traffic split...\")\n",
    "\n",
    "updated_variants = [\n",
    "    {\n",
    "        'VariantName': 'Blue',\n",
    "        'DesiredWeight': 50.0\n",
    "    },\n",
    "    {\n",
    "        'VariantName': 'Green',\n",
    "        'DesiredWeight': 50.0\n",
    "    }\n",
    "]\n",
    "\n",
    "sm_client.update_endpoint_weights_and_capacities(\n",
    "    EndpointName=endpoint_name_bg,\n",
    "    DesiredWeightsAndCapacities=updated_variants\n",
    ")\n",
    "\n",
    "print(\"Traffic split: 50% Blue, 50% Green\")\n",
    "time.sleep(5)\n",
    "\n",
    "# Tester la rÃ©partition\n",
    "print(\"\\nTesting traffic distribution (10 requests):\")\n",
    "results = {'Blue': 0, 'Green': 0}\n",
    "\n",
    "for i in range(10):\n",
    "    response = sm_runtime.invoke_endpoint(\n",
    "        EndpointName=endpoint_name_bg,\n",
    "        Body=','.join(map(str, test_data[0])),\n",
    "        ContentType='text/csv'\n",
    "    )\n",
    "    invoked_variant = response['ResponseMetadata']['HTTPHeaders'].get('x-amzn-invoked-production-variant', 'Unknown')\n",
    "    results[invoked_variant] = results.get(invoked_variant, 0) + 1\n",
    "\n",
    "print(f\"Results: {results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0b85cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Ã‰tape 4: Basculer 100% vers Green\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nSwitching to 100% Green (full cutover)...\")\n",
    "\n",
    "final_variants = [\n",
    "    {\n",
    "        'VariantName': 'Blue',\n",
    "        'DesiredWeight': 0.0\n",
    "    },\n",
    "    {\n",
    "        'VariantName': 'Green',\n",
    "        'DesiredWeight': 100.0\n",
    "    }\n",
    "]\n",
    "\n",
    "sm_client.update_endpoint_weights_and_capacities(\n",
    "    EndpointName=endpoint_name_bg,\n",
    "    DesiredWeightsAndCapacities=final_variants\n",
    ")\n",
    "\n",
    "print(\"Traffic: 100% Green\")\n",
    "time.sleep(5)\n",
    "\n",
    "# VÃ©rifier\n",
    "print(\"\\nTesting final distribution (10 requests):\")\n",
    "results_final = {'Blue': 0, 'Green': 0}\n",
    "\n",
    "for i in range(10):\n",
    "    response = sm_runtime.invoke_endpoint(\n",
    "        EndpointName=endpoint_name_bg,\n",
    "        Body=','.join(map(str, test_data[0])),\n",
    "        ContentType='text/csv'\n",
    "    )\n",
    "    invoked_variant = response['ResponseMetadata']['HTTPHeaders'].get('x-amzn-invoked-production-variant', 'Unknown')\n",
    "    results_final[invoked_variant] = results_final.get(invoked_variant, 0) + 1\n",
    "\n",
    "print(f\"Results: {results_final}\")\n",
    "print(\"\\nBlue/Green deployment complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f6ffab",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 3: Canary Deployment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af673b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Canary Deployment: DÃ©ploiement progressif\n",
    "# ============================================================\n",
    "\n",
    "# CrÃ©er un nouvel endpoint pour Canary\n",
    "endpoint_name_canary = f'canary-endpoint-{int(time.time())}'\n",
    "\n",
    "# Configuration initiale: 90% Blue, 10% Canary\n",
    "canary_variants = [\n",
    "    {\n",
    "        'VariantName': 'Production',\n",
    "        'ModelName': sklearn_model_v1.name,\n",
    "        'InstanceType': 'ml.m5.large',\n",
    "        'InitialInstanceCount': 1,\n",
    "        'InitialVariantWeight': 90.0\n",
    "    },\n",
    "    {\n",
    "        'VariantName': 'Canary',\n",
    "        'ModelName': sklearn_model_v2.name,\n",
    "        'InstanceType': 'ml.m5.large',\n",
    "        'InitialInstanceCount': 1,\n",
    "        'InitialVariantWeight': 10.0\n",
    "    }\n",
    "]\n",
    "\n",
    "canary_config_name = f'canary-config-{int(time.time())}'\n",
    "\n",
    "sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=canary_config_name,\n",
    "    ProductionVariants=canary_variants\n",
    ")\n",
    "\n",
    "sm_client.create_endpoint(\n",
    "    EndpointName=endpoint_name_canary,\n",
    "    EndpointConfigName=canary_config_name\n",
    ")\n",
    "\n",
    "print(f\"Canary endpoint created: {endpoint_name_canary}\")\n",
    "print(\"Initial split: 90% Production, 10% Canary\")\n",
    "print(\"Waiting for endpoint to be in service...\")\n",
    "\n",
    "waiter = sm_client.get_waiter('endpoint_in_service')\n",
    "waiter.wait(EndpointName=endpoint_name_canary)\n",
    "\n",
    "print(\"Canary endpoint ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b888b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Tester la distribution Canary\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nTesting Canary distribution (100 requests):\")\n",
    "\n",
    "canary_results = {'Production': 0, 'Canary': 0, 'Unknown': 0}\n",
    "\n",
    "for i in range(100):\n",
    "    try:\n",
    "        response = sm_runtime.invoke_endpoint(\n",
    "            EndpointName=endpoint_name_canary,\n",
    "            Body=','.join(map(str, test_data[0])),\n",
    "            ContentType='text/csv'\n",
    "        )\n",
    "        variant = response['ResponseMetadata']['HTTPHeaders'].get('x-amzn-invoked-production-variant', 'Unknown')\n",
    "        canary_results[variant] = canary_results.get(variant, 0) + 1\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        break\n",
    "\n",
    "print(f\"\\nDistribution:\")\n",
    "print(f\"Production: {canary_results['Production']}%\")\n",
    "print(f\"Canary: {canary_results['Canary']}%\")\n",
    "print(f\"Unknown: {canary_results.get('Unknown', 0)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29f76ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Progression Canary: 10% â†’ 25% â†’ 50% â†’ 100%\n",
    "# ============================================================\n",
    "\n",
    "canary_stages = [\n",
    "    {'Production': 75.0, 'Canary': 25.0},\n",
    "    {'Production': 50.0, 'Canary': 50.0},\n",
    "    {'Production': 0.0, 'Canary': 100.0}\n",
    "]\n",
    "\n",
    "for i, stage in enumerate(canary_stages, start=2):\n",
    "    print(f\"\\nCanary Stage {i}: Production {stage['Production']}%, Canary {stage['Canary']}%\")\n",
    "    \n",
    "    updated = [\n",
    "        {'VariantName': 'Production', 'DesiredWeight': stage['Production']},\n",
    "        {'VariantName': 'Canary', 'DesiredWeight': stage['Canary']}\n",
    "    ]\n",
    "    \n",
    "    sm_client.update_endpoint_weights_and_capacities(\n",
    "        EndpointName=endpoint_name_canary,\n",
    "        DesiredWeightsAndCapacities=updated\n",
    "    )\n",
    "    \n",
    "    print(\"Waiting 10 seconds for monitoring...\")\n",
    "    time.sleep(10)\n",
    "    \n",
    "    # Tester la distribution\n",
    "    test_results = {'Production': 0, 'Canary': 0}\n",
    "    for j in range(20):\n",
    "        response = sm_runtime.invoke_endpoint(\n",
    "            EndpointName=endpoint_name_canary,\n",
    "            Body=','.join(map(str, test_data[0])),\n",
    "            ContentType='text/csv'\n",
    "        )\n",
    "        variant = response['ResponseMetadata']['HTTPHeaders'].get('x-amzn-invoked-production-variant', 'Unknown')\n",
    "        test_results[variant] = test_results.get(variant, 0) + 1\n",
    "    \n",
    "    print(f\"Verified: Production {test_results['Production']}, Canary {test_results['Canary']}\")\n",
    "\n",
    "print(\"\\nâœ… Canary deployment complete - 100% on new version\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ec066c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 4: Monitoring et MÃ©triques CloudWatch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63141796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# RÃ©cupÃ©rer les mÃ©triques CloudWatch\n",
    "# ============================================================\n",
    "\n",
    "def get_endpoint_metrics(endpoint_name, variant_name, metric_name, minutes=10):\n",
    "    \"\"\"RÃ©cupÃ¨re les mÃ©triques CloudWatch pour un endpoint\"\"\"\n",
    "    \n",
    "    end_time = datetime.utcnow()\n",
    "    start_time = end_time - timedelta(minutes=minutes)\n",
    "    \n",
    "    response = cloudwatch.get_metric_statistics(\n",
    "        Namespace='AWS/SageMaker',\n",
    "        MetricName=metric_name,\n",
    "        Dimensions=[\n",
    "            {'Name': 'EndpointName', 'Value': endpoint_name},\n",
    "            {'Name': 'VariantName', 'Value': variant_name}\n",
    "        ],\n",
    "        StartTime=start_time,\n",
    "        EndTime=end_time,\n",
    "        Period=60,\n",
    "        Statistics=['Average', 'Sum']\n",
    "    )\n",
    "    \n",
    "    return response['Datapoints']\n",
    "\n",
    "# MÃ©triques pour Blue/Green endpoint\n",
    "print(\"CloudWatch Metrics pour Blue/Green Endpoint:\\n\")\n",
    "\n",
    "metrics_to_check = ['Invocations', 'ModelLatency', 'Invocation4XXErrors', 'Invocation5XXErrors']\n",
    "\n",
    "for variant in ['Blue', 'Green']:\n",
    "    print(f\"\\n{variant} Variant:\")\n",
    "    for metric in metrics_to_check:\n",
    "        datapoints = get_endpoint_metrics(endpoint_name_bg, variant, metric, minutes=15)\n",
    "        if datapoints:\n",
    "            latest = sorted(datapoints, key=lambda x: x['Timestamp'])[-1]\n",
    "            value = latest.get('Sum', latest.get('Average', 0))\n",
    "            print(f\"  {metric}: {value:.2f}\")\n",
    "        else:\n",
    "            print(f\"  {metric}: No data\")\n",
    "\n",
    "# MÃ©triques pour Canary endpoint\n",
    "print(\"\\n\\nCloudWatch Metrics pour Canary Endpoint:\\n\")\n",
    "\n",
    "for variant in ['Production', 'Canary']:\n",
    "    print(f\"\\n{variant} Variant:\")\n",
    "    for metric in metrics_to_check:\n",
    "        datapoints = get_endpoint_metrics(endpoint_name_canary, variant, metric, minutes=15)\n",
    "        if datapoints:\n",
    "            latest = sorted(datapoints, key=lambda x: x['Timestamp'])[-1]\n",
    "            value = latest.get('Sum', latest.get('Average', 0))\n",
    "            print(f\"  {metric}: {value:.2f}\")\n",
    "        else:\n",
    "            print(f\"  {metric}: No data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b7efe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CrÃ©er une alarme CloudWatch\n",
    "# ============================================================\n",
    "\n",
    "alarm_name = f'canary-error-rate-{int(time.time())}'\n",
    "\n",
    "# Alarme sur le taux d'erreur 4XX/5XX\n",
    "cloudwatch.put_metric_alarm(\n",
    "    AlarmName=alarm_name,\n",
    "    ComparisonOperator='GreaterThanThreshold',\n",
    "    EvaluationPeriods=2,\n",
    "    MetricName='Invocation4XXErrors',\n",
    "    Namespace='AWS/SageMaker',\n",
    "    Period=60,\n",
    "    Statistic='Sum',\n",
    "    Threshold=10.0,\n",
    "    ActionsEnabled=False,  # DÃ©sactivÃ© pour le lab\n",
    "    AlarmDescription='Alert when Canary has high error rate',\n",
    "    Dimensions=[\n",
    "        {'Name': 'EndpointName', 'Value': endpoint_name_canary},\n",
    "        {'Name': 'VariantName', 'Value': 'Canary'}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"CloudWatch Alarm crÃ©Ã©e: {alarm_name}\")\n",
    "print(\"Cette alarme se dÃ©clencherait si >10 erreurs 4XX en 2 minutes\")\n",
    "print(\"\\nEn production, l'alarme pourrait dÃ©clencher:\")\n",
    "print(\"- Rollback automatique\")\n",
    "print(\"- Notification SNS\")\n",
    "print(\"- Lambda pour corriger\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd6fa1c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 5: Rollback et A/B Testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1c787a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Rollback: Revenir Ã  la version prÃ©cÃ©dente\n",
    "# ============================================================\n",
    "\n",
    "print(\"Simulation d'un problÃ¨me dÃ©tectÃ© sur Green...\")\n",
    "print(\"Initiating ROLLBACK...\\n\")\n",
    "\n",
    "# Rollback = remettre 100% sur Blue\n",
    "rollback_variants = [\n",
    "    {\n",
    "        'VariantName': 'Blue',\n",
    "        'DesiredWeight': 100.0\n",
    "    },\n",
    "    {\n",
    "        'VariantName': 'Green',\n",
    "        'DesiredWeight': 0.0\n",
    "    }\n",
    "]\n",
    "\n",
    "sm_client.update_endpoint_weights_and_capacities(\n",
    "    EndpointName=endpoint_name_bg,\n",
    "    DesiredWeightsAndCapacities=rollback_variants\n",
    ")\n",
    "\n",
    "print(\"âœ… Rollback complete - 100% sur Blue (version stable)\")\n",
    "time.sleep(5)\n",
    "\n",
    "# VÃ©rifier\n",
    "print(\"\\nTesting aprÃ¨s rollback (10 requests):\")\n",
    "rollback_results = {'Blue': 0, 'Green': 0}\n",
    "\n",
    "for i in range(10):\n",
    "    response = sm_runtime.invoke_endpoint(\n",
    "        EndpointName=endpoint_name_bg,\n",
    "        Body=','.join(map(str, test_data[0])),\n",
    "        ContentType='text/csv'\n",
    "    )\n",
    "    variant = response['ResponseMetadata']['HTTPHeaders'].get('x-amzn-invoked-production-variant', 'Unknown')\n",
    "    rollback_results[variant] = rollback_results.get(variant, 0) + 1\n",
    "\n",
    "print(f\"Results: {rollback_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11be295e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# A/B Testing avec analyse statistique\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nA/B Testing: Comparer les performances de 2 variants\\n\")\n",
    "\n",
    "# Configuration 50/50 pour A/B test\n",
    "ab_variants = [\n",
    "    {\n",
    "        'VariantName': 'Blue',\n",
    "        'DesiredWeight': 50.0\n",
    "    },\n",
    "    {\n",
    "        'VariantName': 'Green',\n",
    "        'DesiredWeight': 50.0\n",
    "    }\n",
    "]\n",
    "\n",
    "sm_client.update_endpoint_weights_and_capacities(\n",
    "    EndpointName=endpoint_name_bg,\n",
    "    DesiredWeightsAndCapacities=ab_variants\n",
    ")\n",
    "\n",
    "print(\"A/B Test configuration: 50% Blue, 50% Green\")\n",
    "time.sleep(5)\n",
    "\n",
    "# Collecter les mÃ©triques\n",
    "ab_results = {\n",
    "    'Blue': {'latencies': [], 'predictions': []},\n",
    "    'Green': {'latencies': [], 'predictions': []}\n",
    "}\n",
    "\n",
    "print(\"\\nRunning A/B test (50 requests)...\")\n",
    "for i in range(50):\n",
    "    start = time.time()\n",
    "    response = sm_runtime.invoke_endpoint(\n",
    "        EndpointName=endpoint_name_bg,\n",
    "        Body=','.join(map(str, test_data[0])),\n",
    "        ContentType='text/csv'\n",
    "    )\n",
    "    latency = (time.time() - start) * 1000\n",
    "    \n",
    "    variant = response['ResponseMetadata']['HTTPHeaders'].get('x-amzn-invoked-production-variant', 'Unknown')\n",
    "    prediction = response['Body'].read().decode()\n",
    "    \n",
    "    if variant in ab_results:\n",
    "        ab_results[variant]['latencies'].append(latency)\n",
    "        ab_results[variant]['predictions'].append(prediction)\n",
    "\n",
    "# Analyser les rÃ©sultats\n",
    "print(\"\\nA/B Test Results:\\n\")\n",
    "for variant, data in ab_results.items():\n",
    "    if data['latencies']:\n",
    "        avg_latency = np.mean(data['latencies'])\n",
    "        p50_latency = np.percentile(data['latencies'], 50)\n",
    "        p95_latency = np.percentile(data['latencies'], 95)\n",
    "        \n",
    "        print(f\"{variant}:\")\n",
    "        print(f\"  Requests: {len(data['latencies'])}\")\n",
    "        print(f\"  Avg Latency: {avg_latency:.2f}ms\")\n",
    "        print(f\"  P50 Latency: {p50_latency:.2f}ms\")\n",
    "        print(f\"  P95 Latency: {p95_latency:.2f}ms\")\n",
    "        print()\n",
    "\n",
    "print(\"DÃ©cision A/B Testing:\")\n",
    "if len(ab_results['Green']['latencies']) > 0 and len(ab_results['Blue']['latencies']) > 0:\n",
    "    green_avg = np.mean(ab_results['Green']['latencies'])\n",
    "    blue_avg = np.mean(ab_results['Blue']['latencies'])\n",
    "    \n",
    "    if green_avg < blue_avg * 1.1:  # Green pas plus de 10% plus lent\n",
    "        print(\"âœ… Green wins - Better or similar performance\")\n",
    "    else:\n",
    "        print(\"âŒ Blue wins - Green is significantly slower\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524a9749",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 6: Cleanup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef07f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cleanup\n",
    "# ============================================================\n",
    "\n",
    "print(\"Cleaning up resources...\\n\")\n",
    "\n",
    "# Supprimer les endpoints\n",
    "endpoints_to_delete = [endpoint_name_bg, endpoint_name_canary]\n",
    "\n",
    "for ep_name in endpoints_to_delete:\n",
    "    try:\n",
    "        sm_client.delete_endpoint(EndpointName=ep_name)\n",
    "        print(f\"âœ… Deleted endpoint: {ep_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Could not delete {ep_name}: {e}\")\n",
    "\n",
    "# Supprimer l'alarme CloudWatch\n",
    "try:\n",
    "    cloudwatch.delete_alarms(AlarmNames=[alarm_name])\n",
    "    print(f\"âœ… Deleted alarm: {alarm_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Could not delete alarm: {e}\")\n",
    "\n",
    "print(\"\\nCleanup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11ccbea",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## RÃ©sumÃ©\n",
    "\n",
    "Dans ce lab, vous avez:\n",
    "\n",
    "1. **ImplÃ©mentÃ© Blue/Green Deployment**\n",
    "   - DÃ©ployÃ© deux versions simultanÃ©ment\n",
    "   - BasculÃ© progressivement le trafic (0% â†’ 50% â†’ 100%)\n",
    "   - EffectuÃ© un rollback rapide\n",
    "\n",
    "2. **ConfigurÃ© Canary Deployment**\n",
    "   - DÃ©ploiement progressif (10% â†’ 25% â†’ 50% â†’ 100%)\n",
    "   - Monitoring Ã  chaque Ã©tape\n",
    "   - Validation avant progression\n",
    "\n",
    "3. **MonitorÃ© avec CloudWatch**\n",
    "   - MÃ©triques de latence et d'erreurs\n",
    "   - Alarmes pour dÃ©tection automatique\n",
    "   - Analyse comparative des variants\n",
    "\n",
    "4. **TestÃ© A/B Testing**\n",
    "   - Comparaison statistique de performances\n",
    "   - DÃ©cision basÃ©e sur les donnÃ©es\n",
    "   - Distribution 50/50 du trafic\n",
    "\n",
    "5. **GÃ©rÃ© les rollbacks**\n",
    "   - Retour rapide Ã  la version stable\n",
    "   - Sans downtime\n",
    "   - Automatisable via alarmes\n",
    "\n",
    "### Comparaison des StratÃ©gies\n",
    "\n",
    "| StratÃ©gie | Risque | Vitesse | Rollback | Cas d'Usage |\n",
    "|-----------|--------|---------|----------|-------------|\n",
    "| **Blue/Green** | Moyen | Rapide | InstantanÃ© | Mises Ã  jour majeures |\n",
    "| **Canary** | Faible | Progressive | Facile | Production critique |\n",
    "| **A/B Testing** | Faible | Lente | N/A | Optimisation |\n",
    "| **Shadow** | TrÃ¨s faible | Variable | N/A | Validation |\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Blue/Green**: IdÃ©al pour basculement rapide avec rollback instantanÃ©\n",
    "2. **Canary**: PrÃ©fÃ©rÃ© en production pour minimiser les risques\n",
    "3. **Monitoring**: Toujours surveiller les mÃ©triques (latence, erreurs, coÃ»t)\n",
    "4. **Alarmes**: Configurer des seuils pour rollback automatique\n",
    "5. **Tests**: Valider chaque variant avant d'augmenter le trafic\n",
    "\n",
    "### MÃ©triques ClÃ©s Ã  Surveiller\n",
    "\n",
    "- **Latency**: P50, P95, P99\n",
    "- **Error Rate**: 4XX, 5XX\n",
    "- **Throughput**: Invocations/minute\n",
    "- **Model Performance**: Accuracy, drift\n",
    "- **Cost**: Instance utilization\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- ImplÃ©menter Lambda pour rollback automatique\n",
    "- IntÃ©grer avec CI/CD (CodePipeline)\n",
    "- Ajouter Model Monitor pour data drift\n",
    "- Configurer SNS pour notifications\n",
    "- Automatiser les tests de charge\n",
    "\n",
    "---\n",
    "\n",
    "**Best Practice**: Toujours utiliser Canary pour les dÃ©ploiements en production critique\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
