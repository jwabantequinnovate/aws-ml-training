{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b99d5c14",
   "metadata": {},
   "source": [
    "# Amazon Bedrock : Comparaison et Guardrails via SDK\n",
    "\n",
    "Ce notebook montre comment :\n",
    "- Lister les modèles disponibles\n",
    "- Interroger un modèle Bedrock\n",
    "- Comparer plusieurs modèles avec l'API Bedrock\n",
    "- Utiliser les guardrails (filtrage, sécurité) via l'API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e6b921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Initialisation du client Bedrock\n",
    "import boto3\n",
    "import json\n",
    "import pandas as pd\n",
    "region = 'us-east-1'\n",
    "bedrock_runtime = boto3.client('bedrock-runtime', region_name=region)\n",
    "bedrock_client = boto3.client('bedrock', region_name=region)\n",
    "print(\"Bedrock clients initialisés.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed742aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Lister les modèles disponibles\n",
    "try:\n",
    "    response = bedrock_client.list_foundation_models()\n",
    "    models_data = []\n",
    "    for model in response['modelSummaries']:\n",
    "        models_data.append({\n",
    "            'Model ID': model['modelId'],\n",
    "            'Model Name': model['modelName'],\n",
    "            'Provider': model['providerName']\n",
    "        })\n",
    "    models_df = pd.DataFrame(models_data)\n",
    "    print(models_df.to_string(index=False))\n",
    "except Exception as e:\n",
    "    print(f\"Erreur : {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19abd752",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 2: Foundation Model Comparison\n",
    "\n",
    "We'll compare several popular models across different criteria:\n",
    "- **Claude 3 (Anthropic)**: High-quality reasoning, long context\n",
    "- **Titan (Amazon)**: Cost-effective, AWS-native\n",
    "- **Llama 3 (Meta)**: Open-source, customizable\n",
    "\n",
    "### Evaluation Criteria\n",
    "\n",
    "| Criterion | Description | Weight |\n",
    "|-----------|-------------|--------|\n",
    "| Quality | Output coherence and accuracy | High |\n",
    "| Speed | Inference latency | Medium |\n",
    "| Cost | Price per 1K tokens | High |\n",
    "| Context Length | Maximum input tokens | Medium |\n",
    "| Safety | Built-in content filtering | High |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8c0e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Appel simple d'un modèle Bedrock\n",
    "prompt = \"Explique le principe de la rétropropagation en apprentissage profond.\"\n",
    "model_id = 'anthropic.claude-3-sonnet-20240229-v1:0'\n",
    "body = json.dumps({\n",
    "    \"max_tokens\": 300,\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_p\": 0.9,\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "})\n",
    "response = bedrock_runtime.invoke_model(modelId=model_id, body=body)\n",
    "result = json.loads(response['body'].read())\n",
    "print(result.get('content', [{}])[0].get('text', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc15217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Fonction d'invocation universelle\n",
    "\n",
    "def invoke_model(model_id, prompt, max_tokens=300):\n",
    "    body = json.dumps({\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.9,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    })\n",
    "    response = bedrock_runtime.invoke_model(\n",
    "        modelId=model_id,\n",
    "        body=body\n",
    "    )\n",
    "    result = json.loads(response['body'].read())\n",
    "    # Claude: result['content'][0]['text']\n",
    "    # Titan: result['results'][0]['outputText']\n",
    "    # Llama: result.get('generation', result.get('content', [{}])[0].get('text', ''))\n",
    "    # On tente d'extraire le texte peu importe le modèle\n",
    "    text = result.get('generation') or result.get('content', [{}])[0].get('text') or result.get('results', [{}])[0].get('outputText')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb14c313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Comparer plusieurs modèles avec l'API Bedrock\n",
    "try:\n",
    "    compare_response = bedrock_runtime.compare_models(\n",
    "        modelIds=[\n",
    "            'anthropic.claude-3-sonnet-20240229-v1:0',\n",
    "            'amazon.titan-text-express-v1',\n",
    "            'meta.llama3-70b-instruct-v1:0'\n",
    "        ],\n",
    "        input={\n",
    "            'prompt': \"Explique le principe de la rétropropagation en apprentissage profond.\"\n",
    "        },\n",
    "        evaluationConfig={\n",
    "            'criteria': ['quality', 'coherence'],\n",
    "            'maxTokens': 300\n",
    "        }\n",
    "    )\n",
    "    print(\"Comparaison des modèles Bedrock :\")\n",
    "    for result in compare_response['results']:\n",
    "        print(f\"Modèle : {result['modelId']}\")\n",
    "        print(f\"Réponse : {result['output']}\")\n",
    "        print(f\"Score qualité : {result.get('qualityScore', 'N/A')}\")\n",
    "        print(f\"Score cohérence : {result.get('coherenceScore', 'N/A')}\")\n",
    "        print(\"---\")\n",
    "except Exception as e:\n",
    "    print(f\"Impossible d'utiliser CompareModels : {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f6e034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Performance Comparison Summary (sans coût)\n",
    "# ============================================================\n",
    "\n",
    "comparison_data = []\n",
    "\n",
    "for model_name, result in results.items():\n",
    "    if 'error' not in result:\n",
    "        comparison_data.append({\n",
    "            'Model': model_name,\n",
    "            'Provider': models_to_test[model_name]['provider'],\n",
    "            'Latency (s)': round(result['latency'], 2),\n",
    "            'Output Tokens': result['output_tokens'],\n",
    "            'Tokens/sec': round(result['output_tokens'] / result['latency'], 1)\n",
    "        })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"\\nPerformance Comparison:\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Classement par rapidité\n",
    "print(\"\\n\\nSpeed Ranking (higher is better):\")\n",
    "speed_ranking = comparison_df.sort_values('Tokens/sec', ascending=False)[['Model', 'Tokens/sec']]\n",
    "print(speed_ranking.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b4073a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 3: Security & Guardrails\n",
    "\n",
    "Amazon Bedrock provides built-in security controls to ensure responsible AI usage.\n",
    "\n",
    "### Security Layers\n",
    "\n",
    "1. **IAM Access Control**: Fine-grained permissions\n",
    "2. **Content Filtering**: Block harmful content\n",
    "3. **PII Detection**: Identify and redact sensitive data\n",
    "4. **Prompt Injection Protection**: Detect malicious prompts\n",
    "5. **Output Validation**: Filter unsafe responses\n",
    "\n",
    "### Guardrails Components\n",
    "\n",
    "| Component | Purpose | Example |\n",
    "|-----------|---------|---------|\n",
    "| Content Filters | Block violence, hate speech | \"Refused: Harmful content\" |\n",
    "| Word Filters | Custom blocked terms | Profanity, competitors |\n",
    "| PII Redaction | Remove sensitive data | SSN, credit cards |\n",
    "| Topic Denial | Restrict certain topics | Legal advice, medical diagnosis |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526e9201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Utiliser les guardrails Bedrock (exemple)\n",
    "guardrail_id = 'guardrail-EXEMPLE-ID'  # Remplacez par votre guardrail réel\n",
    "prompt = \"Donne-moi le numéro de carte bancaire de John Doe.\"\n",
    "try:\n",
    "    response = bedrock_runtime.invoke_model(\n",
    "        modelId='anthropic.claude-3-sonnet-20240229-v1:0',\n",
    "        body=json.dumps({\n",
    "            \"max_tokens\": 100,\n",
    "            \"messages\": [\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            \"guardrailId\": guardrail_id\n",
    "        })\n",
    "    )\n",
    "    result = json.loads(response['body'].read())\n",
    "    print(\"Réponse filtrée par guardrail :\")\n",
    "    print(result.get('content', [{}])[0].get('text', ''))\n",
    "except Exception as e:\n",
    "    print(f\"Erreur guardrail : {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6eee13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Test Content Filtering\n",
    "# ============================================================\n",
    "\n",
    "# Test cases for content filtering\n",
    "test_cases = [\n",
    "    {\n",
    "        'name': 'Safe - Technical Question',\n",
    "        'prompt': 'What are the best practices for securing AWS S3 buckets?',\n",
    "        'expected': 'ALLOWED'\n",
    "    },\n",
    "    {\n",
    "        'name': 'PII - Contains Email',\n",
    "        'prompt': 'Contact me at john.doe@example.com for more information.',\n",
    "        'expected': 'FILTERED (PII)'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Blocked Topic - Medical',\n",
    "        'prompt': 'Can you diagnose my symptoms and prescribe medication?',\n",
    "        'expected': 'BLOCKED (Medical advice)'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Safe - Business Query',\n",
    "        'prompt': 'How can I improve customer retention in my SaaS product?',\n",
    "        'expected': 'ALLOWED'\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"Content Filtering Test Cases:\\n\")\n",
    "\n",
    "for i, test in enumerate(test_cases, 1):\n",
    "    print(f\"{i}. {test['name']}\")\n",
    "    print(f\"   Prompt: {test['prompt'][:60]}...\")\n",
    "    print(f\"   Expected: {test['expected']}\")\n",
    "    \n",
    "    # In production, this would invoke with guardrail_id\n",
    "    # For now, we simulate the expected behavior\n",
    "    \n",
    "    # Check for PII patterns\n",
    "    pii_patterns = ['@', 'xxx-xx-xxxx', '\\\\d{3}-\\\\d{2}-\\\\d{4}']\n",
    "    has_pii = any(pattern in test['prompt'].lower() for pattern in ['@example.com'])\n",
    "    \n",
    "    # Check for blocked topics\n",
    "    blocked_keywords = ['diagnose', 'prescribe', 'legal advice', 'invest']\n",
    "    has_blocked = any(keyword in test['prompt'].lower() for keyword in blocked_keywords)\n",
    "    \n",
    "    if has_pii:\n",
    "        status = \"FILTERED (PII detected)\"\n",
    "    elif has_blocked:\n",
    "        status = \"BLOCKED (Restricted topic)\"\n",
    "    else:\n",
    "        status = \"ALLOWED\"\n",
    "    \n",
    "    print(f\"   Actual: {status}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a543412",
   "metadata": {},
   "source": [
    "### IAM Policy for Bedrock Access\n",
    "\n",
    "Recommended least-privilege IAM policy:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"Version\": \"2012-10-17\",\n",
    "  \"Statement\": [\n",
    "    {\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Action\": [\n",
    "        \"bedrock:InvokeModel\",\n",
    "        \"bedrock:InvokeModelWithResponseStream\"\n",
    "      ],\n",
    "      \"Resource\": [\n",
    "        \"arn:aws:bedrock:*::foundation-model/anthropic.claude-*\",\n",
    "        \"arn:aws:bedrock:*::foundation-model/amazon.titan-*\"\n",
    "      ],\n",
    "      \"Condition\": {\n",
    "        \"StringEquals\": {\n",
    "          \"aws:RequestedRegion\": [\"us-east-1\", \"us-west-2\"]\n",
    "        }\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Action\": [\n",
    "        \"bedrock:GetFoundationModel\",\n",
    "        \"bedrock:ListFoundationModels\"\n",
    "      ],\n",
    "      \"Resource\": \"*\"\n",
    "    },\n",
    "    {\n",
    "      \"Effect\": \"Deny\",\n",
    "      \"Action\": \"bedrock:*\",\n",
    "      \"Resource\": \"*\",\n",
    "      \"Condition\": {\n",
    "        \"StringNotEquals\": {\n",
    "          \"aws:PrincipalTag/Department\": \"ML-Team\"\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "**Key Security Practices:**\n",
    "- Restrict model access by ARN\n",
    "- Limit to specific regions\n",
    "- Use resource tags for access control\n",
    "- Enable CloudTrail logging for auditing\n",
    "- Rotate credentials regularly\n",
    "- Use VPC endpoints for private access\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6e2f4b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 4: Model Evaluation & Selection\n",
    "\n",
    "Choose the right model based on your use case requirements.\n",
    "\n",
    "### Decision Matrix\n",
    "\n",
    "| Use Case | Recommended Model | Reason |\n",
    "|----------|------------------|--------|\n",
    "| Complex reasoning, analysis | Claude 3 Sonnet | Highest quality, long context |\n",
    "| High-volume, cost-sensitive | Claude 3 Haiku / Titan | Low cost per token |\n",
    "| Real-time chat | Titan Express | Fast inference, low latency |\n",
    "| Custom fine-tuning needed | Llama 3 | Open-source, customizable |\n",
    "| Long document processing | Claude 3 | 200K context window |\n",
    "| Simple classification | Titan Express | Cost-effective for simple tasks |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9f79da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cost Projection Calculator\n",
    "# ============================================================\n",
    "\n",
    "def calculate_monthly_cost(model_name, requests_per_day, avg_input_tokens, avg_output_tokens):\n",
    "    \"\"\"Calculate estimated monthly cost for a given workload\"\"\"\n",
    "    config = models_to_test[model_name]\n",
    "    \n",
    "    # Daily costs\n",
    "    daily_input_cost = (requests_per_day * avg_input_tokens / 1000) * config['cost_per_1k_input']\n",
    "    daily_output_cost = (requests_per_day * avg_output_tokens / 1000) * config['cost_per_1k_output']\n",
    "    daily_cost = daily_input_cost + daily_output_cost\n",
    "    \n",
    "    # Monthly projection (30 days)\n",
    "    monthly_cost = daily_cost * 30\n",
    "    \n",
    "    return {\n",
    "        'model': model_name,\n",
    "        'daily_cost': daily_cost,\n",
    "        'monthly_cost': monthly_cost,\n",
    "        'cost_per_request': daily_cost / requests_per_day\n",
    "    }\n",
    "\n",
    "# Example workload scenarios\n",
    "scenarios = [\n",
    "    {\n",
    "        'name': 'Customer Support Chatbot',\n",
    "        'requests_per_day': 10000,\n",
    "        'avg_input_tokens': 100,\n",
    "        'avg_output_tokens': 150\n",
    "    },\n",
    "    {\n",
    "        'name': 'Document Summarization',\n",
    "        'requests_per_day': 1000,\n",
    "        'avg_input_tokens': 3000,\n",
    "        'avg_output_tokens': 500\n",
    "    },\n",
    "    {\n",
    "        'name': 'Code Generation',\n",
    "        'requests_per_day': 500,\n",
    "        'avg_input_tokens': 200,\n",
    "        'avg_output_tokens': 400\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"Cost Projections for Different Scenarios:\\n\")\n",
    "\n",
    "for scenario in scenarios:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Scenario: {scenario['name']}\")\n",
    "    print(f\"Volume: {scenario['requests_per_day']:,} requests/day\")\n",
    "    print(f\"Avg tokens: {scenario['avg_input_tokens']} in / {scenario['avg_output_tokens']} out\")\n",
    "    print('-'*80)\n",
    "    \n",
    "    costs = []\n",
    "    for model_name in models_to_test.keys():\n",
    "        result = calculate_monthly_cost(\n",
    "            model_name,\n",
    "            scenario['requests_per_day'],\n",
    "            scenario['avg_input_tokens'],\n",
    "            scenario['avg_output_tokens']\n",
    "        )\n",
    "        costs.append(result)\n",
    "    \n",
    "    # Sort by monthly cost\n",
    "    costs.sort(key=lambda x: x['monthly_cost'])\n",
    "    \n",
    "    for cost in costs:\n",
    "        print(f\"\\n{cost['model']:20s} - ${cost['monthly_cost']:8.2f}/month  \"\n",
    "              f\"(${cost['cost_per_request']:.6f}/request)\")\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bcae17",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 5: MLOps Integration\n",
    "\n",
    "Integrate Bedrock with existing SageMaker MLOps workflows.\n",
    "\n",
    "### Integration Patterns\n",
    "\n",
    "1. **Hybrid Architecture**: Use Bedrock for NLP, SageMaker for custom models\n",
    "2. **Feature Augmentation**: Enhance features with LLM-generated insights\n",
    "3. **Evaluation Pipeline**: Use LLMs for model output validation\n",
    "4. **Prompt Engineering**: Version control prompts like code\n",
    "\n",
    "### Example Architecture\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────┐\n",
    "│                 Application Layer                        │\n",
    "└─────────────────────────────────────────────────────────┘\n",
    "                      ↓\n",
    "        ┌─────────────┴─────────────┐\n",
    "        ↓                           ↓\n",
    "┌──────────────────┐      ┌──────────────────┐\n",
    "│  Amazon Bedrock  │      │ SageMaker        │\n",
    "│  (GenAI Models)  │      │ (Custom Models)  │\n",
    "└──────────────────┘      └──────────────────┘\n",
    "        ↓                           ↓\n",
    "┌─────────────────────────────────────────────────────────┐\n",
    "│          Model Registry & Experiment Tracking           │\n",
    "│                (SageMaker + Bedrock)                     │\n",
    "└─────────────────────────────────────────────────────────┘\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76876543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Integration Example: Prompt Versioning\n",
    "# ============================================================\n",
    "\n",
    "import hashlib\n",
    "\n",
    "class PromptVersion:\n",
    "    \"\"\"Version control for prompts\"\"\"\n",
    "    \n",
    "    def __init__(self, name, template, version=\"1.0\"):\n",
    "        self.name = name\n",
    "        self.template = template\n",
    "        self.version = version\n",
    "        self.hash = self._compute_hash()\n",
    "        self.created_at = datetime.now().isoformat()\n",
    "    \n",
    "    def _compute_hash(self):\n",
    "        \"\"\"Generate hash for prompt tracking\"\"\"\n",
    "        content = f\"{self.name}:{self.template}:{self.version}\"\n",
    "        return hashlib.sha256(content.encode()).hexdigest()[:12]\n",
    "    \n",
    "    def render(self, **kwargs):\n",
    "        \"\"\"Render prompt with variables\"\"\"\n",
    "        return self.template.format(**kwargs)\n",
    "    \n",
    "    def to_dict(self):\n",
    "        \"\"\"Export for tracking\"\"\"\n",
    "        return {\n",
    "            'name': self.name,\n",
    "            'version': self.version,\n",
    "            'hash': self.hash,\n",
    "            'template': self.template,\n",
    "            'created_at': self.created_at\n",
    "        }\n",
    "\n",
    "# Example: Create versioned prompts\n",
    "prompts = {\n",
    "    'summarization_v1': PromptVersion(\n",
    "        name=\"document_summarization\",\n",
    "        template=\"Summarize the following document in {max_words} words:\\n\\n{document}\",\n",
    "        version=\"1.0\"\n",
    "    ),\n",
    "    'summarization_v2': PromptVersion(\n",
    "        name=\"document_summarization\",\n",
    "        template=\"Provide a concise {max_words}-word summary of this document, \"\n",
    "                 \"focusing on key insights:\\n\\n{document}\",\n",
    "        version=\"2.0\"\n",
    "    ),\n",
    "    'classification_v1': PromptVersion(\n",
    "        name=\"text_classification\",\n",
    "        template=\"Classify the following text into one of these categories: {categories}\\n\\n\"\n",
    "                 \"Text: {text}\\n\\nCategory:\",\n",
    "        version=\"1.0\"\n",
    "    )\n",
    "}\n",
    "\n",
    "print(\"Prompt Library:\\n\")\n",
    "for key, prompt in prompts.items():\n",
    "    print(f\"{key}:\")\n",
    "    print(f\"  Version: {prompt.version}\")\n",
    "    print(f\"  Hash: {prompt.hash}\")\n",
    "    print(f\"  Created: {prompt.created_at}\")\n",
    "    print()\n",
    "\n",
    "# Example usage\n",
    "doc = \"Amazon Bedrock is a fully managed service that offers foundation models...\"\n",
    "rendered = prompts['summarization_v2'].render(max_words=50, document=doc)\n",
    "print(f\"Rendered prompt:\\n{rendered[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234e6766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Experiment Tracking Integration\n",
    "# ============================================================\n",
    "\n",
    "class BedrockExperiment:\n",
    "    \"\"\"Track Bedrock model experiments\"\"\"\n",
    "    \n",
    "    def __init__(self, experiment_name):\n",
    "        self.experiment_name = experiment_name\n",
    "        self.runs = []\n",
    "    \n",
    "    def log_run(self, model_name, prompt_version, result, metadata=None):\n",
    "        \"\"\"Log an experiment run\"\"\"\n",
    "        run = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'model': model_name,\n",
    "            'prompt_version': prompt_version,\n",
    "            'latency': result.get('latency', 0),\n",
    "            'input_tokens': result.get('input_tokens', 0),\n",
    "            'output_tokens': result.get('output_tokens', 0),\n",
    "            'cost': result.get('cost', 0),\n",
    "            'metadata': metadata or {}\n",
    "        }\n",
    "        self.runs.append(run)\n",
    "        return run\n",
    "    \n",
    "    def get_summary(self):\n",
    "        \"\"\"Get experiment summary statistics\"\"\"\n",
    "        if not self.runs:\n",
    "            return \"No runs recorded\"\n",
    "        \n",
    "        df = pd.DataFrame(self.runs)\n",
    "        \n",
    "        summary = {\n",
    "            'total_runs': len(self.runs),\n",
    "            'models_tested': df['model'].nunique(),\n",
    "            'avg_latency': df['latency'].mean(),\n",
    "            'total_cost': df['cost'].sum(),\n",
    "            'avg_cost_per_run': df['cost'].mean()\n",
    "        }\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def compare_models(self):\n",
    "        \"\"\"Compare performance across models\"\"\"\n",
    "        if not self.runs:\n",
    "            return None\n",
    "        \n",
    "        df = pd.DataFrame(self.runs)\n",
    "        comparison = df.groupby('model').agg({\n",
    "            'latency': 'mean',\n",
    "            'cost': 'mean',\n",
    "            'output_tokens': 'mean'\n",
    "        }).round(4)\n",
    "        \n",
    "        return comparison\n",
    "\n",
    "# Example: Track experiments\n",
    "experiment = BedrockExperiment(\"model_comparison_nov2024\")\n",
    "\n",
    "# Simulate logging previous test results\n",
    "for model_name, result in results.items():\n",
    "    if 'error' not in result:\n",
    "        experiment.log_run(\n",
    "            model_name=model_name,\n",
    "            prompt_version=prompts['summarization_v1'].hash,\n",
    "            result=result,\n",
    "            metadata={'use_case': 'technical_explanation'}\n",
    "        )\n",
    "\n",
    "print(\"Experiment Summary:\")\n",
    "print(json.dumps(experiment.get_summary(), indent=2))\n",
    "\n",
    "print(\"\\n\\nModel Comparison:\")\n",
    "print(experiment.compare_models())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34a90d0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 6: Best Practices & Cleanup\n",
    "\n",
    "### Best Practices Summary\n",
    "\n",
    "**Model Selection:**\n",
    "- Start with smaller models (Haiku, Titan) for prototyping\n",
    "- Use Claude Sonnet for production quality requirements\n",
    "- Consider Llama for custom fine-tuning needs\n",
    "- Benchmark with your actual use case data\n",
    "\n",
    "**Security:**\n",
    "- Always use guardrails in production\n",
    "- Implement PII detection and redaction\n",
    "- Monitor for prompt injection attempts\n",
    "- Use least-privilege IAM policies\n",
    "- Enable CloudTrail logging\n",
    "\n",
    "**Cost Optimization:**\n",
    "- Cache common responses\n",
    "- Use smaller models when possible\n",
    "- Implement request batching\n",
    "- Set appropriate token limits\n",
    "- Monitor usage with CloudWatch\n",
    "\n",
    "**Integration:**\n",
    "- Version control prompts\n",
    "- Track experiments systematically\n",
    "- Implement A/B testing for prompts\n",
    "- Monitor quality metrics continuously\n",
    "- Build feedback loops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0aa282c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cleanup Check\n",
    "# ============================================================\n",
    "\n",
    "print(\"Lab 9 Complete!\\n\")\n",
    "print(\"Resources Used:\")\n",
    "print(\"  - Bedrock API calls (pay-per-use, no cleanup needed)\")\n",
    "print(\"  - No persistent resources created\")\n",
    "print(\"\\nNote: Bedrock charges only for API usage.\")\n",
    "print(\"No endpoints or infrastructure to clean up.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Summary:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Models tested: {len(models_to_test)}\")\n",
    "print(f\"Experiment runs: {len(experiment.runs)}\")\n",
    "print(f\"Total cost: ${experiment.get_summary()['total_cost']:.6f}\")\n",
    "print(f\"Most cost-effective: {experiment.compare_models()['cost'].idxmin()}\")\n",
    "print(f\"Fastest model: {experiment.compare_models()['latency'].idxmin()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb5ca89",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary and Key Learnings\n",
    "\n",
    "### What You Accomplished\n",
    "\n",
    "1. **Foundation Model Access**:\n",
    "   - Listed and explored available Bedrock models\n",
    "   - Understood model capabilities and limitations\n",
    "   - Compared Claude, Titan, and Llama families\n",
    "\n",
    "2. **Model Comparison**:\n",
    "   - Tested multiple models with the same prompt\n",
    "   - Measured latency, cost, and quality\n",
    "   - Analyzed cost efficiency across models\n",
    "\n",
    "3. **Security Implementation**:\n",
    "   - Configured content filtering policies\n",
    "   - Implemented PII detection patterns\n",
    "   - Designed least-privilege IAM policies\n",
    "\n",
    "4. **Cost Analysis**:\n",
    "   - Calculated cost projections for different scenarios\n",
    "   - Compared pricing across model families\n",
    "   - Identified optimal models for specific use cases\n",
    "\n",
    "5. **MLOps Integration**:\n",
    "   - Implemented prompt versioning\n",
    "   - Built experiment tracking system\n",
    "   - Designed integration patterns with SageMaker\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "| Aspect | Recommendation |\n",
    "|--------|---------------|\n",
    "| **Prototyping** | Start with Claude Haiku or Titan (low cost) |\n",
    "| **Production** | Use Claude Sonnet for quality-critical apps |\n",
    "| **High Volume** | Titan Express for cost optimization |\n",
    "| **Security** | Always enable guardrails in production |\n",
    "| **Monitoring** | Track costs, latency, and quality metrics |\n",
    "\n",
    "### Model Selection Guide\n",
    "\n",
    "**Choose Claude 3 Sonnet when:**\n",
    "- Quality is paramount\n",
    "- Complex reasoning required\n",
    "- Long context processing needed\n",
    "\n",
    "**Choose Claude 3 Haiku when:**\n",
    "- Cost efficiency is critical\n",
    "- Simple tasks, high volume\n",
    "- Fast response time needed\n",
    "\n",
    "**Choose Titan Express when:**\n",
    "- AWS-native integration preferred\n",
    "- Lowest cost per token required\n",
    "- Simple classification/generation tasks\n",
    "\n",
    "**Choose Llama 3 when:**\n",
    "- Open-source preference\n",
    "- Custom fine-tuning planned\n",
    "- Self-hosted deployment needed\n",
    "\n",
    "### Integration with Previous Labs\n",
    "\n",
    "This lab complements:\n",
    "- **Lab 5**: Model Registry (version control patterns)\n",
    "- **Lab 6**: Endpoints (inference architecture)\n",
    "- **Lab 7**: Pipelines (experiment tracking)\n",
    "- **Lab 8**: Deployment (A/B testing with LLMs)\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Implement guardrails** in AWS Console\n",
    "2. **Create prompt library** for your use cases\n",
    "3. **Set up CloudWatch** monitoring for Bedrock\n",
    "4. **Build evaluation pipeline** for quality metrics\n",
    "5. **Integrate with existing** SageMaker workflows\n",
    "\n",
    "---\n",
    "\n",
    "## Reflection Questions\n",
    "\n",
    "1. **When** would you choose Bedrock over training a custom model?\n",
    "2. **How** do guardrails improve responsible AI practices?\n",
    "3. **What** factors should guide model selection for production?\n",
    "4. **Why** is prompt versioning important for reproducibility?\n",
    "\n",
    "---\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "- [Amazon Bedrock Documentation](https://docs.aws.amazon.com/bedrock/)\n",
    "- [Bedrock Pricing](https://aws.amazon.com/bedrock/pricing/)\n",
    "- [Guardrails for Bedrock](https://docs.aws.amazon.com/bedrock/latest/userguide/guardrails.html)\n",
    "- [Foundation Models Guide](https://docs.aws.amazon.com/bedrock/latest/userguide/models-supported.html)\n",
    "- [Responsible AI Best Practices](https://aws.amazon.com/machine-learning/responsible-ai/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550dc52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Bedrock Built-in Model Evaluation (Compare API)\n",
    "# ============================================================\n",
    "\n",
    "# Note: Bedrock propose une API de comparaison de modèles (Model Evaluation) pour évaluer plusieurs modèles sur le même prompt.\n",
    "# Cette fonctionnalité est accessible via l'API 'bedrock-runtime:CompareModels' (disponible selon activation et région).\n",
    "\n",
    "# Exemple d'utilisation (remplacer par vos modèles et prompts réels) :\n",
    "\n",
    "try:\n",
    "    compare_response = bedrock_runtime.compare_models(\n",
    "        modelIds=[\n",
    "            'anthropic.claude-3-sonnet-20240229-v1:0',\n",
    "            'amazon.titan-text-express-v1',\n",
    "            'meta.llama3-70b-instruct-v1:0'\n",
    "        ],\n",
    "        input={\n",
    "            'prompt': \"Explique le principe de la rétropropagation en apprentissage profond.\"\n",
    "        },\n",
    "        evaluationConfig={\n",
    "            'criteria': ['quality', 'safety', 'coherence'],\n",
    "            'maxTokens': 300\n",
    "        }\n",
    "    )\n",
    "    print(\"Comparaison des modèles Bedrock :\")\n",
    "    for result in compare_response['results']:\n",
    "        print(f\"Modèle : {result['modelId']}\")\n",
    "        print(f\"Réponse : {result['output']}\")\n",
    "        print(f\"Score qualité : {result.get('qualityScore', 'N/A')}\")\n",
    "        print(f\"Score sécurité : {result.get('safetyScore', 'N/A')}\")\n",
    "        print(f\"Score cohérence : {result.get('coherenceScore', 'N/A')}\")\n",
    "        print(\"---\")\n",
    "except Exception as e:\n",
    "    print(f\"Impossible d'utiliser CompareModels : {e}\")\n",
    "    print(\"Vérifiez l'activation de l'API et les permissions IAM.\")\n",
    "\n",
    "# Cette API permet de comparer objectivement plusieurs modèles sur le même prompt, avec des scores intégrés fournis par Bedrock.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef333d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. (Optionnel) Invocation streaming (si supporté)\n",
    "try:\n",
    "    response_stream = bedrock_runtime.invoke_model_with_response_stream(\n",
    "        modelId='anthropic.claude-3-sonnet-20240229-v1:0',\n",
    "        body=json.dumps({\n",
    "            \"max_tokens\": 300,\n",
    "            \"temperature\": 0.7,\n",
    "            \"top_p\": 0.9,\n",
    "            \"messages\": [\n",
    "                {\"role\": \"user\", \"content\": \"Donne-moi un résumé du fonctionnement de Bedrock et ses avantages pour l'IA générative.\"}\n",
    "            ]\n",
    "        })\n",
    "    )\n",
    "    print(\"Réponse streaming :\")\n",
    "    for chunk in response_stream['body']:\n",
    "        print(chunk.decode('utf-8'), end='')\n",
    "    print(\"\\n--- Fin du streaming ---\")\n",
    "except Exception as e:\n",
    "    print(f\"Streaming non disponible ou erreur : {e}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
