{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88733bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Environment Setup\n",
    "# ============================================================\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add project root to path\n",
    "project_root = os.path.abspath('../..')\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# Standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import json\n",
    "import re\n",
    "\n",
    "# NLP imports\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix,\n",
    "    accuracy_score, f1_score\n",
    ")\n",
    "\n",
    "# SageMaker imports\n",
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.sklearn import SKLearnModel\n",
    "\n",
    "# Configuration\n",
    "try:\n",
    "    from utils.sagemaker_config import get_sagemaker_config\n",
    "    config = get_sagemaker_config(s3_prefix='lab4-sentiment')\n",
    "    role = config['role']\n",
    "    session = config['session']\n",
    "    bucket = config['bucket']\n",
    "    region = config['region']\n",
    "except ImportError:\n",
    "    print(\"Using fallback configuration\")\n",
    "    role = get_execution_role()\n",
    "    session = sagemaker.Session()\n",
    "    bucket = session.default_bucket()\n",
    "    region = session.boto_region_name\n",
    "\n",
    "print(\"Configuration complete.\")\n",
    "print(f\"Region: {region}\")\n",
    "print(f\"S3 Bucket: {bucket}\")\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0de8c97",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è Endpoint Cleanup (Run if getting scaler.pkl error)\n",
    "\n",
    "Si vous obtenez l'erreur `scaler.pkl not found`, ex√©cutez cette cellule pour supprimer les anciens endpoints avant de red√©ployer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2685c6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cleanup Old Endpoints (if needed)\n",
    "# ============================================================\n",
    "\n",
    "import boto3\n",
    "\n",
    "print(\"üîç Checking for old sentiment-analysis endpoints...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "sm_client = boto3.client('sagemaker')\n",
    "\n",
    "try:\n",
    "    # List existing endpoints\n",
    "    response = sm_client.list_endpoints(\n",
    "        NameContains='sentiment-analysis',\n",
    "        StatusEquals='InService'\n",
    "    )\n",
    "    \n",
    "    endpoints = response['Endpoints']\n",
    "    \n",
    "    if not endpoints:\n",
    "        print(\"‚úÖ No old endpoints found. You can proceed with deployment.\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è  Found {len(endpoints)} existing endpoint(s):\\n\")\n",
    "        for ep in endpoints:\n",
    "            print(f\"  ‚Ä¢ {ep['EndpointName']} (Created: {ep['CreationTime']})\")\n",
    "        \n",
    "        print(\"\\nüóëÔ∏è  Deleting old endpoints to avoid scaler.pkl errors...\")\n",
    "        for ep in endpoints:\n",
    "            endpoint_name = ep['EndpointName']\n",
    "            sm_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "            print(f\"‚úÖ Deleted: {endpoint_name}\")\n",
    "        \n",
    "        print(\"\\n‚úÖ Cleanup complete! You can now deploy a new endpoint.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Note: {e}\")\n",
    "    print(\"Proceeding with notebook...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33aff24d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 1: Data Generation and Exploration\n",
    "\n",
    "We'll generate synthetic product reviews with varying sentiments.\n",
    "\n",
    "### Sentiment Categories\n",
    "\n",
    "| Sentiment | Score Range | Characteristics |\n",
    "|-----------|-------------|-----------------|\n",
    "| Positive | 4-5 stars | Praise, satisfaction |\n",
    "| Neutral | 3 stars | Mixed feelings, OK |\n",
    "| Negative | 1-2 stars | Complaints, dissatisfaction |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743430fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Generate Synthetic Review Dataset\n",
    "# ============================================================\n",
    "\n",
    "def generate_reviews(n_samples=1500, random_state=42):\n",
    "    \"\"\"Generate synthetic product reviews\"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Review templates by sentiment\n",
    "    templates = {\n",
    "        'positive': [\n",
    "            \"Excellent product! {}\",\n",
    "            \"Love this! {}\",\n",
    "            \"Great quality, {}\",\n",
    "            \"Highly recommend! {}\",\n",
    "            \"Perfect! {}\",\n",
    "            \"Amazing, {}\",\n",
    "            \"Best purchase ever! {}\",\n",
    "            \"Outstanding {}\",\n",
    "            \"Fantastic product, {}\",\n",
    "            \"Exceeded expectations! {}\"\n",
    "        ],\n",
    "        'neutral': [\n",
    "            \"It's okay, {}\",\n",
    "            \"Average product, {}\",\n",
    "            \"Nothing special, {}\",\n",
    "            \"Decent but {}\",\n",
    "            \"Works as expected, {}\",\n",
    "            \"Acceptable, {}\",\n",
    "            \"Fair quality, {}\",\n",
    "            \"So-so, {}\",\n",
    "            \"Not bad, {}\",\n",
    "            \"Could be better, {}\"\n",
    "        ],\n",
    "        'negative': [\n",
    "            \"Terrible! {}\",\n",
    "            \"Very disappointed, {}\",\n",
    "            \"Poor quality, {}\",\n",
    "            \"Waste of money! {}\",\n",
    "            \"Don't buy this, {}\",\n",
    "            \"Awful product, {}\",\n",
    "            \"Regret buying, {}\",\n",
    "            \"Horrible, {}\",\n",
    "            \"Not as described, {}\",\n",
    "            \"Complete disaster! {}\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Detail phrases\n",
    "    details = {\n",
    "        'positive': [\n",
    "            \"works perfectly\",\n",
    "            \"great value for money\",\n",
    "            \"fast shipping\",\n",
    "            \"exactly as described\",\n",
    "            \"will buy again\",\n",
    "            \"looks great\",\n",
    "            \"high quality materials\",\n",
    "            \"easy to use\",\n",
    "            \"durable and reliable\",\n",
    "            \"customer service was excellent\"\n",
    "        ],\n",
    "        'neutral': [\n",
    "            \"does the job\",\n",
    "            \"average quality\",\n",
    "            \"price is fair\",\n",
    "            \"shipping took a while\",\n",
    "            \"meets basic needs\",\n",
    "            \"nothing extraordinary\",\n",
    "            \"standard quality\",\n",
    "            \"acceptable for the price\",\n",
    "            \"got what I paid for\",\n",
    "            \"works but could improve\"\n",
    "        ],\n",
    "        'negative': [\n",
    "            \"broke after one day\",\n",
    "            \"not worth the price\",\n",
    "            \"terrible quality\",\n",
    "            \"doesn't work as advertised\",\n",
    "            \"waste of time\",\n",
    "            \"poor customer service\",\n",
    "            \"damaged on arrival\",\n",
    "            \"stopped working quickly\",\n",
    "            \"misleading description\",\n",
    "            \"requesting refund\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    reviews = []\n",
    "    sentiments = []\n",
    "    ratings = []\n",
    "    \n",
    "    # Generate balanced dataset\n",
    "    samples_per_class = n_samples // 3\n",
    "    \n",
    "    for sentiment in ['positive', 'neutral', 'negative']:\n",
    "        for _ in range(samples_per_class):\n",
    "            template = np.random.choice(templates[sentiment])\n",
    "            detail = np.random.choice(details[sentiment])\n",
    "            \n",
    "            review = template.format(detail)\n",
    "            \n",
    "            # Add variation\n",
    "            if np.random.random() < 0.2:\n",
    "                review = review.upper()\n",
    "            \n",
    "            # Assign rating based on sentiment\n",
    "            if sentiment == 'positive':\n",
    "                rating = np.random.choice([4, 5], p=[0.3, 0.7])\n",
    "            elif sentiment == 'neutral':\n",
    "                rating = 3\n",
    "            else:\n",
    "                rating = np.random.choice([1, 2], p=[0.6, 0.4])\n",
    "            \n",
    "            reviews.append(review)\n",
    "            sentiments.append(sentiment)\n",
    "            ratings.append(rating)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'review_id': [f'REV_{i:06d}' for i in range(len(reviews))],\n",
    "        'review_text': reviews,\n",
    "        'sentiment': sentiments,\n",
    "        'rating': ratings\n",
    "    })\n",
    "    \n",
    "    # Shuffle\n",
    "    df = df.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Generate dataset\n",
    "print(\"Generating product review dataset...\")\n",
    "reviews_df = generate_reviews(n_samples=1500, random_state=42)\n",
    "\n",
    "print(f\"\\nDataset shape: {reviews_df.shape}\")\n",
    "print(f\"\\nSentiment distribution:\")\n",
    "print(reviews_df['sentiment'].value_counts())\n",
    "print(f\"\\nRating distribution:\")\n",
    "print(reviews_df['rating'].value_counts().sort_index())\n",
    "print(f\"\\nSample reviews:\")\n",
    "reviews_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0adecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Text Preprocessing\n",
    "# ============================================================\n",
    "\n",
    "def preprocess_review(text):\n",
    "    \"\"\"Clean review text\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'!+', '!', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "reviews_df['review_clean'] = reviews_df['review_text'].apply(preprocess_review)\n",
    "\n",
    "# Text statistics\n",
    "reviews_df['text_length'] = reviews_df['review_clean'].str.len()\n",
    "reviews_df['word_count'] = reviews_df['review_clean'].str.split().str.len()\n",
    "\n",
    "print(\"Text Statistics by Sentiment:\")\n",
    "print(\"=\"*60)\n",
    "print(reviews_df.groupby('sentiment')[['text_length', 'word_count']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fefec6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 2: Exploratory Data Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3fc094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Visualization\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Sentiment distribution\n",
    "ax = axes[0, 0]\n",
    "reviews_df['sentiment'].value_counts().plot(kind='bar', ax=ax, color=['green', 'gray', 'red'])\n",
    "ax.set_title('Sentiment Distribution')\n",
    "ax.set_xlabel('Sentiment')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n",
    "\n",
    "# Rating distribution\n",
    "ax = axes[0, 1]\n",
    "reviews_df['rating'].value_counts().sort_index().plot(kind='bar', ax=ax, color='skyblue')\n",
    "ax.set_title('Rating Distribution')\n",
    "ax.set_xlabel('Rating (stars)')\n",
    "ax.set_ylabel('Count')\n",
    "\n",
    "# Text length by sentiment\n",
    "ax = axes[1, 0]\n",
    "reviews_df.boxplot(column='text_length', by='sentiment', ax=ax)\n",
    "ax.set_title('Text Length by Sentiment')\n",
    "ax.set_xlabel('Sentiment')\n",
    "ax.set_ylabel('Characters')\n",
    "\n",
    "# Word count by sentiment\n",
    "ax = axes[1, 1]\n",
    "reviews_df.boxplot(column='word_count', by='sentiment', ax=ax)\n",
    "ax.set_title('Word Count by Sentiment')\n",
    "ax.set_xlabel('Sentiment')\n",
    "ax.set_ylabel('Words')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Correlation between rating and sentiment\n",
    "print(\"\\nRating vs Sentiment Crosstab:\")\n",
    "print(pd.crosstab(reviews_df['rating'], reviews_df['sentiment']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c73cd5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 3: Feature Extraction and Model Training\n",
    "\n",
    "Using TF-IDF for cost-effective sentiment analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f76d018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TF-IDF Feature Extraction\n",
    "# ============================================================\n",
    "\n",
    "# Prepare data\n",
    "X_text = reviews_df['review_clean']\n",
    "y = reviews_df['sentiment']\n",
    "\n",
    "# Label encoding\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "print(\"Label mapping:\")\n",
    "for idx, label in enumerate(label_encoder.classes_):\n",
    "    print(f\"  {label}: {idx}\")\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_text, y_encoded,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y_encoded\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=800,\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=2,\n",
    "    max_df=0.8,\n",
    "    stop_words='english'\n",
    ")\n",
    "\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "print(f\"\\nTF-IDF matrix shape: {X_train_tfidf.shape}\")\n",
    "print(f\"Vocabulary size: {len(tfidf.vocabulary_)}\")\n",
    "\n",
    "# Top words per sentiment\n",
    "print(\"\\nTop 10 words per sentiment:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "for idx, sentiment in enumerate(label_encoder.classes_):\n",
    "    sentiment_mask = y_train == idx\n",
    "    sentiment_tfidf = X_train_tfidf[sentiment_mask].mean(axis=0).A1\n",
    "    top_indices = sentiment_tfidf.argsort()[-10:][::-1]\n",
    "    top_words = [feature_names[i] for i in top_indices]\n",
    "    print(f\"\\n{sentiment.upper()}:\")\n",
    "    print(f\"  {', '.join(top_words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f7e8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Train Sentiment Classifier\n",
    "# ============================================================\n",
    "\n",
    "# Train logistic regression with balanced classes\n",
    "model = LogisticRegression(\n",
    "    random_state=42,\n",
    "    max_iter=500,\n",
    "    class_weight='balanced',\n",
    "    multi_class='multinomial',\n",
    "    solver='lbfgs'\n",
    ")\n",
    "\n",
    "print(\"Training sentiment classifier...\")\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "y_pred_proba = model.predict_proba(X_test_tfidf)\n",
    "\n",
    "# Metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "f1_weighted = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f\"\\nModel Performance:\")\n",
    "print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "print(f\"  F1 Score (macro): {f1_macro:.4f}\")\n",
    "print(f\"  F1 Score (weighted): {f1_weighted:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da0468f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Detailed Evaluation\n",
    "# ============================================================\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(\n",
    "    y_test,\n",
    "    y_pred,\n",
    "    target_names=label_encoder.classes_\n",
    "))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt='d',\n",
    "    cmap='Blues',\n",
    "    xticklabels=label_encoder.classes_,\n",
    "    yticklabels=label_encoder.classes_\n",
    ")\n",
    "plt.title('Sentiment Analysis Confusion Matrix')\n",
    "plt.ylabel('True Sentiment')\n",
    "plt.xlabel('Predicted Sentiment')\n",
    "plt.show()\n",
    "\n",
    "# Per-sentiment accuracy\n",
    "print(\"\\nPer-Sentiment Accuracy:\")\n",
    "for idx, sentiment in enumerate(label_encoder.classes_):\n",
    "    mask = y_test == idx\n",
    "    sentiment_acc = accuracy_score(y_test[mask], y_pred[mask])\n",
    "    print(f\"  {sentiment.capitalize()}: {sentiment_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa6df9f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 4: Model Interpretation and Testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720fa643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Feature Importance\n",
    "# ============================================================\n",
    "\n",
    "print(\"Most Important Words per Sentiment:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "\n",
    "for idx, sentiment in enumerate(label_encoder.classes_):\n",
    "    coef = model.coef_[idx]\n",
    "    top_positive_idx = coef.argsort()[-10:][::-1]\n",
    "    top_words = [(feature_names[i], coef[i]) for i in top_positive_idx]\n",
    "    \n",
    "    print(f\"\\n{sentiment.upper()} - Top indicators:\")\n",
    "    for word, score in top_words:\n",
    "        print(f\"  {word}: {score:.4f}\")\n",
    "\n",
    "# Test with sample reviews\n",
    "print(\"\\n\\nSample Predictions:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_reviews = [\n",
    "    \"Excellent product! Works perfectly and great value\",\n",
    "    \"It's okay, does the job but nothing special\",\n",
    "    \"Terrible quality! Broke after one day, waste of money\",\n",
    "    \"Amazing! Highly recommend this to everyone\",\n",
    "    \"Not bad but could be better for the price\"\n",
    "]\n",
    "\n",
    "for review in test_reviews:\n",
    "    clean_review = preprocess_review(review)\n",
    "    review_tfidf = tfidf.transform([clean_review])\n",
    "    \n",
    "    pred_idx = model.predict(review_tfidf)[0]\n",
    "    pred_proba = model.predict_proba(review_tfidf)[0]\n",
    "    pred_sentiment = label_encoder.classes_[pred_idx]\n",
    "    \n",
    "    print(f\"\\nReview: '{review}'\")\n",
    "    print(f\"Predicted: {pred_sentiment.upper()} (confidence: {pred_proba[pred_idx]:.2%})\")\n",
    "    print(f\"All probabilities: {dict(zip(label_encoder.classes_, pred_proba))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818f3192",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 5: Model Deployment\n",
    "\n",
    "Deploy sentiment analysis model to SageMaker.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b17392a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Package Model for Deployment\n",
    "# ============================================================\n",
    "\n",
    "import joblib\n",
    "import tarfile\n",
    "import shutil\n",
    "\n",
    "# Create model directory\n",
    "model_dir = 'sentiment_model'\n",
    "if os.path.exists(model_dir):\n",
    "    shutil.rmtree(model_dir)\n",
    "os.makedirs(model_dir)\n",
    "\n",
    "# Save artifacts\n",
    "joblib.dump(model, os.path.join(model_dir, 'model.pkl'))\n",
    "joblib.dump(tfidf, os.path.join(model_dir, 'tfidf.pkl'))\n",
    "joblib.dump(label_encoder, os.path.join(model_dir, 'label_encoder.pkl'))\n",
    "\n",
    "print(\"Model artifacts saved\")\n",
    "\n",
    "# Create inference script\n",
    "inference_code = \"\"\"\n",
    "import joblib\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    model = joblib.load(os.path.join(model_dir, 'model.pkl'))\n",
    "    tfidf = joblib.load(os.path.join(model_dir, 'tfidf.pkl'))\n",
    "    label_encoder = joblib.load(os.path.join(model_dir, 'label_encoder.pkl'))\n",
    "    return {'model': model, 'tfidf': tfidf, 'label_encoder': label_encoder}\n",
    "\n",
    "def preprocess_review(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'!+', '!', text)\n",
    "    text = re.sub(r'\\\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def input_fn(request_body, request_content_type):\n",
    "    if request_content_type == 'application/json':\n",
    "        data = json.loads(request_body)\n",
    "        if isinstance(data, dict):\n",
    "            return [data.get('review', '')]\n",
    "        elif isinstance(data, list):\n",
    "            return [item.get('review', '') if isinstance(item, dict) else str(item) for item in data]\n",
    "        else:\n",
    "            return [str(data)]\n",
    "    else:\n",
    "        return [request_body]\n",
    "\n",
    "def predict_fn(input_data, model_dict):\n",
    "    model = model_dict['model']\n",
    "    tfidf = model_dict['tfidf']\n",
    "    label_encoder = model_dict['label_encoder']\n",
    "    \n",
    "    # Preprocess\n",
    "    reviews_clean = [preprocess_review(review) for review in input_data]\n",
    "    \n",
    "    # Vectorize\n",
    "    X_tfidf = tfidf.transform(reviews_clean)\n",
    "    \n",
    "    # Predict\n",
    "    predictions = model.predict(X_tfidf)\n",
    "    probabilities = model.predict_proba(X_tfidf)\n",
    "    \n",
    "    # Format results\n",
    "    results = []\n",
    "    for pred_idx, proba in zip(predictions, probabilities):\n",
    "        sentiment = label_encoder.classes_[pred_idx]\n",
    "        confidence = float(proba[pred_idx])\n",
    "        all_probs = {label: float(prob) for label, prob in zip(label_encoder.classes_, proba)}\n",
    "        \n",
    "        results.append({\n",
    "            'sentiment': sentiment,\n",
    "            'confidence': confidence,\n",
    "            'probabilities': all_probs\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "def output_fn(prediction, accept):\n",
    "    if accept == 'application/json':\n",
    "        return json.dumps(prediction), accept\n",
    "    raise ValueError(f'Unsupported accept type: {accept}')\n",
    "\"\"\"\n",
    "\n",
    "with open(os.path.join(model_dir, 'inference.py'), 'w') as f:\n",
    "    f.write(inference_code)\n",
    "\n",
    "# Create requirements\n",
    "requirements = \"\"\"scikit-learn==1.3.0\n",
    "joblib==1.3.2\n",
    "numpy==1.24.3\n",
    "\"\"\"\n",
    "\n",
    "with open(os.path.join(model_dir, 'requirements.txt'), 'w') as f:\n",
    "    f.write(requirements)\n",
    "\n",
    "# Create tar.gz\n",
    "model_archive = 'model.tar.gz'\n",
    "with tarfile.open(model_archive, 'w:gz') as tar:\n",
    "    tar.add(model_dir, arcname='.')\n",
    "\n",
    "# Upload to S3\n",
    "model_s3_key = f'lab4-sentiment/models/{datetime.now().strftime(\"%Y%m%d-%H%M%S\")}/model.tar.gz'\n",
    "s3_client = boto3.client('s3')\n",
    "s3_client.upload_file(model_archive, bucket, model_s3_key)\n",
    "\n",
    "model_s3_uri = f's3://{bucket}/{model_s3_key}'\n",
    "print(f\"Model uploaded to: {model_s3_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5737d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Deploy to SageMaker Endpoint\n",
    "# ============================================================\n",
    "\n",
    "sklearn_model = SKLearnModel(\n",
    "    model_data=model_s3_uri,\n",
    "    role=role,\n",
    "    entry_point='inference.py',\n",
    "    framework_version='1.2-1',\n",
    "    py_version='py3',\n",
    "    name=f'sentiment-analysis-{datetime.now().strftime(\"%Y%m%d-%H%M%S\")}',\n",
    "    env={'SAGEMAKER_PROGRAM': 'inference.py'}\n",
    ")\n",
    "\n",
    "print(\"Deploying model to endpoint...\")\n",
    "print(\"This will take 5-10 minutes...\")\n",
    "\n",
    "predictor = sklearn_model.deploy("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073e1ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Test Endpoint\n",
    "# ============================================================\n",
    "\n",
    "test_reviews = [\n",
    "    {\"review\": \"Absolutely love this product! Best purchase ever\"},\n",
    "    {\"review\": \"It's decent, works as expected\"},\n",
    "    {\"review\": \"Terrible quality, very disappointed\"},\n",
    "    {\"review\": \"Amazing! Exceeded all my expectations\"}\n",
    "]\n",
    "\n",
    "print(\"Testing sentiment analysis endpoint:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for review in test_reviews:\n",
    "    response = predictor.predict(review)\n",
    "    \n",
    "    print(f\"\\nReview: '{review['review']}'\")\n",
    "    print(f\"Sentiment: {response[0]['sentiment'].upper()}\")\n",
    "    print(f\"Confidence: {response[0]['confidence']:.2%}\")\n",
    "\n",
    "# Batch prediction\n",
    "print(\"\\n\\nBatch prediction test:\")\n",
    "batch_response = predictor.predict(test_reviews)\n",
    "\n",
    "for i, (review, result) in enumerate(zip(test_reviews, batch_response)):\n",
    "    print(f\"\\n{i+1}. {result['sentiment'].upper()} ({result['confidence']:.0%}) - '{review['review'][:50]}...'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092bc4d9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 8: SageMaker Model Monitor - Data & Model Drift\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5a03d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SageMaker Model Monitor - Setup\n",
    "# ============================================================\n",
    "\n",
    "from sagemaker.model_monitor import DefaultModelMonitor, CronExpressionGenerator\n",
    "from sagemaker.model_monitor.dataset_format import DatasetFormat\n",
    "\n",
    "print(\"SageMaker Model Monitor - Setup\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nModel Monitor tracks data quality and model performance drift\\n\")\n",
    "\n",
    "# Cr√©er un Model Monitor\n",
    "model_monitor = DefaultModelMonitor(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.large',\n",
    "    volume_size_in_gb=20,\n",
    "    max_runtime_in_seconds=3600,\n",
    "    sagemaker_session=session\n",
    ")\n",
    "\n",
    "print(f\"Model Monitor created: {model_monitor}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b264b4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Create Baseline Dataset\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nCreating baseline dataset for monitoring...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Pr√©parer des donn√©es de baseline (√©chantillon du train set)\n",
    "baseline_df = pd.DataFrame({\n",
    "    'review': train_reviews[:1000],\n",
    "    'sentiment': [label_encoder.inverse_transform([s])[0] for s in train_labels[:1000]]\n",
    "})\n",
    "\n",
    "# Sauvegarder le baseline\n",
    "baseline_path = f's3://{bucket}/model-monitor/baseline'\n",
    "baseline_file = '/tmp/baseline.csv'\n",
    "baseline_df.to_csv(baseline_file, index=False, header=False)\n",
    "boto3.client('s3').upload_file(baseline_file, bucket, 'model-monitor/baseline/baseline.csv')\n",
    "\n",
    "print(f\"Baseline dataset saved: {baseline_path}\")\n",
    "print(f\"Baseline size: {len(baseline_df)} samples\")\n",
    "\n",
    "# Sugg√©rer une baseline (requires endpoint data capture enabled)\n",
    "print(\"\\nNote: In production, you would:\")\n",
    "print(\"1. Enable data capture on the endpoint\")\n",
    "print(\"2. Suggest baseline from captured data\")\n",
    "print(\"3. Create monitoring schedule\")\n",
    "print(\"\\nExample code:\")\n",
    "print(\"\"\"\n",
    "# Enable data capture when deploying\n",
    "data_capture_config = DataCaptureConfig(\n",
    "    enable_capture=True,\n",
    "    sampling_percentage=100,\n",
    "    destination_s3_uri=f's3://{bucket}/data-capture'\n",
    ")\n",
    "\n",
    "predictor = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m5.large',\n",
    "    data_capture_config=data_capture_config\n",
    ")\n",
    "\n",
    "# Suggest baseline\n",
    "model_monitor.suggest_baseline(\n",
    "    baseline_dataset=baseline_path,\n",
    "    dataset_format=DatasetFormat.csv(header=False),\n",
    "    output_s3_uri=f's3://{bucket}/model-monitor/baseline-results'\n",
    ")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6a3d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Monitor Data Quality - Simulated Drift\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\\nSimulating Data Drift Detection:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Simuler des donn√©es de production (avec drift)\n",
    "print(\"\\n1. Baseline Distribution (Training Data):\")\n",
    "baseline_sentiments = train_df['sentiment'].value_counts(normalize=True)\n",
    "print(baseline_sentiments)\n",
    "\n",
    "# Simuler du drift: plus de reviews n√©gatives\n",
    "drift_data = pd.concat([\n",
    "    train_df[train_df['sentiment'] == 'negative'].sample(600, replace=True),\n",
    "    train_df[train_df['sentiment'] == 'neutral'].sample(200, replace=True),\n",
    "    train_df[train_df['sentiment'] == 'positive'].sample(200, replace=True)\n",
    "])\n",
    "\n",
    "print(\"\\n2. Production Distribution (With Drift):\")\n",
    "drift_sentiments = drift_data['sentiment'].value_counts(normalize=True)\n",
    "print(drift_sentiments)\n",
    "\n",
    "# Calculer la divergence\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "baseline_counts = train_df['sentiment'].value_counts()\n",
    "drift_counts = drift_data['sentiment'].value_counts()\n",
    "\n",
    "# Aligner les indices\n",
    "all_categories = list(set(baseline_counts.index) | set(drift_counts.index))\n",
    "baseline_aligned = [baseline_counts.get(cat, 0) for cat in all_categories]\n",
    "drift_aligned = [drift_counts.get(cat, 0) for cat in all_categories]\n",
    "\n",
    "# Chi-square test\n",
    "chi2, p_value, dof, expected = chi2_contingency([baseline_aligned, drift_aligned])\n",
    "\n",
    "print(f\"\\n3. Drift Detection:\")\n",
    "print(f\"Chi-square statistic: {chi2:.4f}\")\n",
    "print(f\"P-value: {p_value:.4f}\")\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(\"‚ö†Ô∏è  ALERT: Significant distribution shift detected!\")\n",
    "    print(\"Action: Investigate cause and consider retraining\")\n",
    "else:\n",
    "    print(\"‚úÖ No significant drift detected\")\n",
    "\n",
    "# Visualiser le drift\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "baseline_sentiments.plot(kind='bar', ax=axes[0], color='skyblue', alpha=0.7)\n",
    "axes[0].set_title('Baseline Distribution (Training)')\n",
    "axes[0].set_ylabel('Proportion')\n",
    "axes[0].set_xlabel('Sentiment')\n",
    "axes[0].set_ylim(0, 1)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "drift_sentiments.plot(kind='bar', ax=axes[1], color='coral', alpha=0.7)\n",
    "axes[1].set_title('Production Distribution (With Drift)')\n",
    "axes[1].set_ylabel('Proportion')\n",
    "axes[1].set_xlabel('Sentiment')\n",
    "axes[1].set_ylim(0, 1)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_monitor_drift.png', dpi=100, bbox_inches='tight')\n",
    "print(f\"\\nüìä Drift visualization saved\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea66b907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Monitor Model Quality\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\\nMonitoring Model Quality:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Comparer performance sur baseline vs drift data\n",
    "print(\"\\n1. Baseline Performance:\")\n",
    "baseline_pred = model.predict(vectorizer.transform(train_reviews[:1000]))\n",
    "baseline_accuracy = accuracy_score(train_labels[:1000], baseline_pred)\n",
    "print(f\"Accuracy: {baseline_accuracy:.2%}\")\n",
    "\n",
    "print(\"\\n2. Production Performance (with drift):\")\n",
    "drift_reviews = drift_data['review'].tolist()\n",
    "drift_labels_encoded = label_encoder.transform(drift_data['sentiment'])\n",
    "drift_pred = model.predict(vectorizer.transform(drift_reviews))\n",
    "drift_accuracy = accuracy_score(drift_labels_encoded, drift_pred)\n",
    "print(f\"Accuracy: {drift_accuracy:.2%}\")\n",
    "\n",
    "# Calculer la d√©gradation\n",
    "degradation = (baseline_accuracy - drift_accuracy) / baseline_accuracy * 100\n",
    "print(f\"\\n3. Model Degradation: {degradation:.2f}%\")\n",
    "\n",
    "if abs(degradation) > 5:\n",
    "    print(\"‚ö†Ô∏è  ALERT: Model performance degraded significantly!\")\n",
    "    print(\"Action: Retrain model with recent data\")\n",
    "else:\n",
    "    print(\"‚úÖ Model performance stable\")\n",
    "\n",
    "# Classification report d√©taill√©\n",
    "print(\"\\n4. Detailed Performance on Drift Data:\")\n",
    "print(classification_report(\n",
    "    drift_labels_encoded, \n",
    "    drift_pred, \n",
    "    target_names=label_encoder.classes_\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a532fbc7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### SageMaker Model Monitor Benefits\n",
    "\n",
    "**Data Quality Monitoring:**\n",
    "- Detect input data drift\n",
    "- Track feature distributions\n",
    "- Identify missing or invalid values\n",
    "- Alert on anomalous inputs\n",
    "\n",
    "**Model Quality Monitoring:**\n",
    "- Track prediction accuracy over time\n",
    "- Detect performance degradation\n",
    "- Monitor prediction drift\n",
    "- Trigger retraining when needed\n",
    "\n",
    "**Monitoring Schedule:**\n",
    "```python\n",
    "# In production, create a monitoring schedule\n",
    "from sagemaker.model_monitor import CronExpressionGenerator\n",
    "\n",
    "model_monitor.create_monitoring_schedule(\n",
    "    monitor_schedule_name=f'sentiment-monitor-{timestamp}',\n",
    "    endpoint_input=predictor.endpoint_name,\n",
    "    output_s3_uri=f's3://{bucket}/monitoring-results',\n",
    "    statistics=baseline_statistics_uri,\n",
    "    constraints=baseline_constraints_uri,\n",
    "    schedule_cron_expression=CronExpressionGenerator.hourly()\n",
    ")\n",
    "```\n",
    "\n",
    "**Metrics to Monitor:**\n",
    "- **Data Drift**: Distribution shifts, new categories\n",
    "- **Model Drift**: Accuracy, precision, recall changes\n",
    "- **Inference Patterns**: Request volume, latency\n",
    "- **Data Quality**: Missing values, outliers\n",
    "\n",
    "**Alerting Options:**\n",
    "- CloudWatch Alarms\n",
    "- SNS notifications\n",
    "- Lambda triggers for auto-retraining\n",
    "- Email/Slack alerts\n",
    "\n",
    "**When to Retrain:**\n",
    "- Significant data drift (p-value < 0.05)\n",
    "- Performance degradation > 5%\n",
    "- New patterns in production data\n",
    "- Scheduled periodic retraining\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b7218a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Generate Monitoring Report\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\\nModel Monitoring Report:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "monitoring_report = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'baseline_metrics': {\n",
    "        'samples': len(train_df),\n",
    "        'accuracy': float(baseline_accuracy),\n",
    "        'distribution': baseline_sentiments.to_dict()\n",
    "    },\n",
    "    'production_metrics': {\n",
    "        'samples': len(drift_data),\n",
    "        'accuracy': float(drift_accuracy),\n",
    "        'distribution': drift_sentiments.to_dict()\n",
    "    },\n",
    "    'drift_detection': {\n",
    "        'chi2_statistic': float(chi2),\n",
    "        'p_value': float(p_value),\n",
    "        'significant_drift': bool(p_value < 0.05)\n",
    "    },\n",
    "    'model_health': {\n",
    "        'degradation_percent': float(degradation),\n",
    "        'action_required': bool(abs(degradation) > 5),\n",
    "        'recommendation': 'Retrain model' if abs(degradation) > 5 else 'Monitor'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(json.dumps(monitoring_report, indent=2))\n",
    "\n",
    "# Sauvegarder le rapport\n",
    "report_path = '/tmp/monitoring_report.json'\n",
    "with open(report_path, 'w') as f:\n",
    "    json.dump(monitoring_report, f, indent=2)\n",
    "\n",
    "# Upload vers S3\n",
    "boto3.client('s3').upload_file(\n",
    "    report_path, \n",
    "    bucket, \n",
    "    f'model-monitor/reports/{datetime.now().strftime(\"%Y%m%d-%H%M%S\")}.json'\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Monitoring report saved to S3\")\n",
    "print(f\"\\nRecommendation: {monitoring_report['model_health']['recommendation']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6609a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cleanup All Resources\n",
    "# ============================================================\n",
    "\n",
    "print(\"üßπ Cleaning up Lab 4 resources...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Delete ALL sentiment-analysis endpoints\n",
    "sm_client = boto3.client('sagemaker')\n",
    "\n",
    "try:\n",
    "    response = sm_client.list_endpoints(NameContains='sentiment-analysis')\n",
    "    endpoints = response['Endpoints']\n",
    "    \n",
    "    if endpoints:\n",
    "        print(f\"\\nüóëÔ∏è  Deleting {len(endpoints)} endpoint(s)...\")\n",
    "        for ep in endpoints:\n",
    "            endpoint_name = ep['EndpointName']\n",
    "            try:\n",
    "                sm_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "                print(f\"  ‚úÖ Deleted: {endpoint_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ö†Ô∏è  {endpoint_name}: {e}\")\n",
    "    else:\n",
    "        print(\"  No endpoints to delete\")\n",
    "except Exception as e:\n",
    "    print(f\"  Error listing endpoints: {e}\")\n",
    "\n",
    "# Cleanup local files\n",
    "print(\"\\nüìÅ Cleaning up local files...\")\n",
    "try:\n",
    "    if 'model_dir' in globals() and os.path.exists(model_dir):\n",
    "        shutil.rmtree(model_dir)\n",
    "        print(\"  ‚úÖ Deleted model directory\")\n",
    "    if 'model_archive' in globals() and os.path.exists(model_archive):\n",
    "        os.remove(model_archive)\n",
    "        print(\"  ‚úÖ Deleted model archive\")\n",
    "except Exception as e:\n",
    "    print(f\"  ‚ö†Ô∏è  {e}\")\n",
    "\n",
    "print(\"\\n‚úÖ Lab 4 cleanup complete!\")\n",
    "print(\"üí° Note: S3 data and models are kept for future use\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cdce63",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this lab, you:\n",
    "\n",
    "1. Generated and analyzed synthetic product review data\n",
    "2. Built sentiment classification model using TF-IDF\n",
    "3. Evaluated model performance on 3-class sentiment task\n",
    "4. Interpreted feature importance to understand predictions\n",
    "5. Deployed sentiment model to SageMaker endpoint\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **Sentiment Analysis**: Critical for understanding customer feedback\n",
    "- **Multi-class Classification**: Neutral sentiment adds complexity\n",
    "- **TF-IDF**: Cost-effective approach for sentiment tasks\n",
    "- **Feature Importance**: Shows which words drive sentiment predictions\n",
    "- **SageMaker Deployment**: Production-ready sentiment analysis\n",
    "\n",
    "### Cost Optimization\n",
    "\n",
    "| Component | Choice | Benefit |\n",
    "|-----------|--------|---------|\n",
    "| Model | TF-IDF + Logistic Regression | No GPU required |\n",
    "| Instance | ml.m5.large | Cost-effective |\n",
    "| Training | < 1 minute | Minimal cost |\n",
    "| Vocabulary | 800 features | Small model size |\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Lab 5: MLOps Packaging (Feature Store, Model Registry)\n",
    "- Try different sentiment granularities (5-star ratings)\n",
    "- Experiment with handling sarcasm and mixed sentiments\n",
    "- Add aspect-based sentiment analysis\n",
    "\n",
    "---\n",
    "\n",
    "**Remember to delete your endpoint to avoid charges!**\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
